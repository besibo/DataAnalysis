[
  {
    "objectID": "01-RBasics.html#préambule",
    "href": "01-RBasics.html#préambule",
    "title": "1  R et RStudio : les bases",
    "section": "1.1 Préambule",
    "text": "1.1 Préambule\nAvant de commencer à explorer des données dans R, il y a plusieurs concepts clés qu’il faut comprendre en premier lieu :\n\nQue sont R et RStudio ?\nComment s’y prend-on pour coder dans R ?\nQue sont les packages ?\n\nMême si vous pensez être déjà à l’aise avec ces concepts, lisez attentivement ce chapitre et faites les exercices demandés. Cela vous rafraîchira probablement la mémoire, et il n’est pas impossible que vous appreniez une chose ou deux au passage. Une bonne maîtrise des éléments présentés dans ce chapitre est indispensable pour aborder sereinement les chapitres suivants, à commencer par le Chapitre 2, qui présente un jeu de données que nous explorerons en détail un peu plus tard. Lisez donc attentivement ce chapitre et faites bien tous les exercices demandés.\nCe chapitre est en grande partie basé sur les 3 ressources suivantes que je vous encourage à consulter si vous souhaitez obtenir plus de détails :\n\nL’ouvrage intitulé ModernDive, de Chester Ismay et Albert Y. Kim. Une bonne partie de ce livre est très largement inspirée de cet ouvrage. C’est en anglais, mais c’est un très bon texte d’introduction aux statistiques sous R et RStudio.\nL’ouvrage intitulé Getting used to R, RStudio, and R Markdown de Chester Ismay, comprend des podcasts (en anglais toujours) que vous pouvez suivre en apprenant.\nLes tutoriels en ligne de DataCamp. DataCamp est une plateforme de e-learning accessible depuis n’importe quel navigateur internet et dont la priorité est l’enseignement des “data sciences”. Leurs tutoriels vous aideront à apprendre certains des concepts de développés dans ce livre.\n\n\n\n\n\n\n\nImportant\n\n\n\nAvant d’aller plus loin, rendez-vous sur le site de DataCamp, créez-vous un compte gratuit, et reprenez la lecture de ce livre."
  },
  {
    "objectID": "01-RBasics.html#que-sont-r-et-rstudio",
    "href": "01-RBasics.html#que-sont-r-et-rstudio",
    "title": "1  R et RStudio : les bases",
    "section": "1.2 Que sont R et RStudio ?",
    "text": "1.2 Que sont R et RStudio ?\nPour l’ensemble de ces TP, j’attends de vous que vous utilisiez R via RStudio. Les utilisateurs novices confondent souvent les deux. Pour tenter une analogie simple :\n\nR est le moteur d’une voiture\nRStudio est l’habitacle, le tableau de bord, les pédales…\n\nSi vous n’avez pas de moteur, vous n’irez nulle part. En revanche, un moteur sans tableau de bord est difficile à manœuvrer. Il est en effet beaucoup plus simple de faire avancer une voiture depuis l’habitacle, plutôt qu’en actionnant à la main les câbles et leviers du moteur.\nEn l’occurrence, R est un langage de programmation capable de produire des graphiques et de réaliser des analyses statistiques, des plus simples aux plus complexes. RStudio est un “emballage” qui rend l’utilisation de R plus aisée. RStudio est ce qu’on appelle un IDE ou “Integrated Development Environment”. On peut utiliser R sans RStudio, mais c’est nettement plus compliqué, nettement moins pratique.\n\n1.2.1 Installation\n\n\n\n\n\n\nAvertissement\n\n\n\nSi vous travaillez exclusivement sur les ordinateurs de l’Université, vous pouvez passer cette section. En revanche, si vous souhaitez utiliser R et RStudio sur votre ordinateur personnel, alors lisez attentivement la suite !\n\n\nAvant tout, vous devez télécharger et installer R, puis RStudio, dans cet ordre :\n\nTéléchargez et installez R\n\n\nVous devez installer ce logiciel en premier.\nCliquez sur le lien de téléchargement qui correspond à votre système d’exploitation, puis, sur “base”, si vous êtes sous Windows, sur “R-4.3.1-x86_64.pkg” si vous êtes sous Mac avec processeur Intel, ou sur R-4.3.1-arm64.pkg si vous êtes sous Mac avec processeur M1 ou M2 (sous Mac, cliquez sur le Menu , puis sur “À propos de ce Mac” et regardez à la rubrique “Processeur”), et suivez les instructions.\n\n\nTéléchargez et installez RStudio\n\n\nCliquez sur “RStudio Desktop”, puis sur “Download RStudio Desktop”.\nChoisissez la version gratuite et cliquez sur le lien de téléchargement qui correspond à votre système d’exploitation.\n\n\n\n1.2.2 Utiliser R depuis RStudio\nPuisqu’il est beaucoup plus facile d’utiliser Rstudio pour interagir avec R, nous utiliserons exclusivement l’interface de RStudio. Après les installations réalisées à la Section 1.2.1, vous disposez de 2 nouveaux logiciels sur votre ordinateur. RStudio ne peut fonctionner sans R, mais nous travaillerons exclusivement dans RStudio :\n\nR, ne pas ouvrir ceci :   \nRStudio, ouvrir cela : \n\nÀ l’université, vous trouverez RStudio dans le menu Windows, à condition d’être connecté à la machine virtuelle bio. Quand vous ouvrez RStudio pour la première fois, vous devriez obtenir une fenêtre qui ressemble à ceci :\n\nPrenez le temps d’explorer cette interface, cliquez sur les différents onglets, ouvrez les menus, allez faire un tour dans les préférences du logiciel pour découvrir les différents panneaux de l’application, en particulier la Console dans laquelle nous exécuterons très bientôt du code R."
  },
  {
    "objectID": "01-RBasics.html#sec-code",
    "href": "01-RBasics.html#sec-code",
    "title": "1  R et RStudio : les bases",
    "section": "1.3 Comment exécuter du code R ?",
    "text": "1.3 Comment exécuter du code R ?\nContrairement à d’autres logiciels comme Excel, STATA ou SAS qui fournissent des interfaces où tout se fait en cliquant avec sa souris, R est un langage interprété, ce qui signifie que vous devez taper des commandes, écrites en code R. C’est-à-dire que vous devez programmer en R (j’utilise les termes “coder” et “programmer” de manière interchangeable dans ce livre).\nIl n’est pas nécessaire d’être un programmeur pour utiliser R, néanmoins, il est nécessaire de programmer ! Il existe en effet un ensemble de concepts de programmation de base que les utilisateurs de R doivent comprendre et maîtriser. Et progresser en programmation vous permettra d’automatiser de plus en plus de choses, et donc, à terme, de gagner beaucoup de temps. Par conséquent, bien que ce livre ne soit pas un livre sur la programmation, vous en apprendrez juste assez en programmation pour explorer et analyser efficacement des données.\n\n1.3.1 La console\nLa façon la plus simple d’interagir avec RStudio (mais pas du tout la meilleure !) consiste à taper directement des commandes que R pourra comprendre dans la Console.\nCliquez dans la console (après le symbole &gt;) et tapez ceci, sans oublier de valider en tapant sur la touche Entrée :\n\n3 + 8\n\n[1] 11\n\n\nFélicitations, vous venez de taper votre première instruction R : vous savez maintenant faire des additions !\nDans la version en ligne de ce livre (en html), à chaque fois que du code R sera fourni, il apparaîtra dans un cadre grisé avec une ligne bleue à gauche, comme ci-dessus. Vous pourrez toujours taper dans RStudio, les commandes qui figurent dans ces blocs de code, afin d’obtenir vous même les résultats souhaités. Dans ce livre, lorsque les commandes R produisent des résultats, ils sont affichés juste en dessous des blocs de code. Enfin, en passant la souris sur les blocs de code, vous verrez apparaître, à droite, une icône de presse-papier qui vous permettra de copier-coller les commandes du livre dans la console de RStudio ou, très bientôt, dans vos scripts.\n\n\n\n\n\n\nLes risques du “copier-coller” \n\n\n\nAttention : il est fortement conseillé de réserver les copier-coller aux blocs de commandes de (très) grande taille, ou en cas d’erreur de syntaxe inexplicable. L’expérience a en effet montré qu’on apprend beaucoup mieux en tapant soi-même les commandes. Ça n’est que comme cela que l’on peut prendre conscience de toutes les subtilités du langage (par exemple, faut-il mettre une virgule ou un point, une parenthèse ou un crochet, le symbole moins ou un tilde, etc.). Je vous conseille donc de taper vous-même les commandes autant que possible.\n\n\n\n\n1.3.2 Les scripts\nTaper du code directement dans la console est probablement la pire façon de travailler dans RStudio. Cela est parfois utile pour faire un rapide calcul, ou pour vérifier qu’une commande fonctionne correctement. Mais la plupart du temps, vous devriez taper vos commandes dans un script.\n\n\n\n\n\n\nDéfinition importante !\n\n\n\nUn script est un fichier au format “texte brut” (cela signifie qu’il n’y a pas de mise en forme et que ce fichier peut-être ouvert par n’importe quel éditeur de texte, y compris les plus simples comme le bloc notes de Windows), dans lequel vous pouvez taper :\n\ndes instructions qui seront comprises par R comme si vous les tapiez directement dans la console\ndes lignes de commentaires, qui doivent obligatoirement commencer par le symbole #.\n\n\n\nLes avantages de travailler dans un script sont nombreux :\n\nVous pouvez sauvegarder votre script à tout moment (vous devriez prendre l’habitude de le sauvegarder très régulièrement). Vous gardez ainsi la trace de toutes les commandes que vous avez tapées.\nVous pouvez aisément partager votre script pour collaborer avec vos collègues de promo et enseignants.\nVous pouvez documenter votre démarche et les différentes étapes de vos analyses. Vous devez ajouter autant de commentaires que possible. Cela permettra à vos collaborateurs de comprendre ce que vous avez fait. Et dans 6 mois, cela vous permettra de comprendre ce que vous avez fait. Si votre démarche vous paraît cohérente aujourd’hui, il n’est en effet pas garanti que vous vous souviendrez de chaque détail quand vous vous re-plongerez dans vos analyses dans quelques temps. Donc aidez-vous vous même en commentant vos scripts dès maintenant.\nUn script bien structuré, bien indenté (avec les bons retours à la ligne, des sauts de lignes, des espaces, bref, de l’air) et clair permet de rendre vos analyses répétables. Si vous passez 15 heures à analyser un tableau de données précis, il vous suffira de quelques secondes pour analyser un nouveau jeu de données similaire : vous n’aurez que quelques lignes à modifier dans votre script original pour l’appliquer à de nouvelles données.\n\nVous pouvez créer un script en cliquant dans le menu “File &gt; New File &gt; R Script”. Un nouveau panneau s’ouvre dans l’application. Pensez à sauvegarder immédiatement votre nouveau script en cliquant dans le menu “File &gt; Save” ou “File &gt; Save as…”. Il faut pour cela lui donner un nom et choisir un emplacement sur votre disque dur.\n\n\n\n\n\n\nOù sauvegarder vos scripts ? \n\n\n\nJe vous encourage vivement à créer, sur votre disque dur, un nouveau dossier spécifique, que vous nommerez par exemple Data_Analysis. Il est important que le nom de ce dossier ne contienne pas de caractères spéciaux (e.g. accents, cédilles, apostrophes, espaces, etc.). Ce dossier devrait être facilement accessible : vous y enregistrerez tous vos scripts, vos jeux de données, vos graphiques, etc.\nSi vous travaillez sur les ordinateurs de l’université, créez obligatoirement votre dossier sur le disque W:\\. Il s’agit de votre espace personnel sur le réseau de l’université. Cela vous garantit que vous retrouverez votre script la prochaine fois, même si vous utilisez un ordinateur différent.\n\n\nÀ partir de maintenant, vous ne devriez plus taper de commande directement dans la console. Tapez systématiquement vos commandes dans un script et sauvegardez-le régulièrement.\nPour exécuter les commandes du script dans la console, il suffit de placer le curseur sur la ligne contenant la commande et de presser les touches ctrl + enter (ou command + enter sous macOS). Si un message d’erreur s’affiche dans la console, c’est que votre instruction était erronée. Modifiez la directement dans votre script et pressez à nouveau les touches ctrl + enter (ou command + enter sous macOS) pour tenter à nouveau votre chance. Idéalement, votre script ne devrait contenir que des commandes qui fonctionnent et des commentaires expliquant à quoi servent ces commandes.\nVoici un exemple de script que je ne vous demande pas de reproduire. Lisez simplement attentivement son contenu :\n\n# Penser à installer le package ggplot2 si besoin\n# install.packages(\"ggplot2\")\n\n# Chargement du package\nlibrary(ggplot2)\n\n# Mise en mémoire des données de qualité de l'air à New-York de mai à\n# septembre 1973\ndata(airquality)\n\n# Affichage des premières lignes du tableau de données\nhead(airquality)\n\n# Quelle est la structure de ce tableau ?\nstr(airquality)\n\n# Réalisation d'un graphique présentant la relation entre la concentration\n# en ozone atmosphérique en ppb et la température en degrés Fahrenheit\nggplot(data = airquality, mapping = aes(x = Temp, y = Ozone)) +\n  geom_point() +\n  geom_smooth(method = \"loess\")\n\n# On constate une augmentation importante de la concentration d'ozone \n# pour des températures supérieures à 75ºF\n\nMême si vous ne comprenez pas encore les commandes qui figurent dans ce script (ça viendra !), voici ce que vous devez en retenir :\n\nLe script contient plus de lignes de commentaires que de commandes R.\nChaque étape de l’analyse est décrite en détail.\nLes 2 dernières lignes du script décrivent les résultats obtenus (ici, un graphique).\nSeules des commandes pertinentes et qui fonctionnent ont été conservées dans ce script.\nChaque ligne de commentaire commence par #. Il est ainsi possible de conserver certaines commandes R dans le script, “pour mémoire”, sans pour autant qu’elle ne soient exécutées. C’est le cas pour la ligne # install.packages(\"ggplot2\").\n\nSi j’exécute ce script dans la console de RStudio (en sélectionnant toutes les lignes et en pressant les touches ctrl + enter ou command + enter sous macOS), voilà ce qui est produit :\n\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n\n\n\n\n\n\n1.3.3 Les projets, ou Rprojects\nPour travailler le plus efficacement possible avec RStudio, vous devriez créer, à l’intérieur de votre dossier de travail, un nouveau fichier très particulier, qui s’appelle, dans le jargon de RStudio, un Rproject.\nPour le créer, cliquez simplement dans le Menu “File &gt; New Project…”. Cette boîte de dialogue devrait apparaître :\n\n\n\n\n\nChoisissez “Existing Directory”, puis, dans la boîte de dialogue suivante :\n\n\n\n\n\ncliquez sur “Browse…”, naviguez jusqu’au dossier que vous avez créé plus tôt sur votre disque dur et qui contient votre script, puis cliquez sur “Create Project”. La fenêtre de RStudio se ferme, puis une nouvelle fenêtre vierge apparaît. En apparence, rien n’a changé ou presque. Pourtant :\n\nen haut à droite de la fenêtre de RStudio, le logiciel indique maintenant que vous êtes bel et bien à l’intérieur d’un Rproject. Au lieu de Project: (None), on lit maintenant le nom du Rproject (chez moi, Data_Analysis)\ndans le quart inférieur droit de l’interface, l’onglet “Files” ne présente plus le même aspect. Avant de créer le Rproject, cet onglet présentait le chemin vers le dossier utilisé par défaut par le logiciel, ainsi que son contenu. Il s’agissait d’un dossier système auquel il vaut mieux ne pas toucher pour éviter les problèmes. Après la création du Rproject, l’onglet “Files” indique le contenu du dossier contenant le projet. Autrement dit, c’est ici que vous trouverez vos scripts, tableaux de données dans différents formats, figures sauvegardées, etc. Dans cet onglet, vous pouvez donc cliquer sur le nom de votre script pour l’ouvrir à nouveau, le modifier, l’exécuter…\n\n\n\n\n\n\n\nSans Rproject\n\n\n\n\n\n\n\nAvec RProject\n\n\n\n\n\n\nun nouveau fichier portant l’extension .Rproj a été créé dans votre dossier de travail. La prochaine fois que vous voudrez travailler dans RStudio, il vous suffira de double-cliquer sur ce fichier dans l’explorateur de fichier de Windows ou le Finder de MacOS, pour que RStudio s’ouvre, et que vous retrouviez tous vos fichiers et scripts de la fois précédente\n\n\n\n\n\n\nPour vérifier que tout s’est bien passé jusqu’ici, tapez la commande suivante dans votre script puis envoyez-la dans la console en pressant les touches ctrl + entrée (ou command + entrée sous MacOS).\n\ngetwd()\n\nRStudio doit vous afficher, dans la console, le chemin jusqu’à votre répertoire de travail ou “Working Directory” en anglais (getwd() est l’abréviation de “GET Working Directory”). Si tout s’est bien passé, ce chemin doit être celui du dossier qui contient votre script et le fichier .Rproj que vous venez de créer. Si ce n’est pas le cas, reprenez calmement toutes les étapes décrites depuis le début de la Section 1.3.2. Si ça ne fonctionne toujours pas, contactez-moi sur Slack.\n\n\n\n\n\n\nPour résumer… \n\n\n\nLes Rprojects sont un moyen très pratique de travailler efficacement dans RStudio car ils permettent de gérer facilement la question du répertoire de travail. Lorsque vous envisagez de travailler sur un nouveau sujet/projet/jeu de données/compte-rendu de TP…, les étapes à suivre, pour vous mettre dans une configuration idéale qui vous évitera bien des problèmes par la suite, sont donc les suivantes :\n\nSur votre ordinateur, créez un nouveau dossier avec un nom simple, et à un endroit facile d’accès (pas de caractères spéciaux dans le chemin du dossier si possible)\nDémarrez RStudio\nDans le logiciel, cliquez dans le menu “File &gt; New Project…”\nChoisissez “Existing Directory”, puis naviguez jusqu’au dossier que vous venez de créer\nCliquez sur “Create Project”\nCréer un nouveau script (menu “File &gt; New File &gt; R script”)\nDonnez un nom à votre script (menu “File &gt; Save As…”) pour le sauvegarder. Par défaut, RStudio vous propose d’enregistrer votre script dans le dossier de votre Rproject, ce qui est parfait.\nTapez getwd() dans votre script et exécutez cette commande en l’envoyant dans la console.\n\nSi le chemin qui s’affiche est celui du dossier contenant votre Rproject et votre script, félicitation, vous êtes prêt·e à travailler. Avec un peu d’habitude, ces étapes ne prennent qu’une à deux minutes.\n\n\n\n\n1.3.4 Concepts de base en programmation et terminologie\nAprès ces considérations techniques sur l’utilisation et les réglages de RStudio, nous entrons maintenant dans le vif du sujet avec la découverte des premiers éléments de syntaxe du langage R.\n\n1.3.4.1 Objets, types, vecteurs, facteurs et tableaux de données\nPour vous présenter les concepts de base et la terminologie de la programmation dont nous aurons besoin, vous allez suivre des tutoriels en ligne sur le site de DataCamp. Pour cette première prise en main, tout va maintenant se passer dans votre navigateur internet, et vous pouvez donc mettre de côté RStudio pour l’instant. Vous allez voir que l’interface de DataCamp ressemble à une version simplifiée de l’éditeur de script et de la console de RStudio : vous n’aurez pas à vous soucier des réglages, de Rprojects ou de sauvegarder quoi que ce soit. Si vous avez correctement créé votre compte gratuit DataCamp comme indiqué au tout début de la Chapitre 1, votre progression sera sauvegardée automatiquement. Il vous suffit de cliquer sur les liens direct ci-dessous pour démarrer les tutoriels en ligne.\nAvant de démarrer, quelques précisions :\n\npour chaque tutoriel que je vous demande de suivre, j’indique ci-dessous une liste des concepts de programmation qui sont couverts. N’hésitez pas à vous y référer (et à y revenir) tout au long du semestre si vous avez oublié certaines choses\nce tutoriel DataCamp contient 6 chapitres. Seuls les chapitres 1, 2, 4, et 5 doivent être suivis. Nous ne travaillerons pas sur les matrices ni sur les listes\nà la fin de chaque chapitre du tutoriel, revenez à ce livre en ligne pour cliquer sur le lien direct ver le chapitre suivant. Procéder ainsi vous évitera de suivre des chapitres inutiles du tutoriel, et cela vous permettra également d’éviter les demandes d’inscriptions payantes à DataCamp\n\nIl est important de noter que, bien que ces tutoriels sont d’excellentes introductions, une lecture seule, même attentive, est insuffisante pour un apprentissage en profondeur et une rétention à long terme. Il faut pour cela pratiquer et répéter. Outre les exercices demandés dans DataCamp, que vous devez effectuer directement dans votre navigateur, je vous encourage à prendre des notes, à multiplier les essais, directement dans la console de RStudio, ou, de préférence, dans un script que vous annoterez, pour vous assurer que vous avez bien compris chaque partie.\nAllez maintenant découvrir le cours d’introduction à R sur DataCamp, et cliquez sur les liens des chapitres ci-dessous. Au fur et à mesure de votre travail, notez les termes importants et ce à quoi ils font référence.\n\nChapitre 1 : introduction\n\nLa console : l’endroit où vous tapez des commandes\nLes objets : où les valeurs sont stockées, comment assigner des valeurs à des objets\nLes types de données : entiers, doubles/numériques, caractères et logiques\n\nChapitre 2 : vecteurs\n\nLes vecteurs : des collections de valeurs du même type\n\nChapitre 4 : les facteurs\n\nDes données catégorielles (et non pas numériques) représentées dans R sous forme de factors\n\nChapitre 5 : les jeux de données ou data.frame\n\nLes data.frames sont similaires aux feuilles de calcul rectangulaires que l’on peut produire dans un tableur. Dans R, ce sont des objets rectangulaires (des tableaux !) contenant des jeux de données : les lignes correspondent aux observations et les colonnes aux variables décrivant les observations. La plupart du temps, c’est le format de données que nous utiliserons. Plus de détails dans le Chapitre 2\n\n\nAvant de passer à la suite, il nous reste 2 grandes notions à découvrir dans le domaine du code et de la syntaxe afin de pouvoir travailler efficacement dans R : les opérateurs de comparaison d’une part, et les fonctions d’autre part. Pour les découvrir et expérimenter, et puisque vous avez terminé les tutoriels DataCamp, reprenez maintenant RStudio et travaillez dans votre script.\n\n\n1.3.4.2 Opérateurs de comparaison\nComme leur nom l’indique, ils permettent de comparer des valeurs ou des objets. Les principaux opérateurs de comparaison sont :\n\n== : égal à\n!= : différent de\n&gt; : supérieur à\n&lt; : inférieur à\n&gt;= : supérieur ou égal à\n&lt;= : inférieur ou égal à\n\nAinsi, on peut tester si 3 est égal à 5 :\n\n3 == 5\n\n[1] FALSE\n\n\nLa réponse est bien entendu FALSE. Est-ce que 3 est inférieur à 5 ?\n\n3 &lt; 5\n\n[1] TRUE\n\n\nLa réponse est maintenant TRUE. Lorsque l’on utilise un opérateur de comparaison, la réponse est toujours soit vrai (TRUE), soit faux (FALSE).\nIl est aussi possible de comparer des chaînes de charactères :\n\n\"Bonjour\" == \"Au revoir\"\n\n[1] FALSE\n\n\"Bonjour\" &gt;= \"Au revoir\"\n\n[1] TRUE\n\n\nManifestement, “Bonjour” est supérieur ou égal à “Au revoir”. En fait, R utilise l’ordre alphabétique pour comparer les chaînes de caractères. Puisque dans l’alphabet, le “B” de “Bonjour” arrive après le “A” de “Au revoir”, pour R, “Bonjour” est supérieur à “Au revoir”.\nIl est également possible d’utiliser ces opérateurs pour comparer un chiffre et un vecteur :\n\ntailles_pop1 &lt;- c(112, 28, 86, 14, 154, 73, 63, 48)\ntailles_pop1 &gt; 80\n\n[1]  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE\n\n\nIci, l’opérateur nous permet d’identifier quels éléments du vecteur taille_pop1 sont supérieurs à 80. Il s’agit des éléments placés en première, troisième et cinquième positions.\nIl est aussi possible de comparer 2 vecteurs qui contiennent le même nombre d’éléments :\n\ntailles_pop2 &lt;- c(114, 27, 38, 91, 54, 83, 33, 68)\ntailles_pop1 &gt; tailles_pop2\n\n[1] FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n\n\nLes comparaisons sont ici faites élément par élément. Ainsi, les observations 2, 3, 5 et 7 du vecteur tailles_pop1 sont supérieures aux observations 2, 3, 5 et 7 du vecteur tailles_pop2 respectivement.\nCes vecteurs de vrais/faux sont très utiles car ils peuvent permettre de compter le nombre d’éléments répondant à une certains condition :\n\nsum(tailles_pop1 &gt; tailles_pop2)\n\n[1] 4\n\n\nLorsque l’on effectue une opération arithmétique (comme le calcul d’une somme ou d’une moyenne) sur un vecteur de vrais/faux, les TRUE sont remplacés par 1 et les FALSE par 0. La somme nous indique donc le nombre de vrais dans un vecteur de vrais/faux, et la moyenne nous indique la proportion de vrais :\n\nmean(tailles_pop1 &gt; tailles_pop2)\n\n[1] 0.5\n\n\nNote : Attention, si les vecteurs comparés n’ont pas la même taille, un message d’avertissement est affiché :\n\ntailles_pop3 &lt;- c(43, 56, 92)\ntailles_pop1\n\n[1] 112  28  86  14 154  73  63  48\n\ntailles_pop3\n\n[1] 43 56 92\n\ntailles_pop3 &gt; tailles_pop1\n\nWarning in tailles_pop3 &gt; tailles_pop1: la taille d'un objet plus long n'est\npas multiple de la taille d'un objet plus court\n\n\n[1] FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE\n\n\nIci, R renvoie un résultat, accompagné d’un message d’avertissement qui nous indique que tout ne s’est probablement pas déroulé comme on le pensait. Dans un cas comme celui là, R va en effet recycler l’objet le plus court, ici tailles_pop3 pour qu’une comparaison puisse être faite avec chaque élément de l’objet le plus long (ici, tailles_pop1). Ainsi, 43 est comparé à 112, 56 est comparé à 28 et 92 est comparé à 86. Puisque tailles_pop3 ne contient plus d’éléments, ils sont recyclés, dans le même ordre : 43 est comparé à 14, 56 est comparé à 154, et ainsi de suite jusqu’à ce que tous les éléments de tailles_pop1 aient été passés en revue.\nCe type de recyclage est très risqué car il est difficile de savoir ce qui a été comparé avec quoi. En travaillant avec des tableaux plutôt qu’avec des vecteurs, le problème est généralement évité puisque toutes les colonnes d’un data.frame contiennent le même nombre d’éléments.\n\n\n\n\n\n\nErreur ou avertissement ?  ou  ?\n\n\n\nIl ne faut pas confondre message d’erreur et message d’avertissement :\n\nUn message d’erreur commence généralement par Error ou Erreur et indique que R n’a pas compris ce que vous lui demandiez. Il n’a donc pas été en mesure de faire quoi que ce soit et votre commande n’a donc pas été exécutée. Vous devez absolument revenir à votre code et corriger la commande fautive car il y a fort à parier que si vous ne le faites pas, les commandes suivantes renverrons à leur tour un message d’erreur. Il est donc important de toujours revenir à la première erreur d’un script et de la corriger avant de passer à la suite.\nUn message d’avertissement commence généralement par Warning et vous indique que quelque chose d’inhabituel, ou de “non-optimal” a été réalisé. Un résultat a été produit, mais peut-être n’est-il pas conforme à ce que vous attendiez. La prudence est donc requise.\n\nDans les deux cas, un message explique de façon plus ou moins claire ce qui a posé problème. Progresser dans la maîtrise du logiciel et du langage signifie en grande partie progresser dans la compréhension de la signification de ces messages parfois obscures. Pour progresser, il faut donc commencer par lire attentivement ces messages, et tenter de comprendre ce qu’ils veulent dire.\n\n\nDernière chose concernant les opérateurs de comparaison : la question des données manquantes. Dans R les données manquantes sont symbolisées par cette notation : NA, abréviation de “Not Available”. Le symbole NaN (comme “Not a Number”) est parfois aussi observé lorsque des opérations ont conduit à des indéterminations. Mais c’est plus rare et la plupart du temps, les NaNs peuvent être traités comme les NAs. L’un des problèmes des données manquantes est qu’il est nécessaire de prendre des précautions pour réaliser des comparaisons les impliquant :\n\n3 == NA\n\n[1] NA\n\n\nOn s’attend logiquement à ce que 3 ne soit pas considéré comme égal à NA, et donc, on s’attend à obtenir FALSE. Pourtant, le résultat est NA. La comparaison d’un élément quelconque à une donnée manquante fournit toujours une donnée manquante : la comparaison ne peut pas se faire, R n’a donc rien à retourner. C’est également le cas aussi lorsque l’on compare deux valeurs manquantes :\n\nNA == NA\n\n[1] NA\n\n\nC’est en fait assez logique. Imaginons que j’ignore l’âge de Pierre et l’âge de Marie. Il n’y a aucune raison pour que leur âge soit le même, mais il est tout à fait possible qu’il le soit. C’est impossible à déterminer :\n\nage_Pierre &lt;- NA\nage_Marie &lt;- NA\nage_Pierre == age_Marie\n\n[1] NA\n\n\nMais alors comment faire pour savoir si une valeur est manquante puisqu’on ne peut pas utiliser les opérateurs de comparaison ? On utilise la fonction is.na() :\n\nis.na(age_Pierre)\n\n[1] TRUE\n\nis.na(tailles_pop3)\n\n[1] FALSE FALSE FALSE\n\n\nD’une façon générale, le point d’exclamation permet de signifier à R que nous souhaitons obtenir le contraire d’une expression :\n\n!is.na(age_Pierre)\n\n[1] FALSE\n\n!is.na(tailles_pop3)\n\n[1] TRUE TRUE TRUE\n\n\nCette fonction nous sera très utile plus tard pour éliminer toutes les lignes d’un tableau contenant des valeurs manquantes.\n\n\n1.3.4.3 L’utilisation des fonctions\nDans R, les fonctions sont des objets particuliers qui permettent d’effectuer des tâches très variées. Du calcul d’une moyenne à la création d’un graphique, en passant par la réalisation d’analyses statistiques complexes ou simplement l’affichage du chemin du répertoire de travail, tout, dans R, repose sur l’utilisation de fonctions. Vous en avez déjà vu un certain nombre :\n\n\n\n\n\n\n\nFonction\nPour quoi faire ?\n\n\n\n\nc()\nCréer des vecteurs\n\n\nclass()\nAfficher ou modifier la classe d’un objet\n\n\nfactor()\nCréer des facteurs\n\n\ngetwd()\nAfficher le chemin du répertoire de travail\n\n\nhead()\nAfficher les premiers éléments d’un objet\n\n\nis.na()\nTester si un objet contient des valeurs manquantes\n\n\nmean()\nCalculer une moyenne\n\n\nnames()\nAfficher ou modifier le nom des éléments d’un vecteur\n\n\norder()\nOrdonner les éléments d’un objet\n\n\nsubset()\nExtraire une partie des éléments d’un objet\n\n\nsum()\nCalculer une somme\n\n\ntail()\nAfficher les derniers éléments d’un objet\n\n\n\nCette liste va très rapidement s’allonger au fil des séances. Je vous conseille donc vivement de tenir à jour une liste des fonctions décrites, avec une explication de leur fonctionnement et éventuellement un exemple de syntaxe.\nCertaines fonctions ont besoin d’arguments (par exemple, la fonction factor()), d’autres peuvent s’en passer (par exemple, la fonction getwd()). Pour apprendre comment utiliser une fonction particulière, pour découvrir quels sont ses arguments possibles, quel est leur rôle et leur intérêt, la meilleure solution est de consulter l’aide de cette fonction. Il suffit pour cela de taper un ? suivi du nom de la fonction :\n\n?factor()\n\nToutes les fonctions et jeux de données disponibles dans R disposent d’un fichier d’aide similaire. Cela peut faire un peu peur au premier abord (tout est en anglais !), mais ces fichiers d’aide ont l’avantage d’être très complets, de fournir des exemples d’utilisation, et ils sont tous construits sur le même modèle. Vous avez donc tout intérêt à vous familiariser avec eux. Vous devriez d’ailleurs prendre l’habitude de consulter l’aide de chaque fonction qui vous pose un problème. Par exemple, le logarithme (en base 10) de 100 devrait faire 2, car 100 est égal à 10^2. Pourtant :\n\nlog(100)\n\n[1] 4.60517\n\n\nQue se passe-t’il ? Pour le savoir, il faut consulter l’aide de la fonction log :\n\n?log()\n\nCe fichier d’aide nous apprend que par défaut, la syntaxe de la fonction log() est la suivante :\n\nlog(x, base = exp(1))\n\nPar défaut, la base du logarithme est fixée à exp(1). Nous avons donc calculé un logarithme népérien (en base e). Cette fonction prend donc 2 arguments :\n\nx ne possède pas de valeur par défaut : il nous faut obligatoirement fournir quelque chose (la rubrique “Argument” du fichier d’aide nous indique que x doit être un vecteur numérique ou complexe) afin que la fonction puisse calculer un logarithme\nbase possède un argument par défaut. Si nous ne spécifions pas nous même la valeur de base, elle sera fixée à sa valeur par défaut, c’est à dire exp(1).\n\nPour calculer le logarithme de 100 en base 10, il faut donc taper, au choix, l’une de ces 3 expressions :\n\nlog(x = 100, base = 10)\n\n[1] 2\n\nlog(100, base = 10)\n\n[1] 2\n\nlog(100, 10)\n\n[1] 2\n\n\nLe nom des arguments d’une fonction peut être omis tant que ses arguments sont indiqués dans l’ordre attendu par la fonction (cet ordre est celui qui est précisé à la rubrique “Usage” du fichier d’aide de la fonction). Il est possible de modifier l’ordre des arguments d’une fonction, mais il faut alors être parfaitement explicite et utiliser les noms des arguments tels que définis dans le fichier d’aide.\nAinsi, pour calculer le logarithme de 100 en base 10, on ne peut pas taper :\n\nlog(10, 100)\n\n[1] 0.5\n\n\ncar cela revient à calculer le logarithme de 10 en base 100. On peut en revanche taper :\n\nlog(base = 10, x = 100)\n\n[1] 2"
  },
  {
    "objectID": "01-RBasics.html#sec-packages",
    "href": "01-RBasics.html#sec-packages",
    "title": "1  R et RStudio : les bases",
    "section": "1.4 Les packages additionels",
    "text": "1.4 Les packages additionels\nUne source de confusion importante pour les nouveaux utilisateurs de R est la notion de package. Les packages étendent les fonctionnalités de R en fournissant des fonctions, des données et de la documentation supplémentaires et peuvent être téléchargés gratuitement sur Internet. Ils sont écrits par une communauté mondiale d’utilisateurs de R. Par exemple, parmi les plus de 18000 packages disponibles à l’heure actuelle, nous utiliseront fréquemment :\n\nLe package ggplot2 pour la visualisation des données dans le Chapitre 3\nLe package dplyr pour manipuler des tableaux de données dans le Chapitre 5\n\nUne bonne analogie pour les packages R : ils sont comme les apps que vous téléchargez sur un téléphone portable. R est comme un nouveau téléphone mobile. Il est capable de faire certaines choses lorsque vous l’utilisez pour la première fois, mais il ne sait pas tout faire. Les packages sont comme les apps que vous pouvez télécharger dans l’App Store et Google Play. Pour utiliser un package, comme pour utiliser Instagram, vous devez :\n\nLe télécharger et l’installer. Vous ne le faites qu’une fois grâce à la commande install.packages()\nLe charger (en d’autres termes, l’ouvrir) en utilisant la commande library() à chaque nouvelle session de travail\n\nDonc, tout comme vous ne pouvez commencer à partager des photos avec vos amis sur Instagram que si vous installez d’abord l’application et que vous l’ouvrez, vous ne pouvez accéder aux données et fonctions d’un package R que si vous installez d’abord le package et le chargez avec la fonction library(). Passons en revue ces 2 étapes.\n\n1.4.1 Installation d’un package\nIl y a deux façons d’installer un package. Par example, pour installer le package ggplot2 :\n\nLe plus simple : Dans le quart inférieur droit de l’interface de Rstudio :\n\nCliquez sur l’onglet “Packages”\nCliquez sur “Install”\nTapez le nom du package dans le champ “Packages (separate multiple with space or comma):” Pour notre exemple, tapez ggplot2\nCliquez sur “Install”\n\nMétode alternative : Dans la console, tapez install.packages(\"ggplot2\") (vous devez inclure les guillemets).\n\nEn procédant de l’une ou l’autre façon, installez également les packages suivants : tidyverse et palmerpenguins. Le tidyverse est un “méta-package”, qui permet en fait d’installer de nombreux packages en une seule commande, dont ggplot2, tidyr, dplyr, magrittr et bien d’autres. Le package palmerpenguins contient un jeu de données dont nous nous servirons copieusement dans les chapitres suivants.\n\n\n\n\n\n\nNote : install.packages()\n\n\n\nUn package doit être installé une fois seulement sur un ordinateur, sauf si une version plus récente est disponible et que vous souhaitez mettre à jour ce package. Il n’est donc pas nécessaire de laisser ces commandes dans votre script. Sinon, vous risquez de ré-installer les packages à chaque nouvelle session de travail, ce qui est inutile et consomme inutilement de la bande passante, des ressources numériques, et donc, du carbone…  \n\n\n\n\n1.4.2 Charger un package en mémoire\nAprès avoir installé un package, vous pouvez le charger en utilisant la fonction library(). Par exemple, pour charger ggplot2 et dplyr tapez ceci dans la console :\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nPuisque ces packages font partie du tidyverse, on aurait pu les charger tous les deux (et d’autres) en une seule étape en tapant :\n\nlibrary(tidyverse)\n\nQuand vous exécutez une commande, si vous voyez un message d’erreur commençant par :\nError: could not find function...\nc’est probablement parce que vous tentez d’utiliser une fonction qui fait partie d’un package que vous n’avez pas chargé. Pour corriger l’erreur, il suffit donc de charger le package approprié avec la commande library().\n\n\n\n\n\n\nNote : library()\n\n\n\nVous devrez charger à nouveau chaque package que vous souhaitez utiliser à chaque fois que vous ouvrirez une nouvelle session de travail dans RStudio (à chaque nouveau démarrage du logiciel, donc). C’est une erreur fréquente pour les débutants. Pour l’éviter, pensez bien à intégrer, tout en haut de votre script, les commandes library() nécessaires pour chaque package que vous comptez utiliser."
  },
  {
    "objectID": "01-RBasics.html#sec-exo-1",
    "href": "01-RBasics.html#sec-exo-1",
    "title": "1  R et RStudio : les bases",
    "section": "1.5 Exercice",
    "text": "1.5 Exercice\nDans votre dossier de travail, créez un nouveau script que vous nommerez ExoDiamonds.R. Vous prendrez soin d’ajouter autant de commentaires que nécessaire dans votre script afin de le structurer correctement.\n\nTéléchargez (si besoin) et chargez le package ggplot2\nChargez le jeu de données diamonds grâce à la commande data(diamonds)\nDéterminez le nombre de lignes et de colonnes de ce tableau nommé diamonds\nCréez un nouveau tableau que vous nommerez diamants_chers qui contiendra uniquement les informations des diamants dont le prix est supérieur ou égal à $15000.\nCombien de diamants coûtent $15000 ou plus ?\nCela représente quelle proportion du jeu de données de départ ?\nTriez ce tableau par ordre de prix décroissants et affichez les informations des 20 diamants les plus chers."
  },
  {
    "objectID": "02-Dataset.html#préambule",
    "href": "02-Dataset.html#préambule",
    "title": "2  Explorez votre premier jeu de données",
    "section": "2.1 Préambule",
    "text": "2.1 Préambule\nMettons en pratique tout ce que nous avons appris pour commencer à explorer un jeu de données réel. Les données nous parviennent sous différents formats, des images au texte en passant par des tableaux de chiffres. Tout au long de ce document, nous nous concentrerons sur les ensembles de données qui peuvent être stockés dans une feuille de calcul, car il s’agit de la manière la plus courante de collecter des données dans de nombreux domaines. N’oubliez pas ce que nous avons appris dans la Section 1.3.4.1 : ces ensembles de données de type “tableurs” sont appelés data.frame dans R, et nous nous concentrerons sur l’utilisation de ces objets tout au long de ce livre. S’il est évidemment possible d’importer dans R des données stockées dans des fichiers Excel ou des fichiers textes, nous allons dans un premier temps faire plus simple : nous travaillerons avec des données déjà disponibles dans un packages que nous avons installé dans la Section 1.4.\nAinsi, commençons par charger les packages nécessaires pour ce chapitre (cela suppose que vous les ayez déjà installés ; relisez la Section 1.4 pour plus d’informations sur l’installation et le chargement des packages R si vous ne l’avez pas déjà fait). Au début de chaque chapitre, nous aurons systématiquement besoin de charger quelques packages. Donc n’oubliez pas de les installer au préalable si besoin.\n\n# Pensez à installer ces packages avant de les charger si besoin \nlibrary(dplyr)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "02-Dataset.html#le-package-palmerpenguins",
    "href": "02-Dataset.html#le-package-palmerpenguins",
    "title": "2  Explorez votre premier jeu de données",
    "section": "2.2 Le package palmerpenguins",
    "text": "2.2 Le package palmerpenguins\nCe package (Horst, Hill, et Gorman 2022) contient un jeu de données collectées par Kristen Gorman (membre du ``Long Term Ecological Research Network’’) et la station de Palmer en Antarctique (Gorman, Williams, et Fraser 2014). Les données contiennent des informations au sujet de 330 individus appartenant à 3 espèces de manchots (voir Figure 2.1) étudiés sur 3 îles de l’archipel de Palmer, an Antarctique. Ces espèces ont fait l’objet de nombreuses études comparatives, notamment afin de déterminer comment elles utilisent le milieu pour acquérir des ressources. Puisque ces 3 espèces sont proches sur le plan phylogénétique et qu’elles occupent le même habitat, la question de la compétition inter-spécifique, pour l’espace et les ressources, se pose tout naturellement.\n\n\n\nFigure 2.1: Les 3 espèces de manchots de l’archipel de Palmer. Illustration : Allison Horst"
  },
  {
    "objectID": "02-Dataset.html#le-data-frame-penguins",
    "href": "02-Dataset.html#le-data-frame-penguins",
    "title": "2  Explorez votre premier jeu de données",
    "section": "2.3 Le data frame penguins",
    "text": "2.3 Le data frame penguins\nNous allons commencer par explorer le jeu de données penguins qui est inclus avec le package palmerpenguins afin de nous faire une idée de sa structure. Dans votre script, tapez la commande suivante et exécutez la dans la console (selon les réglages de RStudio et la largeur de votre console, l’affichage peut varier légèrement) :\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nEssayons de décrypter cet affichage :\n\nA tibble: 344 x 8 : un tibble est un data.frame amélioré. Il a toutes les caractéristiques d’un data.frame, (tapez class(penguins) pour vous en convaincre), mais en plus, il a quelques propriétés intéressantes sur lesquelles nous reviendrons plus tard. Ce tibble possède donc :\n\n344 lignes\n8 colonnes, qui correspondent aux variables. Dans un tibble, les observations sont toujours en lignes et les variables en colonnes\n\nspecies, island, bill_length_mm, bill_depth_mm, flipper_length_mm… sont les noms des colonnes, c’est à dire les variables de ce jeu de données\nNous avons ensuite les 10 premières lignes du tableau\n... with 334 more rows, and abbreviated variable names..., nous indique que 334 lignes ne logent pas à l’écran et que le nom de certains variables a été abrégé afin de permettre un affichage plus clair. Ces données font toutefois partie intégrante du tableau penguins\nles noms complets de toutes les variables abrégées sont également indiqués\n\nCette façon d’afficher les tableaux est spécifique des tibbles. Vous noterez que le type de chaque variable est indiqué entre &lt;...&gt;, juste sous les noms de colonnes. Voici certains des types de données que vous pourrez rencontrer :\n\n&lt;int&gt; : nombres entiers (“integers”)\n&lt;dbl&gt; : nombres réels (“doubles”)\n&lt;chr&gt; : caractères (“characters”)\n&lt;fct&gt; : facteurs (“factors”)\n&lt;ord&gt; : facteurs ordonnés (“ordinals”)\n&lt;lgl&gt; : logiques (colonne de vrais/faux : “logical”)\n&lt;date&gt; : dates\n&lt;time&gt; : heures\n&lt;dttm&gt; : combinaison de date et d’heure (“date time”)\n\nCette façon d’afficher le contenu d’un tableau permet d’y voir (beaucoup) plus clair que l’affichage classique d’un data.frame. Malheureusement, ce n’est pas toujours suffisant. Voyons quelles sont les autres méthodes permettant d’explorer un data.frame."
  },
  {
    "objectID": "02-Dataset.html#explorer-un-data.frame",
    "href": "02-Dataset.html#explorer-un-data.frame",
    "title": "2  Explorez votre premier jeu de données",
    "section": "2.4 Explorer un data.frame",
    "text": "2.4 Explorer un data.frame\nParmi les nombreuses façons d’avoir une idée des données contenues dans un data.frame tel que penguins, on présente ici 3 fonctions qui prennent le nom du data.frame en guise d’argument, et un opérateur :\n\nla fonction View() intégrée à RStudio. C’est celle que vous utiliserez le plus souvent. Attention, elle s’écrit avec un “V” majuscule\nla fonction glimpse() chargée avec le package dplyr. Elle est très similaire à la fonction str() découverte dans les tutoriels de DataCamp\nl’opérateur $ permet d’accéder à une unique variable d’un data.frame\nla fonction skim() du package skimr permet d’obtenir un résumé complet mais très synthétique et visuel des variables d’un data.frame\n\n\n2.4.1 View()\nTapez View(penguins) dans votre script et exécutez la commande. Un nouvel onglet contenant ce qui ressemble à un tableur doit s’ouvrir.\n\n\n\n\n\n\nQuizz : à quoi correspondent chacune des lignes de ce tableau ?\n\n\n\n\naux données d’une espèce\naux données d’une île\naux données d’un individu\naux données d’une population (plusieurs manchots à la fois)\n\n\n\nIci, vous pouvez donc explorer la totalité du tableau, passer chaque variable en revue, et même appliquer des filtres pour ne visualiser qu’une partie des données. Par exemple, essayez de déterminer combien d’individus sont issus de l’île “Biscoe”.\nCe tableau n’est pas facile à manipuler. Il est impossible de corriger des valeurs, et lorsque l’on applique des filtres, il est impossible de récupérer uniquement les données filtrées. Nous verrons plus tard comment les obtenir en tapant des commandes simples dans un script. La seule utilité de ce tableau est donc l’exploration visuelle des données.\n\n\n2.4.2 glimpse()\nLa seconde façon d’explorer les données contenues dans un tableau est d’utiliser la fonction glimpse() après avoir chargé le package dplyr :\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nIci, les premières observations sont présentées en lignes pour chaque variable du jeu de données. Là encore, le type de chaque variable est précisé. Essayez d’identifier 3 variables catégorielles. À quoi correspondent-elles ? En quoi sont-elles différentes des variables numériques ?\n\n\n2.4.3 L’opérateur $\nL’opérateur $ permet d’accéder à une unique variable grâce à son nom. Par exemple on peut accéder à toutes les données concernant les noms d’espèces (variable species du tableau penguins) en tapant :\n\npenguins$species\n\n  [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n  [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo   \n[155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap\n[281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[344] Chinstrap\nLevels: Adelie Chinstrap Gentoo\n\n\nCela nous permet de récupérer les données sous la forme d’un vecteur ou, comme ici, d’un facteur. Attention toutefois, le tableau penguins contient beaucoup de lignes. Récupérer une variable grâce à cet opérateur peut rapidement saturer la console. Nous serons amenés à manipuler des tableaux contenant plusieurs dizaines ou centaines de milliers de lignes. C’est le cas du tableau diamonds du package ggplot2 que vous avez découvert dans les exercice de la Section 1.5.\nSi, par exemple, vous souhaitez extraire les données relatives à la clarté des diamants (colonne clarity) du tableau diamonds, vous pouvez taper ceci :\n\nlibrary(ggplot2)\ndiamonds$clarity\n\nLe résultat est pour le moins indigeste ! Lorsqu’un tableau contient de nombreuses lignes, c’est rarement une bonne idée de transformer l’une de ses colonnes en vecteur. Dans la mesure du possible, les données d’un tableau doivent rester dans le tableau.\n\n\n2.4.4 skim()\nPour utiliser la fonction skim(), vous devez au préalable installer le package skimr :\n\ninstall.packages(\"skimr\")\n\nCe package est un peu “expérimental” et il se peut que l’installation pose problème. Si un message d’erreur apparaît lors de l’installation, procédez comme suit :\n\nQuittez RStudio (sans oublier de sauvegarder votre travail au préalable)\nRelancez RStudio et dans la console, tapez ceci :\n\n\ninstall.packages(\"rlang\")\n\n\nTentez d’installer skimr à nouveau.\nExécutez à nouveau tout votre script afin de retrouver votre travail dans l’état où il était avant de quitter RStudio.\n\nSi l’installation de skimr s’est bien passée, vous pouvez maintenant taper ceci :\nlibrary(skimr)\nskim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\nNous aurons l’occasion de revenir en détail sur la signification de tous ces indices à la Section 6.4. À ce stade, retenez que cette fonction skim() permet d’accéder à un résumé très détaillé de chaque variable d’un jeu de données. Par exemple, on apprend ici que la masse corporelle moyenne des manchots de l’ensemble du jeu de données vaut 4201.75 grammes (ligne body_mass_g, colonne mean), avec un écart-type de 0.82 grammes (colonne sd), et que la masse de 2 individus est manquante (colonne n_missing). Cette fonction nous sera donc très utile lorsque nous aborderons la question des statistiques descriptives.\n\n\n2.4.5 Les fichiers d’aide\nUne fonctionnalité particulièrement utile de R est son système d’aide. On peut obtenir de l’aide au sujet de n’importe quelle fonction et de n’importe quel jeu de données en tapant un “?” immédiatement suivi du nom de la fonction ou de l’objet.\nPar exemple, examinez l’aide du jeu de données penguins :\n\n?penguins\n\nVous devriez absolument prendre l’habitude d’examiner les fichiers d’aide des fonctions ou jeux de données pour lesquels vous avez des questions. Ces fichiers sont très complets, et même s’il peuvent paraître impressionnants au premier abord, ils sont tous structurés sur le même modèle et vous aideront à comprendre comment utiliser les fonctions, quels sont les arguments possibles, à quoi ils servent et comment les utiliser.\nPrenez le temps d’examiner le fichier d’aide du jeu de données penguins. Avant de passer à la suite, assurez-vous d’avoir compris à quoi correspondent chacune des 8 variables de ce tableau."
  },
  {
    "objectID": "02-Dataset.html#sec-exo-2",
    "href": "02-Dataset.html#sec-exo-2",
    "title": "2  Explorez votre premier jeu de données",
    "section": "2.5 Exercices",
    "text": "2.5 Exercices\nConsultez l’aide du jeu de données diamonds du package ggplot2.\n\nQuel est le code de la couleur la plus prisée ?\nQuel est le code de la moins bonne clarté ?\nÀ quoi correspond la variable z ?\nEn quoi la variable depth est-elle différente de la variable z ?\n\nInstallez le package nycflights13 et consultez son aide en tapant help(package=\"nycflights13\").\n\nConsultez l’aide des 5 jeux de données de ce package.\nÀ quoi correspond la variable visib ?\nDans quel tableau se trouve-t-elle ?\nCombien de lignes possède ce tableau ?\n\n\n\n\n\nGorman, Kristen B., Tony D. Williams, et William R Fraser. 2014. « Ecological Sexual Dimorphism and Environmental Variability within a Community of Antarctic Penguins (Genus Pygoscelis) ». PLOS ONE 9 (mars): 1‑14. https://doi.org/10.1371/journal.pone.0090081.\n\n\nHorst, Allison, Alison Hill, et Kristen Gorman. 2022. palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://CRAN.R-project.org/package=palmerpenguins."
  },
  {
    "objectID": "03-Visualization.html#préambule",
    "href": "03-Visualization.html#préambule",
    "title": "3  Visualiser des données avec ggplot2",
    "section": "3.1 Préambule",
    "text": "3.1 Préambule\nDans le Chapitre 1 et le Chapitre 2, vous avez découvert les concepts essentiels qu’il est important de maîtriser avant de commencer à explorer en détail des données dans R. Les éléments de syntaxe abordés dans la Section 1.3 sont nombreux et vous n’avez probablement pas tout retenu. C’est pourquoi je vous conseille de garder les tutoriels de DataCamp à portée de main afin de pouvoir refaire les parties que vous maîtrisez le moins. Ce n’est qu’en répétant plusieurs fois ces tutoriels que les choses seront vraiment comprises et que vous les retiendrez. Ainsi, si des éléments de code présentés ci-dessous vous semblent obscurs, revenez en arrière : toutes les réponses à vos questions se trouvent probablement dans les chapitres précédents.\nAprès la découverte des bases du langage R, nous abordons maintenant les parties de ce livre qui concernent la “science des données” (ou “Data Science” pour nos amis anglo-saxons). Nous allons voir dans ce chapitre qu’outre les fonctions View() et glimpse(), l’exploration visuelle via la représentation graphique des données est un moyen indispensable et très puissant pour comprendre ce qui se passe dans un jeu de données.\n\n\n\n\n\n\nImportant\n\n\n\nLa visualisation de vos données est un préalable indispensable à toute analyse statistique.\n\n\nLa visualisation des données est en outre un excellent point de départ quand on découvre la programmation sous R, car ses bénéfices sont clairs et immédiats : vous pouvez créer des graphiques élégants et informatifs qui vous aident à comprendre les données. Dans ce chapitre, vous allez donc plonger dans l’art de la visualisation des données, en apprenant la structure de base des graphiques réalisés avec ggplot2 qui permettent de transformer des données numériques et catégorielles en graphiques.\nToutefois, la visualisation seule ne suffit généralement pas. Il est en effet souvent nécessaire de transformer les données pour produire des représentations plus parlantes. Ainsi, dans le Chapitre 5, vous découvrirez les fonctions clés qui vous permettront de sélectionner des variables importantes, de filtrer des observations, de créer de nouvelles variables, ou d’en modifier la forme.\nCe n’est qu’en combinant les transformations de données et représentations graphiques d’une part, avec votre curiosité et votre esprit critique d’autre part, que vous serez véritablement en mesure de réaliser une analyse exploratoire de vos données à la fois utile et pertinente. C’est la seule façon d’identifier des questions intéressantes sur vos données, afin de tenter d’y répondre par les analyses statistiques et la modélisation."
  },
  {
    "objectID": "03-Visualization.html#prérequis",
    "href": "03-Visualization.html#prérequis",
    "title": "3  Visualiser des données avec ggplot2",
    "section": "3.2 Prérequis",
    "text": "3.2 Prérequis\nDans ce chapitre, nous aurons besoin des packages suivants :\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(nycflights13)\nlibrary(gapminder)\nlibrary(scales)\n\nSi ce n’est pas déjà fait, pensez à les installer avant de les charger en mémoire.\nAu niveau le plus élémentaire, les graphiques permettent de comprendre comment les variables se comparent en termes de tendance centrale (à quel endroit les valeurs ont tendance à être localisées, regroupées) et leur dispersion (comment les données varient autour du centre). La chose la plus importante à savoir sur les graphiques est qu’ils doivent être créés pour que votre public (le professeur qui vous évalue, le collègue avec qui vous collaborez, votre futur employeur, etc.) comprenne bien les résultats et les informations que vous souhaitez transmettre. Il s’agit d’un exercice d’équilibriste : d’une part, vous voulez mettre en évidence autant de relations significatives et de résultats intéressants que possible, mais de l’autre, vous ne voulez pas trop en inclure, afin d’éviter de rendre votre graphique illisible ou de submerger votre public. Tout comme n’importe quel paragraphe de document écrit, un graphique doit permettre de communiquer un message (une idée forte, un résultat marquant, une hypothèse nouvelle, etc).\nComme nous le verrons, les graphiques nous aident également à repérer les tendances extrêmes et les valeurs aberrantes dans nos données. Nous verrons aussi qu’une façon de faire, assez classique, consiste à comparer la distribution d’une variable quantitative pour les différents niveaux d’une variable catégorielle.\n\n\n\n\n\n\nObjectifs\n\n\n\nDans ce chapitre, vous apprendrez à :\n\nfaire différents types de graphiques exploratoires avec le package ggplot2   \nchoisir le ou les graphiques appropriés selon la nature des variables dont vous disposez ou que vous souhaitez mettre en relation\nmettre vos graphiques en forme pour les intégrer dans vos rapports ou compte-rendus de TP"
  },
  {
    "objectID": "03-Visualization.html#sec-gggraph",
    "href": "03-Visualization.html#sec-gggraph",
    "title": "3  Visualiser des données avec ggplot2",
    "section": "3.3 La grammaire des graphiques",
    "text": "3.3 La grammaire des graphiques\nLes lettres gg du package ggplot2 sont l’abréviation de “grammar of graphics” : la grammaire des graphiques. De la même manière que nous construisons des phrases en respectant des règles grammaticales précises (usage des noms, des verbes, des sujets et adjectifs…), la grammaire des graphiques établit un certain nombre de règles permettant de construire des graphiques : elle précise les composants d’un graphique en suivant le cadre théorique défini par Wilkinson (2005).\n\n3.3.1 Éléments de la grammaire\nEn bref, la grammaire des graphiques nous dit que :\n\nUn graphique est l’association (mapping) de données/variables (data) à des attributs esthétiques (aesthetics) d’objets géométriques (geometric objects).\n\nPour clarifier, on peut disséquer un graphique en 3 éléments essentiels :\n\ndata : le jeu de données contenant les variables que l’on va associer à des objets géométriques. Pour ggplot2 les données doivent obligatoirement être stockées dans un data.frame ou un tibble\ngeom : les objets géométriques en question. Cela fait référence aux types d’objets que l’on peut observer sur le graphique (des points, des lignes, des barres, etc.)\naes : les attributs esthétiques des objets géométriques présents sur le graphique. Par exemple, la position sur les axes x et y, la couleur, la taille, la transparence, la forme, etc. Chacun de ces attributs esthétiques peut-être associé à une variable de notre jeu de données.\n\nExaminons un exemple pour bien comprendre.\n\n\n3.3.2 Gapminder\nEn février 2006, un statisticien du nom de Hans Rosling a donné un TED Talk intitulé “The best stats you’we ever seen”. Au cours de cette conférence, Hans Rosling présente des données sur l’économie mondiale, la santé et le développement des pays du monde. Les données sont disponibles sur ce site et dans le package gapminder.\nPour l’année 2007, le jeu de données contient des informations pour 142 pays. Examinons les premières lignes de ce jeu de données :\n\n\n\nLes 6 premières lignes du jeu de données gapminder pour l’année 2007.\n\n\nCountry\nContinent\nLife Expectancy\nPopulation\nGDP per Capita\n\n\n\n\nAfghanistan\nAsia\n43.828\n31889923\n974.5803\n\n\nAlbania\nEurope\n76.423\n3600523\n5937.0295\n\n\nAlgeria\nAfrica\n72.301\n33333216\n6223.3675\n\n\nAngola\nAfrica\n42.731\n12420476\n4797.2313\n\n\nArgentina\nAmericas\n75.320\n40301927\n12779.3796\n\n\nAustralia\nOceania\n81.235\n20434176\n34435.3674\n\n\n\n\n\nPour chaque ligne, les variables suivantes sont décrites :\n\nCountry : le pays\nContinent : le continent\nLife Expectancy : espérance de vie à la naissance\nPopulation : nombre de personnes vivant dans le pays\nGDP per Capita : produit intérieur brut (PIB) par habitant en dollars américains. GDP est l’abréviation de “Growth Domestic Product”. C’est un indicateur de l’activité économique d’un pays, parfois utilisé comme une approximation du revenu moyen par habitant.\n\nExaminons maintenant la Figure 3.1 qui représente ces variables pour chacun des 142 pays de ce jeu de données (notez l’utilisation de la notation scientifique dans la légende, et de l’échelle logarithmique de l’axe des abscisses).\n\n\n\n\n\nFigure 3.1: Espérance de vie en fonction du PIB par habitant en 2007.\n\n\n\n\nSi on décrypte ce graphique du point de vue de la grammaire des graphiques, on voit que :\n\nla variable GDP per Capita est associée à l’aesthetic x de la position des points\nla variable Life Expectancy est associée à l’aesthetic y de la position des points\nla variable Population est associée à l’aesthetic size (taille) des points\nla variable Continent est associée à l’aesthetic color (couleur) des points\n\nIci, l’objet géométrique (ou geom) qui représente les données est le point. Les données (ou data) sont contenues dans le tableau gapminder et chacune de ces variables est associée (mapping) aux caractéristiques esthétiques des points.\n\n\n3.3.3 Autres éléments de la grammaire des graphiques\nOutre les éléments indispensables évoqués ici (data, mapping, aes, et geom), il existe d’autres aspects de la grammaire des graphiques qui permettent de contrôler l’aspect des graphiques. Ils ne sont pas toujours indispensables. Nous en verrons néanmoins quelque-uns particulièrement utiles :\n\nfacet : c’est un moyen très pratique de scinder le jeu de données en plusieurs sous-groupes et de produire automatiquement un graphique pour chacun d’entre eux.\nposition : permet notamment de modifier la position des barres d’un barplot.\nlabs : permet de définir les titres, sous-titres et légendes des axes d’un graphique\ntheme : permet de modifier l’apect général des graphiques en appliquant des thèmes prédéfinis ou en modifiant certains aspects de thèmes existants\n\n\n\n3.3.4 Le package ggplot2\nComme indiqué plus haut, le package ggplot2 (Wickham et al. 2024) permet de réaliser des graphiques dans R en respectant les principes de la grammaire des graphiques. Vous avez probablement remarqué que depuis le début de la section Section 3.3, beaucoup de termes sont écrits dans la police réservée au code informatique. C’est parce que les éléments de la grammaire des graphiques sont tous précisés dans la fonction ggplot() qui demande, au grand minimum, que les éléments suivants soient spécifiés :\n\nle nom du data.frame contenant les variables qui seront utilisées pour le graphique. Ce nom correspond à l’argument data de la fonction ggplot().\nl’association des variables à des attributs esthétiques. Cela se fait grâce à l’argument mapping et la fonction aes()\n\nAprès avoir spécifié ces éléments, on ajoute des couches supplémentaires au graphique grâce au signe +. La couche la plus essentielle à ajouter à un graphique, est une couche contenant un élément géométrique, ou geom (par exemple des points, des lignes ou des barres). D’autres couches peuvent s’ajouter pour spécifier des titres, des facets ou des modifications des axes et des thèmes du graphique.\nDans le cadre de ce cours, nous verrons un grand nombre de types de gra[hiques distincts, y compris les 5 types de graphiques les plus courants :\n\nles nuages de points\nles graphiques en lignes\nles histogrammes\nles diagrammes bâtons\nles boîtes à moustaches\n\n\n\n3.3.5 Votre premier graphique\nReprenons maintenant le jeu de données penguins :\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nComme évoqué plus haut, il s’agit d’un tibble. Plusieurs de ses variables concernent la biométrie des manchots, en particulier de son bec (voir Figure 3.2).\n\n\n\nFigure 3.2: Morphométrie du bec des manchots. Illustration de Allison Horst\n\n\nSupposons qu’on cherche à déterminer si la longueur du bec des manchots est proportionnelle à leur masse. Pour produire un graphique permettant de le déterminer, nous avons besoin des éléments suivants :\n\ndata : le tableau penguins\nun objet géométrique, ici, des points (geom_point()) puisque nous disposons de 2 variables numériques (plus de détails à ce sujet plus bas)\nl’association de certaines variables du jeu de données (ici, body_mass_g et bill_length_mm) à certaines caractéristiques esthétiques du graphiques (ici, la position sur les axes des x et des y), grâce à l’argument mapping et la fonction aes().\n\nConcrètement, voilà le code qu’il faut taper dans votre script :\n\nggplot(data = penguins, mapping = aes(x = body_mass_g, y = bill_length_mm))\n\n\n\n\nCette première ligne de code permet de faire plusieurs choses :\n\non indique à R qu’on souhaite faire un graphique (avec la fonction ggplot())\non indique à R que les données sont contenues dans l’objet penguins avec data = penguins\non associe (avec mapping = la variable body_mass_g à l’axe des x et la variable bill_length_mm à l’axe des y. On fait cela grâce à aes(x = body_mass_g, y = bill_length_mm)\n\nCette commande génère la première couche du graphique. Il n’y a pas encore de données car nous n’avons pas indiqué quel type d’objet géométrique nous souhaitons afficher, mais la fenêtre graphique est bel et bien créée, les axes apparaissent, ils sont légendés et leur échelle est adaptée aux variables du tableau penguins que nous avons sélectionnées. Pour terminer le graphique, il nous faut donc ajouter une seconde couche, celle de l’objet géométrique :\n\nggplot(data = penguins, mapping = aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nRelation entre masse corporelle et longueur du bec chez les manchots de l’archipel de Palmer\n\n\n\n\nAu moment de produire ce graphique, R nous indique que 2 lignes du tableau penguins ne figurent pas sur ce graphique car elles possèdent des données manquantes (NA), pour l’une et/ou l’autre des variables que nous avons sélectionnées. La fonction geom-point() est donc incapable de les placer sur le graphique.\nVous avez donc ici un premier exemple de graphique très simple. Il est loin d’être parfait (à minima, le titre des axes devrait être modifié), mais il a le mérite de vous présenter la syntaxe que vous devrez utiliser pour produire presque tous les graphiques qui vous seront utiles avec ggplot2. En outre, on peut percevoir qu’il semble exister une relation positive (mais imparfaite) entre longueur des becs et masse des individus. Il faut toutefois être prudent car nous avons ici utilisé toutes les données disponibles (donc les données des 3 espèces à la fois), ce qui est loin d’être pertinent.\n\n\n\n\n\n\nEn résumé\n\n\n\n\nAu sein de la fonction ggplot(), on spécifie 2 composants de la grammaire des graphiques :\n\nle nom du tableau contenant les données grâce à l’argument data = penguins\nl’association (mapping) des variables du tableau de données à des caractéristiques esthétiques (aes()) en précisant aes(x = body_mass_g, y = bill_length_mm) :\n\nla variable body_mass_g est associée à l’esthétique de position x\nla variable bill_length_mm est associée à l’esthétique de position y\n\n\nOn ajoute une couche au graphique ggplot() grâce au symbole +. La couche en question précise le troisième élément indispensable de la grammaire des graphiques : l’objet geométrique. Ici, les objets sont des points. On le spécifie grâce à la fonction geom_point().\n\n\n\nQuelques remarques concernant les couches :\n\nNotez que le signe + est placé à la fin de la ligne. Vous recevrez un message d’erreur si vous le placez au début.\nQuand vous ajoutez une couche à un graphique, je vous encourage vivement à presser la touche enter de votre clavier juste après le symbole +. Ainsi, le code correspondant à chaque couche sera sur une ligne distincte, ce qui augmente considérablement la lisibilité de votre code.\nComme indiqué dans la Section 1.3.4.3, tant que les arguments d’une fonction sont spécifiés dans l’ordre, on peut se passer d’écrire leur nom. Ainsi, les deux blocs de commande suivants produisent exactement le même résultat :\n\n\n# Le nom des arguments est précisé\nggplot(data = penguins, mapping = aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_point()\n\n# Le nom des arguments est omis\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_point()\n\n\n\n3.3.6 Exercices\n\nDonnez une raison pratique expliquant pourquoi les variables body_mass_g et bill_length_mm ont une relation positive\nQuelles variables (pas nécessairement dans le tableau penguins) pourraient avoir une corrélation négative (relation négative) avec body_mass_g ? Pourquoi ? Rappelez-vous que nous étudions ici des variables numériques.\nCitez les éléments de ce graphique/de ces données qui vous sautent le plus aux yeux ?\nCréez un nouveau nuage de points en utilisant d’autres variables du jeu de données penguins"
  },
  {
    "objectID": "03-Visualization.html#quel-graphique-dans-quelle-situation",
    "href": "03-Visualization.html#quel-graphique-dans-quelle-situation",
    "title": "3  Visualiser des données avec ggplot2",
    "section": "3.4 Quel graphique dans quelle situation ?",
    "text": "3.4 Quel graphique dans quelle situation ?\nIl n’est pas possible de faire n’importe quel type de graphique dans n’importe quelle situation. Selon le nombre de variables dont on dispose ou que l’on souhaite examiner, et selon la nature de ces variables (numériques et/ou catégorielles), le choix des types de graphiques possibles sera limité. Par exemple, les diagrammes bâtons sont réservés aux variables catégorielles, alors que les histogrammes sont possibles uniquement avec les variables numériques continues. Néanmoins, dans certaines situations, plusieurs choix de graphiques seront possibles, et vous aurez donc une certaine liberté. Vos choix seront alors guidés par les objectifs que vous souhaiterez atteindre grâce aux graphiques, ainsi que par vos préférences.\n\n\n\n\n\n\nObjectifs\n\n\n\nDans la suite de ce chapitre, nous traiterons donc des situations les plus courantes : quel(s) type(s) de graphique(s) produire lorsque l’on dispose d’une, deux ou trois variables ? Quel(s) type(s) de graphique(s) produire lorsque les variables sont toutes numériques, toutes catégorielles, ou lorsqu’on dispose de variables des deux types ?\nPour chaque situation, un ou des exemples seront fournis à partir des données du tableau penguins. Cela sera aussi l’occasion de présenter quelques-unes des nombreuses subtilités liées à l’utilisation du package ggplot2."
  },
  {
    "objectID": "03-Visualization.html#une-seule-variable-numérique",
    "href": "03-Visualization.html#une-seule-variable-numérique",
    "title": "3  Visualiser des données avec ggplot2",
    "section": "3.5 Une seule variable numérique",
    "text": "3.5 Une seule variable numérique\nLorsque l’on souhaite examiner une unique variable numérique, deux types de représentations graphiques sont en général possibles :\n\nles histogrammes : la variable d’intérêt est placée sur l’axe des x du graphique. Les valeurs utilisées sur l’axe des y est calculée automatiquement par le logiciel.\nles nuages de points : la variable d’intérêt est placée sur l’axe des y. L’axe des x porte soit un simple numéro d’indice pour chaque observation, soit une unique valeur sans importance, la même pour toutes les observations.\n\nLes syntaxes et options pour ces 2 types de graphiques sont présentées ci-dessous.\n\n3.5.1 Les histogrammes\n\n3.5.1.1 Syntaxe élémentaire\nImaginons que l’on s’intéresse à la variable body_mass_g du jeu de données penguins.\nLa syntaxe permettant de produire un histogramme, sous sa forme la plus simple, est la suivante :\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nDeux messages nous sont adressés par le logiciel :\n\nWarning: Removed 2 rows containing non-finite values (stat_bin). Ce message indique, comme pour le premier nuage de points, que 2 individus du tableau penguins ont une masse corporelle inconnue (NA). Ces 2 individus (donc les deux lignes correspondantes), ont été ignorés pour produire ce graphique\n'stat_bin()' using 'bins = 30'. Pick better value with 'binwidth'. Ce message indique que R a choisi pour nous les limites des classes utilisées pour faire l’histogramme. Sur un histogramme, la variable d’intérêt (toujours numérique et continue), qui apparaît sur l’axe des abscisses, est en effet “découpée” en plusieurs classes, en général de même taille, afin de permettre une représentation de la distribution des valeurs. Ici, R indique qu’il a créé 30 catégories pour nous, et que nous pouvons faire un choix différent grâce à l’argument binwidth. Nous y reviendrons un peu plus loin.\n\nSur ce graphique, l’axe des abscisses porte donc la variable continue “découpée” en classes de mêmes largeur, et l’axe des ordonnées renseigne sur le nombre (count ou fréquence absolue) d’individus observés dans chaque classe. Les zones du graphique où les barres sont les plus hautes indiquent donc les caractéristiques des individus observés le plus fréquemment. À l’inverse, les barres les plus courtes correspondent à des valeurs de masse rarement observées. Au final, ce type de graphique permet de visualiser la distribution des données pour une variable numérique continue.\nIci, on constate qu’une majorité d’individus semble avoir des masses proches de 3500 grammes. Une autre portion non négligeable des individus (mais moins importante) semble avoir une masse légèrement supérieure à 4500 grammes. Enfin, les masses supérieures à 6000 grammes sont très rares. L’histogramme nous permet également de visualiser l’étendue des données : les manchots étudiés ici ont des masses qui s’étalent d’un peu plus de 2500 grammes à un peu moins de 6500 grammes.\n\n\n3.5.1.2 Couleur\nPour rendre ce graphique plus facilement lisible, on peut en modifier la couleur :\n\nla couleur de remplissage des barres peut-être spécifiée grâce à l’argument fill =\nla couleur de contour des barres peut-être spécifiée grâce à l’argument color =\n\nUne liste des couleurs disponibles dans R peut être affichée dans la console en tapant :\n\ncolors()\n\nVous pouvez voir à quelle couleur correspond chacun de ces noms dans ce document pdf.\nMettons à jour notre histogramme en ajoutant un peu de couleur :\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(fill = \"steelblue\", color = \"black\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nLes 30 classes de masses sont maintenant plus facilement visibles et distingables.\n\n\n3.5.1.3 À l’intérieur ou à l’extérieur de aes() ?\nLes couleurs de remplissage et de contour des barres d’un histogramme font partie des caractéristiques esthétiques du graphique. Pourtant, elles ne sont pas précisées à l’intérieur de la fonction aes(). La raison est simple mais importante :\n\n\n\n\n\n\nImportant\n\n\n\nOn place à l’intérieur de aes() uniquement les caractéristiques esthétiques du graphique que l’on souhaite associer à des variables du jeu de données.\n\n\nIci, les couleurs que l’on indique sont des constantes : toutes les barres ont les mêmes couleur de remplissage et de contour. On n’associe pas une variable du jeu de données à ces caractéristiques esthétiques. On place donc fill = et color = à l’extérieur de aes(). Si on se trompe, voilà ce qui se produit :\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(aes(fill = \"steelblue\", color = \"black\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nLes couleurs qui apparaissent ne correspondent pas à ce qui est demandé, et une légende ne correspondant à rien apparaît à droite du graphique. La syntaxe utilisée ici suppose en effet que \"steelblue\" et \"black\" seraient des variables du jeu de données penguins. Puisqu’elles n’existent pas, R essaie de se débrouiller pour interpréter comme il peut ce qu’on lui demande, et finit par produire ce graphique incohérent. La couleur utilisée est la première couleur de la palette par défaut de ggplot2.\nPour élaborer des graphiques plus avancés, il faudra donc toujours vous poser la question suivante : la caractéristique esthétique que je souhaite modifier doit-elle être associée à une valeur constante que je fixe pour toutes les barres ou tous les points d’un graphique, et alors, je l’indique en dehors de aes(), ou est-elle au contraire associée à une variable du jeu de données, et alors, je l’indique à l’intérieur de aes().\nIl est bien sûr possible d’avoir un mélange des deux. Par exemple, le code suivant permet d’associer la couleur de remplissage au sexe des individus étudiés (variable sex du jeu de données penguins), et de spécifier une valeur constante pour la couleur de contour des barres (ici, le noir) :\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(aes(fill = sex), color = \"black\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nOn constate que toutes les barres ont un contour noir, mais que plusieurs couleurs de remplissage apparaissent maintenant, selon le sexe des individus, dans chaque classe de masse. Une légende adaptée est aussi créée automatiquement à droite du graphique. On apprend ainsi que les individus les plus lourds sont tous des mâles. On constate également que le sexe de certains individus est inconnu.\nAu final, nous ne sommes déjà plus dans la situation où on examine une unique variable numérique. Nous avons en effet ici un graphique nous permettant de mettre en relation une variable numérique (la masse des individus en grammes) et une variable catégorielle (le sexe des individus). Nous reviendrons plus tard sur ce type de graphiques.\n\n\n3.5.1.4 La largeur des classes\nComme évoqué plus haut, par défaut, R choisit arbitrairement de découper la variable numérique utilisée en 30 classes de même largeur afin de produire l’histogramme. Ça n’est que rarement un bon choix, et malheureusement, il n’y a pas de règle permettant de définir à coup sûr le bon nombre de classes pour visualiser au mieux la distribution d’une variable numérique. Il faut en effet presque toujours procéder par essais-erreurs successifs. Il est possible d’ajuster les caractéristiques (nombre et/ou largeur) des classes de l’histogramme de l’une des 3 façons suivantes :\n\nEn ajustant le nombre de classes avec bins.\nEn précisant la largeur des classes avec binwidth.\nEn fournissant manuellement les limites des classes avec breaks.\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(fill = \"steelblue\", color = \"black\",\n                 bins = 10)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nIci, diminuer le nombre de classes à 10 a pour effet de trop lisser la distribution des données. On ne visualise plus les variations subtiles de la distribution. À l’inverse, trop augmenter le nombre de classes n’est pas pertinent non plus :\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(fill = \"steelblue\", color = \"black\",\n                 bins = 100)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nIci, passer à 100 classes de taille génère un histogramme plein de trous, avec des classes très étroites, dont certaines sont très représentées, et immédiatement suivies ou précédées par des classes très peu représentées. Cela n’a pas de logique, et c’est presque toujours le signe qu’il faut réduire le nombre de classes.\nAu final, pour ces données, un nombre de classes compris entre 20 et 30 semble un bon choix :\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(fill = \"steelblue\", color = \"black\",\n                 bins = 25)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nC’est un bon choix, entre trop peu d’information, et trop de bruit visuel. Évidemment, ce nombre sera différent pour chaque jeu de données. On constate ici à peu près 3 pics (autour de 3500 grammes, un peu au-dessus de 4500 grammes, et autour de 5500 grammes) qui reflètent bien la distribution de ces données.\nOn peut également modifier la largeur des classes (et non plus leur nombre) avec binwidth :\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(fill = \"steelblue\", color = \"black\",\n                 binwidth = 200)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nIci, chaque catégorie recouvre 200 grammes. Avec l’argument bins, on indique à R combien on souhaite obtenir de classes, et il détermine automatiquement leur largeur. Avec binwidth, on indique la largeur des classes souhaitées, et R détermine automatiquement le nombre de classes nécessaires pour couvrir la totalité des données.\nEnfin, il est possible de déterminer manuellement les limites des classes souhaitées avec l’argument breaks :\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(fill = \"steelblue\", color = \"black\",\n                 breaks = c(2500, 2750, 3000, 3500, 4000, 4500, 5000, 6000, 7000))\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nVous constatez ici que les choix effectués ne sont pas très pertinents : toutes les classes n’ont pas la même largeur. Cela rend l’interprétation difficile. Il est donc vivement conseillé, pour spécifier breaks, de créer des suites régulières, comme avec la fonction seq() (consultez son fichier d’aide et les exemples) :\n\nlimites &lt;- seq(from = 2500, to = 6500, by = 250)\nlimites\n\n [1] 2500 2750 3000 3250 3500 3750 4000 4250 4500 4750 5000 5250 5500 5750 6000\n[16] 6250 6500\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(fill = \"steelblue\", color = \"black\",\n                 breaks = limites)\n\n\n\n\nUn exemple d’utilisation de l’argument breaks\n\n\n\n\nIl est important que toute la gamme des valeurs de body_mass_g soit bien couverte par les limites des classes que nous avons définies, sinon, certaines valeurs sont omises et l’histogramme est donc incomplet/incorrect. Une façon de s’en assurer est d’afficher le résumé des données pour la colonne body_mass_g du jeu de données penguins :\n\nsummary(penguins$body_mass_g)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   2700    3550    4050    4202    4750    6300       2 \n\n\nOn voit ici que les masses varient de 2700 à 6300 grammes. Les classes que nous avons définies couvrent une plage de masses plus large (de 2500 à 6500). Toutes les données sont donc bien intégrées à l’histogramme.\n\n\n3.5.1.5 geom_rug et geom_density\nLa fonction geom_histogram() n’est pas la seule qui permette de visualiser la distribution des données. Il est en effet possible d’utiliser d’autres objets géométriques, en plus ou à la place de geom_histogram() pour ajouter de l’information sur le graphique, ou pour visualiser différemment la distribution des mêmes données.\nLa fonction geom_rug() permet d’ajouter les données réelles sous forme de segments, sous un histogramme. Cela prend souvent l’aspect d’une sorte de tapis, d’où le nom de la fonction (“rug” signifie “tapis” en anglais). Pour ajouter une couche supplémentaire au graphique, on ajoute simplement un + à la fin de la dernière ligne, et sur la ligne suivante, on ajoute un objet géométrique supplémentaire :\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(fill = \"steelblue\", color = \"black\",\n                 bins = 25) +\n  geom_rug()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nLes tirets qui sont maintenant visibles en-dessous de l’histogramme correspondent aux 342 valeurs de masses réellement observées dans le jeu de données. Puisque certaines tailles ont été observées plusieurs fois, faire des tirets semi-transparents nous permettra de mieux visualiser quelles tailles ont été observées fréquemment ou rarement. On peut régler la transparence des éléments d’un graphique avec l’argument alpha =, qui prend des valeurs comprises entre 0 (transparence totale) et 1 (opacité totale) :\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(fill = \"steelblue\", color = \"black\",\n                 bins = 25) +\n  geom_rug(alpha = 0.3)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nLes tirets sont maintenant d’autant plus foncés que les tailles ont été observées un grand nombre de fois. On retrouve bien ici la distribution décrite plus haut, avec 3 principaux groupes de valeurs. Cela révèle certainement en partie la complexité des données : ces tailles correspondent en effet aux mesures effectuées chez 3 espèces distinctes qui peuvent avoir des caractéristiques différentes, sans compter que le sexe des individus, qui n’apparaît pas ici, entre aussi probablement en jeu. Nous y reviendrons plus tard.\nLa fonction geom_density() permet de s’affranchir de la question du nombre ou de la largeur des classes de taille :\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_density(fill = \"steelblue\", color = \"black\", alpha = 0.7, bw = 300)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\nOn obtient une sorte d’histogramme lissé qui fait bien apparaître les 3 tailles les plus fréquentes (au niveau des 3 “bosses” du graphique). Inutile ici de spécifier un nombre de classes de taille, ou leur largeur : le lissage est ici automatique. On peut modifier l’importance du lissage avec l’argument bw, mais la valeur choisie par défaut par R est généralement tout à fait satisfaisante. Vous pouvez essayer avec une valeur de lissage de 30, puis de 500 pour vous rendre compte de l’effet de ce paramètre.\nNotez également que si l’histogramme présentait des valeurs d’abondance sur l’axe des y (des nombres d’individus), le graphique de densité présente, comme son nom l’indique, l’information de densité des observations. Cela signifie que la surface totale sous la courbe (en bleu) vaut 1. Cela peut s’avérer utile pour comparer plusieurs distributions pour lesquelles on disposes de tailles d’échantillons très différentes.\nEnfin, on peut créer un graphique qui présentera à la fois l’histogramme (avec geom_histogram()), les données individuelles (avec geom_rug()) et la courbe de densité (avec geom_density()). Mais pour que tout s’affiche correctement, il faut indiquer à geom_histogram que l’axe des y doit porter les densités et non les abondances. On fait cela en précisant y = after_stat(density), cela indique à R que la variable density ne figure pas dans le tableau penguins, mais qu’elle est calculée par la fonction geom_histogram() :\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 fill = \"steelblue\", color = \"black\",\n                 bins = 25, alpha = 0.7) +\n  geom_rug(alpha = 0.3) +\n  geom_density(color = \"purple\", linewidth = 2)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\nNotez l’utilisation des arguments alpha, color et size, pour modifier l’aspect de différents éléments du graphique. Assurez-vous d’avoir compris comment on les utilise, et faites vos propres expériences.\n\n\n3.5.1.6 Un mot sur la position de la fonction aes()\nSur le dernier exemple, vous constatez que la fonction aes() apparaît une fois à l’intérieur de la fonction ggplot(), et une autre fois à l’intérieur de geom_histogram(). Pourquoi ne pas avoir tapé, plus simplement :\n\nggplot(penguins, aes(x = body_mass_g, y = after_stat(density))) +\n  geom_histogram(fill = \"steelblue\", color = \"black\",\n                 bins = 25, alpha = 0.7) +\n  geom_rug(alpha = 0.3) +\n  geom_density(color = \"purple\", linewidth = 2)\n\nL’explication est relativement simple, mais importante :\n\n\n\n\n\n\nImportant\n\n\n\nCe qui est spécifié dans la fonction ggplot() s’applique à toutes les couches du graphiques (donc ici, aux 3 couches geom_histogram(), geom_rug() et geom_density()).\nCe qui est spécifié dans une fonction geom_...() ne s’applique qu’à cette couche géométrique particulière.\n\n\nAinsi, ajouter y = after_stat(density) à l’intérieur de ggplot() renvoie donc un message d’erreur, car seule la fonction geom_histogram() calcule la variable density, seule la fonction geom_histogram() sait quoi faire de cette variable. Dans notre exemple, il est en revanche logique d’ajouter aes(x = body_mass_g) dans la fonction ggplot(), car nos trois couches géométriques ont besoin de cet argument, et pour les 3 couches géométriques, on associe bien cette variable body_mass_g à l’axe des x. Toutefois, rien ne nous empêche d’écrire ceci à la place :\n\nggplot(data = penguins) +\n  geom_histogram(aes(x = body_mass_g, y = after_stat(density)),\n                 fill = \"steelblue\", color = \"black\",\n                 bins = 25, alpha = 0.7) +\n  geom_rug(aes(x = body_mass_g),\n           alpha = 0.3) +\n  geom_density(aes(x = body_mass_g),\n               color = \"purple\", linewidth = 2)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\nC’est plus long, mais c’est tout à fait correct et ça produit exactement le même résultat qu’auparavant.\n\n\n\n3.5.2 Les nuages de points et stripcharts\nPour ces deux types de graphiques, la variable numérique sera portée par l’axe des y, et toutes les valeurs seront visibles, de façon non agrégée (contrairement aux histogrammes où les valeurs individuelles sont rassemblées à l’intérieur de classes). La différence entre les deux types de graphiques tient à la nature des informations qui figureront sur l’axe des x :\n\nPour les nuages de points, l’axe des x portera simplement l’information du numéro d’observation pour chaque individu. L’individu placé sur la première ligne du tableau de données portera l’indice 1. L’individu placé sur la deuxième ligne du tableau de données portera l’indice 2, et ainsi de suite jusqu’à l’individu placé sur la dernière ligne du tableau (il portera ici l’indice 344 puisque le tableau compte 344 lignes)\nPour un stripchart, l’axe des x portera une unique valeur, la même pour tous les individus\n\nDans les deux cas, l’axe des x ne nous sera pas vraiment utile. Il nous servira simplement à afficher des points sur un graphique, mais puisque nous ne disposons que d’une unique variable, c’est bien l’axe des y qui nous intéressera en priorité. Pour faire un nuage de points, on utilise geom_point(), et pour un stripchart geom_jitter(). Commençons par examiner le nuage de points pour la variable body-mass-g :\n\nggplot(penguins, aes(x = seq_along(body_mass_g), y = body_mass_g)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nC’est la fonction seq_along(), que l’on associe à l’axe des x, qui permet de faire apparaître les numéros de lignes du tableau penguins. On constate ici que 3 groupes de points sont présents :\n\nPour les lignes 1 à 150 (environ), un premier groupe de points présente des masses comprises entre 3000 et 4800 grammes environ.\nPour les lignes 151 à 275 (environ), un second groupes de points présente des masses comprises entre 4000 et plus de 6000 grammes.\nPour les lignes 276 à 344 (environ), un troisième groupe de points présente des valeurs similaires à celles du premier groupe.\n\nEn examinant le tableau penguins de plus près, on se rend compte que les 3 espèces de manchots sont présentées dans l’ordre. Ainsi, ces 3 groupes correspondent à 3 espèces différentes. Pour le visualiser, il suffit d’associer la variable species à la couleur des points. Puisqu’on cherche à associer une variable du tableau de données à une caractéristique esthétique d’un objet géométrique, on renseigne color = species à l’intérieur de aes() :\n\nggplot(penguins, aes(x = seq_along(body_mass_g), y = body_mass_g)) +\n  geom_point(aes(color = species))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nFigure 3.3: Nuage de points des masses corporelles des 3 espèces de manchots\n\n\n\n\nAttention, nous ne sommes déjà plus dans la situation d’une unique variable numérique : nous avons ici visualisé 2 variables : une numérique (portée par l’axe des y) et une catégorielle (l’espèce représentée par la couleur des points). Ici, on constate que les espèces Adélie et Chinstrap semblent avoir approximativement la même gamme de masses, alors que les Gentoo semblent nettement plus lourds.\nComme pour les histogrammes, on peut utiliser des caractéristiques esthétiques variées pour modifier l’apparence des points :\n\nalpha : la transparence. Choisir une valeur comprise entre 0 (invisible) et 1 (totalement opaque)\nsize : la taille des points\ncolor : la couleur des points (ou de leur contour pour les symboles qui permettent de spécifier une couleur de remplissage et une couleur de contour)\nfill : la couleur de remplissage des points (pour les symboles qui permettent de spécifier une couleur de remplissage et une couleur de contour)\nshape : pour modifier les symboles utilisés. Les symboles possibles sont codés ainsi :\n\n\n\n\nFigure 3.4: Liste des symboles et codes correspondants pour les graphiques faisant apparaître des points. Pour les symboles 21 à 25, il sera possible de spécifier une couleur de remplissage fill et une couleur de contour color. Pour tous les autres symboles, les changements de couleurs se feront avec l’argument color.\n\n\nChacune de ces caractéristiques esthétiques peut être associée à une variable d’un tableau (il faut alors le spécifier à l’intérieur de aes()), ou à une valeur unique, constante et identique pour tous les points du graphique (il faut alors le spécifier à l’extérieur de aes()). Par exemple :\n\nggplot(penguins, aes(x = seq_along(body_mass_g), y = body_mass_g)) +\n  geom_point(shape = 23, fill = \"steelblue\", color = \"black\", \n             size = 3, alpha = 0.5)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nL’ajout de la transparence permet de régler le problème des points qui se superposent (un phénomène nommé “overplotting”).\nExaminons à présent un exemple de stripchart :\n\nggplot(penguins, aes(x = \"\", y = body_mass_g)) +\n  geom_jitter()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nEn indiquant x = \"\", nous créons une unique catégorie pour l’axe des abscisses, qui sera utilisée pour placer les valeurs de tous les individus. Les valeurs de body_mass_g sont lues sur l’axe des y, comme pour un nuage de point classique. Si les points apparaissent dispersés, c’est en raison de 2 arguments spécifiques de la fonction geom_jitter() :\n\nwidth = permet de spécifier l’étendue horizontale du bruit aléatoire qui sera utilisé pour placer les points\nheight = permet de spécifier l’étendue verticale du bruit aléatoire qui sera utilisé pour placer les points\n\nSi nous ne renseignons pas nous même ces deux arguments, ils sont fixés automatiquement par le logiciel, ce qui n’est pas souhaitable, notamment pour le bruit vertical. Pour mieux comprendre, voyons ce qui se passe dans 3 situations :\n\nggplot(penguins, aes(x = \"\", y = body_mass_g)) +\n  geom_jitter(width = 0, height = 0)\n\nggplot(penguins, aes(x = \"\", y = body_mass_g)) +\n  geom_jitter(width = 0.1, height = 0)\n\nggplot(penguins, aes(x = \"\", y = body_mass_g)) +\n  geom_jitter(width = 0.1, height = 2000)\n\n\n\n\n\n\n\n(a) Pas de dispersion horizontale, pas de dispersion verticale\n\n\n\n\n\n\n\n(b) Faible dispersion horizontale, pas de dispersion verticale\n\n\n\n\n\n\n\n(c) Faible dispersion horizontale, forte dispersion verticale\n\n\n\n\nFigure 3.5: Trois exemples de stripchart\n\n\n\n\nLe premier exemple (Figure 3.5 (a)) ne présente aucune dispersion, ni horizontale (width = 0), ni verticale (height = 0). Les points apparaissent donc tous alignés, ils ont en effet tous la même valeur sur l’axe des abscisses. Leur position sur l’axe des y reflète la masse réellement observée pour chaque individu. Cette façon de représenter les données n’est pas très utile car la superposition des points vient empêcher la visualisation correcte de la distribution : ici, il est impossible de dire quelles sont les masses les plus fréquemment observées ou les plus rarement observées.\nLe second exemple (Figure 3.5 (b)) présente une dispersion horizontale modérée (width = 0.1) et pas de dispersion verticale (height = 0). Ici, tous les points ne sont plus alignés sur une seule droite. Puisque nous avons fixé width = 0.1, la position horizontale des points est choisie aléatoirement par R : il ajoute un léger bruit horizontal aléatoire, soit positif, soit négatif, avant de placer les points le long de l’axe des abscisses. Plus la valeur de width sera élevée, plus l’étendue du bruit horizontal sera importante. Sur l’axe des y en revanche, aucun bruit n’a été ajouté (height = 0). La position des points le long de cet axe reflète donc parfaitement la masse de chaque individu telle qu’enregistrées dans le tableau penguins et c’est bien ce que nous voulons. D’ailleurs, on constate que l’axe des ordonnées est strictement identique (même étendue, même graduations…) pour les 2 premiers sous-graphiques. C’est ce type de représentation que nous recherchons. En effet, l’absence de bruit vertical nous permet de visualiser correctement (donc sans distorsion) la variable numérique choisie (ici body_mass_g), et le bruit horizontal nous permet d’étaler légèrement les points de part et d’autres d’un axe vertical virtuel, ce qui a pour effet de réduire l’overplotting, et ce qui nous permet donc de visualiser les zones où les points sont plus nombreux/denses et les zones où les observations sont plus rares. Ici, on observe une majorité de points entre 3000 et 4000 grammes, une densité de points intermédiaire entre 4000 et 5000 grammes, et des points moins nombreux (donc moins d’individus) pour les masses supérieures à 5000 grammes.\nLe troisième exemple (Figure 3.5 (c)) présente une dispersion horizontale modérée (width = 0.1) et une importante dispersion verticale (height = 2000). Cela signifie que la position des points sur l’axe des y ne reflète plus les vraies valeurs de masses enregistrées dans le tableau penguins, mais des valeurs de masses auxquelles un bruit aléatoire a été ajouté ou retiré. C’est ce qui explique que l’axe des ordonnées ne présente pas la même échelle que pour les 2 autres graphiques. Ce n’est évidemment pas souhaitable, car si nous voulons bel et bien ajouter un bruit horizontal pour éviter la superposition des points, il est essentiel de ne pas modifier la position verticale des points qui nous renseigne sur la variable d’intérêt. Ici, la Figure 3.5 (c) présente un axe des y différent des 2 autres sous-figures, et la position verticale des points a été tellement altérée qu’on ne peut plus distinguer la sur-abondance de données entre 3000 et 4000 grammes, ni la sous-représentation des observations au-dessus de 5000 grammes. Il sera donc important à l’avenir de toujours fixer height = 0 pour faire un stripchart correct.\n\n\n\n\n\n\n\nImportant\n\n\n\nSur un stripchart :\n\nla position verticale des points ne doit jamais être modifiée. On fixera donc toujours height = 0\nla position horizontale des points doit être modifiée afin d’éviter l’overplotting et de visualiser les zones de fortes et faibles densités de points. On choisira donc en général des valeurs de width comprises entre 0.1 et 0.4\n\n\n\nEnfin, puisqu’un stripchart permet d’afficher des points sur un graphiques, les arguments permettant de modifier l’aspect des points sont les mêmes que pour les nuages de points. Par exemple :\n\nggplot(penguins, aes(x = \"\", y = body_mass_g)) +\n  geom_jitter(aes(color = species, shape = species),\n              size = 3, alpha = 0.6, \n              width = 0.1, height = 0)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nFigure 3.6: Un exemple de stripchart\n\n\n\n\nSur cette figure, comme pour le nuage de points réalisé plus haut, j’ai associé la variable species à la couleur des points (donc à l’intérieur de aes()). J’ai également associé cette variable à la forme des points shape = species à l’intérieur de aes(). C’est ce qui explique que chacune des 3 espèces apparaît sous la forme de symboles de formes et de couleurs différents. Pour limiter l’overplotting, j’ai spécifié un bruit horizontal, et j’ai fixé le bruit vertical à zéro. Enfin j’ai augmenté la taille des symboles (avec size = 3, en dehors de aes() car 3 est une constante qui s’appliquera à tous les points du graphique de la même manière) et leur transparence (avec alpha = 0.6, toujours en dehors de aes() pour la même raison). On constate ici encore que les masses corporelles des manchots Adélie et Chinstrap sont très similaires, et inférieures à celles de l’espèce Gentoo.\n\n\n3.5.3 Exercices\n\nÀ quoi sert l’argument stroke pour les nuages de points et les stripcharts ?\nCréez de nouveaux graphiques (histogramme et diagramme de densité) avec la variable contenant l’information de la longueur des nageoires des manchots flipper_length_mm. Décrivez les graphiques obtenus. Vos observations sont-elles cohérentes avec ce que nous savons maintenant des masses individuelles ?\nVisualisez ces données avec un nuage de points ou un stripchart. Retrouvez-vous les mêmes informations de distribution ?"
  },
  {
    "objectID": "03-Visualization.html#une-seule-variable-catégorielle",
    "href": "03-Visualization.html#une-seule-variable-catégorielle",
    "title": "3  Visualiser des données avec ggplot2",
    "section": "3.6 Une seule variable catégorielle",
    "text": "3.6 Une seule variable catégorielle\n\n3.6.1 Les diagrammes bâtons\nComme nous l’avons vu plus haut, les histogrammes permettent de visualiser la distribution d’une variable numérique continue. Souvent, on souhaite visualiser la distribution d’une variable catégorielle. C’est une tâche relativement aisée puisqu’elle consiste simplement à compter combien d’éléments tombent dans chacune des catégories (ou modalités) de la variable catégorielle. Le meilleur moyen de visualiser de telles données de comptage (aka fréquences) est de réaliser un diagramme bâtons, autrement appelé barplot ou barchart.\nUne difficulté, toutefois, concerne la façon dont les données sont présentées : est-ce que la variable d’intérêt est “pré-comptée” ou non ? Par exemple, le code ci-dessous crée 2 data.frame qui représentent la même collection de fruits : 3 pommes, 2 oranges et 4 bananes :\n\npanier &lt;- tibble(\n  fruit = c(\"pomme\", \"pomme\", \"banane\", \"pomme\", \"orange\", \"banane\", \"orange\", \"banane\", \"banane\")\n)\n\npanier_counted &lt;- tibble(\n  fruit = c(\"pomme\", \"orange\", \"banane\"),\n  nombre = c(3, 2, 4)\n)\n\nLe tableau panier contient des données qui n’ont pas encore été comptées. Le tableau contient donc une unique variable nommée fruit :\n\npanier\n\n# A tibble: 9 × 1\n  fruit \n  &lt;chr&gt; \n1 pomme \n2 pomme \n3 banane\n4 pomme \n5 orange\n6 banane\n7 orange\n8 banane\n9 banane\n\n\nÀ l’inverse, le tableau panier_counted contient des données qui ont déjà été comptées. Le tableau contient donc 2 variables dans 2 colonnes distinctes : une colonne fruit et une colonne nombre, mais seulement 3 lignes puisque seulement 3 modalités (les catégories de la variable catégorielle) sont présentes pour la variable fruit :\n\npanier_counted\n\n# A tibble: 3 × 2\n  fruit  nombre\n  &lt;chr&gt;   &lt;dbl&gt;\n1 pomme       3\n2 orange      2\n3 banane      4\n\n\nLes deux tableaux panier et panier_counted représentent exactement les mêmes données, mais sous deux formats différents. Du fait de ces deux formats possibles, deux objets géométriques distincts devront être utilisés pour représenter les données. Le graphique obtenu sera le même, mais à chaque format de tableau son geom_...().\n\n3.6.1.1 geom_bar() et geom_col()\nPour visualiser les données non pré-comptées, on utilise geom_bar() :\n\nggplot(panier, aes(x = fruit)) +\n  geom_bar()\n\n\n\n\nFigure 3.7: Barplot pour des données non pré-comptées.\n\n\n\n\nPour visualiser les données déjà pré-comptées, on utilise geom_col() :\n\nggplot(panier_counted, aes(x = fruit, y = nombre)) +\n  geom_col()\n\n\n\n\nFigure 3.8: Barplot pour des données pré-comptées.\n\n\n\n\nNotez que les figures Figure 3.7 et Figure 3.8 sont absolument identiques (à l’exception du titre de l’axe des ordonnées), mais qu’elles ont été créées à partir de 2 tableaux de données différents. En particulier, notez que :\n\nLe code qui génère la figure Figure 3.7 utilise le jeu de données panier, et n’associe pas de variable à l’axe des ordonnées : dans la fonction aes(), seule la variable associée à x est précisée. C’est la fonction geom_bar() qui calcule automatiquement les abondances (ou fréquences) pour chaque catégorie de la variable fruit. La variable count est ainsi générée automatiquement et associée à y.\nLe code qui génère la figure Figure 3.8 utilise le jeu de données panier_counted. Ici, c’est bien l’utilisateur qui associe la variable nombre à l’axe des y à l’intérieur de la fonction aes(). La fonction geom_col() a besoin de 2 variables (une variable catégorielle pour l’axe des x et une numérique pour l’axe des y) pour fonctionner.\n\nAutrement dit, lorsque vous souhaiterez créer un diagramme bâtons, il faudra donc au préalable vérifier de quel type de données vous disposez pour choisir l’objet géométrique approprié :\n\n\n\n\n\n\nDiagrammes bâtons\n\n\n\n\nSi la variable catégorielle n’est pas pré-comptée dans le tableau de données  geom_bar(). La variable catégorielle est associée à l’esthétique x du graphique. On ne renseigne pas y.\nSi la variable catégorielle est pré-comptée dans le tableau de données  geom_col(). La variable catégorielle est associée à l’esthétique x du graphique. On associe explicitement les comptages à l’esthétique y du graphique.\n\n\n\nEnfin, notez que l’ordre des modalité (ou catégories) qui apparaissent sur l’axe des abscisses est l’ordre alphabétique : la modalité banane apparaît à gauche, puis la modalité orange et enfin la modalité pomme. Bien souvent, cet ordre alphabétique n’est pas pertinent. Nous verrons plus loin comment faire pour trier les catégories par ordre croissant ou décroissant. C’est en effet une possibilité intéressante qui est impossible pour les histogrammes (car l’axe des x porte une variable numérique continue qu’il est impossible de “mélanger”), mais souvent vivement recommandée pour les diagrammes bâtons.\n\n\n3.6.1.2 Un exemple concret\nRevenons aux manchots. Imaginons que nous souhaitions connaître le nombre d’individus étudiés pour chaque espèce. Dans le jeu de données penguins, la variable species indique à quelle espèce appartiennent chacun des 344 individus étudiés. Une façon simple de représenter ces données est donc la suivante :\n\nggplot(penguins, aes(x = species)) +\n  geom_bar()\n\n\n\n\nFigure 3.9: Effectifs pour les 3 espèces de manchots étudiées\n\n\n\n\nIci, geom_bar() a compté le nombre d’occurrences de chaque espèce dans le tableau penguins et a automatiquement associé ce nombre à l’axe des ordonnées.\nLà encore, les modalités sont triées par ordre alphabétique sur l’axe des abscisses. Il est généralement plus utile de trier les catégories par ordre décroissant. Nous pouvons faire cela facilement grâce à la fonction fct_infreq() du package forcats, qui permet de modifier l’ordre des modalités d’une variable catégorielle (ou facteur). Si vous avez installé le tidyverse, le package forcast doit être disponible sur votre ordinateur. N’oubliez pas de le charger si besoin :\n\nlibrary(forcats)\nggplot(penguins, aes(x = fct_infreq(species))) +\n  geom_bar()\n\n\n\n\nFigure 3.10: Effectifs pour les 3 espèces de manchots étudiées, triés en ordre décroissant\n\n\n\n\nOrdonner les catégories par ordre décroissant est souvent indispensable afin de faciliter la lecture du graphique et les comparaisons entre catégories.\nSi nous souhaitons connaître le nombre précis d’individus de chaque espèce, il nous faut faire appel à plusieurs fonctions du package dplyr que nous détaillerons dans le chapitre Chapitre 5. Ci-dessous, nous créons un nouveau tableau species_table contenant le nombre d’individus de chaque espèce et les espèces sont ordonnées par abondance décroissante :\n\nspecies_table &lt;- penguins %&gt;%   # On prend le tableau penguins, puis...\n  count(species) %&gt;%            # On compte les effectifs de chaque espèce, puis...\n  arrange(desc(n))              # On trie par effectif décroissants ...\nspecies_table                   # Enfin, on affiche la nouvelle table\n\n# A tibble: 3 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Gentoo      124\n3 Chinstrap    68\n\n\nIci, la table a été triée par effectifs décroissants. Mais attention, les niveaux du facteur species n’ont pas été modifiés :\n\nfactor(species_table$species)\n\n[1] Adelie    Gentoo    Chinstrap\nLevels: Adelie Chinstrap Gentoo\n\n\nLe premier niveau est toujours Adélie, puis Chinstrap, en enfin Gentoo, et non pas l’ordre du tableau nouvellement créé (Adelie, puis Gentoo, puis Chinstrap) car les niveaux sont toujours triés par ordre alphabétique. La conséquence est que si nous devions faire un diagramme bâtons avec ces données, la fonction geom_col() ne permettrait pas d’ordonner les catégories correctement :\n\nggplot(species_table, aes(x = species, y = n)) +\n  geom_col()\n\n\n\n\nSi nous souhaitons trier ces catégories par effectif décroissant, la fonction fct_infreq() ne nous est ici d’aucune utilité. En effet, le tableau species_table contient une seule ligne pour chaque espèce, donc une fréquence de 1 pour chaque espèce. Le critère de la fréquence d’occurrence des modalités dans le tableau de données ne peut donc pas être utilisé. Pour parvenir à nos fins avec ce tableau déjà précompté, il faut cette fois utiliser la fonction fct_reorder() pour ordonner correctement les catégories. Cette fonction prends 3 arguments :\n\nLa variable catégorielle dont on souhaite réordonner les niveaux (ici, la variable species du tableau species_table).\nUne variable numérique qui permet d’ordonner les catégories (ici, la variable n du même tableau).\nL’argument optionnel .desc qui permet de préciser si le tri doit être fait en ordre croissant (c’est le cas par défaut) ou décroissant.\n\n\nggplot(species_table, \n       aes(x = fct_reorder(species, n, .desc = TRUE), y = n)) +\n  geom_col()\n\n\n\n\nVous voyez donc que selon le type de données dont vous disposez (soit un tableau comme penguins, avec toutes les observations, soit un tableau beaucoup plus compact comme species_table), la démarche permettant de produire un diagramme bâtons, dans lequel les catégories seront triées, sera différente.\nUne dernière précision : inverser l’ordre des variables sur les axes du graphiques permet de faire un diagramme bâtons horizontal. C’est parfois très utile lorsque les modalités de la variable catégorielle sont nombreuses et/ou que leur nom est long. Faire apparaître les modalités sur l’axe des y au lieu de l’axe des x peut rendre leur lecture plus aisée :\n\nggplot(penguins, aes(y = fct_infreq(species))) +\n  geom_bar(fill = \"steelblue\", color = \"black\", alpha = 0.7)\n\n\n\nggplot(species_table, \n       aes(y = fct_reorder(species, n, .desc = TRUE), x = n)) +\n  geom_col(fill = \"firebrick\", color = \"black\", alpha = 0.7)\n\n\n\n\n\n\n3.6.1.3 Exercices\n\nQuelle est la différence entre un histogramme et un diagramme bâtons ?\nPourquoi les histogrammes sont-ils inadaptés pour visualiser des données catégorielles ?\nPourquoi ne peut-on pas trier un histogramme par ordre croissant ?\nQuelle île de l’archipel Palmer a fourni le plus grand nombre de manchots pour cette étude ?\n\n\n\n\n3.6.2 Éviter à tout prix les diagrammes circulaires\nÀ mon grand désarroi, l’un des graphiques classiquement utilisé pour représenter la distribution d’une variable catégorielle est le diagramme circulaire (ou diagramme camembert, piechart en anglais). C’est presque toujours la plus mauvaise visualisation possible pour représenter les effectifs ou pourcentages associés aux modalités d’une variable catégorielle. Je vous demande de l’éviter à tout prix. Notre cerveau n’est en effet pas correctement équipé pour comparer des angles et des surfaces. Ainsi, par exemple, nous avons naturellement tendance à surestimer les angles supérieurs à 90º, et à sous-estimer les angles inférieurs à 90º. En d’autres termes, il est difficile pour les humains de comparer des grandeurs sur des diagrammes circulaires.\nÀ titre d’exemple, examinez ce diagramme, qui reprend les mêmes chiffres que précédemment, et tentez de répondre aux questions suivantes :\n\n\n\n\n\nFigure 3.11: Répartition des effectifs par espèce et par sexe\n\n\n\n\n\nQuelle est la catégorie la plus représentée ?\nDe combien de fois la part des Gentoo mâles est-elle supérieure à celle des Chinstrap femelles ? (1,5 fois, 2 fois, 2.5 fois ?…)\nQuelle est la quatrième catégorie la plus représentée ?\n\nIl est difficile (voir impossible) de répondre précisément à ces questions avec le diagramme circulaire de la Figure 3.11, alors qu’il est très simple d’obtenir des réponses précises avec un diagramme bâtons tel que présenté à la Figure 3.12 ci-dessous (vérifiez-le !) :\n\n\n\n\n\nFigure 3.12: Répartition des effectifs par espèce et par sexe"
  },
  {
    "objectID": "03-Visualization.html#deux-variables-numériques",
    "href": "03-Visualization.html#deux-variables-numériques",
    "title": "3  Visualiser des données avec ggplot2",
    "section": "3.7 Deux variables numériques",
    "text": "3.7 Deux variables numériques\nLa représentation graphique la plus adaptée à la visualisation des relations entre deux variables numériques est aussi l’une des plus simples : il s’agit des nuages de points que nous avons déjà évoqués. Ici dépendant, puisque nous disposons de 2 variables numériques, nous allons en associer une à l’axe des x et l’autre à l’axe des y. Si l’on pressent que l’une des deux variables pourrait “expliquer” la seconde, ou être en partie responsable de ses variations, on l’appelle variable explicative et on la placera alors sur l’axe des x. L’autre variable, celle que l’on suppose influencée par la première est appelée variable expliquée, et sera associée à l’axe des y.\nLes nuages de points de 2 variables numériques permettent donc de visualiser les relations (supposées ou réelles) entre deux variables.\n\n3.7.1 Nuage de points\n\n3.7.1.1 Syntaxe élémentaire\nPrenons un exemple : nous souhaitons examiner les relations qui existent entre la masse corporelle des individus et la longueur de leur nageoire. Une relation allométrique simple suppose en effet que plus un individu est grand et lourd, plus ses membres seront développés. La nature de la relation allométrique peut toutefois être radicalement différente selon les espèces. Pour l’instant, nous ne nous intéressons pas aux éventuelles différences entre espèces et nous examinerons donc l’ensemble des données, toutes espèces confondues.\n\nggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nIci, j’associe body_mass_g à l’axe des x car je suppose que c’est la variable explicative. Il est en effet plus logique de considérer que la masse corporelle influence la longueur des nageoires plutôt que le contraire. La variable expliquée, ici flipper_length_mm est associée à l’axe des y.\nLa syntaxe est donc très simple, et le graphique obtenu permet de constater que plus les individus sont lourds, plus leurs nageoires sont longues.\n\n\n3.7.1.2 Droite de tendance\nSi l’on souhaite visualiser (modéliser !) cette association entre les deux variables, on peut ajouter sur ce graphique une courbe de tendance ou une droite de régression avec l’objet géométrique geom_smooth() :\n\nggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nL’argument method = \"lm\" indique que nous souhaitons ajouter une droite de régression (lm est l’abréviation de “linear model”). L’intervalle grisé autour de la droite représente l’incertitude associée à la régression et indique que la “vraie” droite de régression, dans la population générale (et pas seulement dans notre échantillon de 344 individus) est probablement située dans cette zone grisée. Nous aurons l’occasion de revenir en détail sur la notion de régression linéaire et d’incertitude associée plus loin dans ce livre.\n\n\n3.7.1.3 Autres caractéristiques esthétiques\nComme pour tous les graphiques faisant apparaître des points, il est possible de modifier les caractéristiques esthétiques habituelles, soit en les associant à des variables du jeu de données (et en l’indiquant à l’intérieur de aes()), soit en les fixant à des valeurs constantes qui s’appliqueront à tous les points (et en l’indiquant alors en dehors de aes()). L’exemple ci-dessous illustre ces possibilités :\n\nggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm, fill = species)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  geom_smooth(aes(color = species), method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nL’argument se = FALSE de la fonction geom_smooth() permet de ne pas afficher l’intervalle d’incertitude de la régression linéaire. Ici, j’ai associé la couleur de remplissage des points et la couleur des droites de régression aux espèces (donc à l’intérieur de aes(), soit dans ggplot() soit dans geom_smooth()), et j’ai fixé pour tous les points, le choix du type de symbole (shape = 21, voir Figure 3.4), la couleur de contour (color = \"black\") et la transparence alpha = 0.6.\nLà encore, il ne s’agit plus strictement d’un graphique représentant les relations entre 2 variables numériques, mais bien entre 3 variables : deux variables numériques (body_mass_g et flipper_length_mm) et une variable catégorielle ou facteur (species). Il est finalement très simple d’ajouter d’autres variables sur un graphique bivarié tel qu’un nuage de points.\n\n\n\n3.7.2 Les graphiques en lignes\nLes graphiques en lignes, ou “linegraphs” sont généralement utilisés lorsque l’axe des x porte une information temporelle, et l’axe des y une autre variable numérique. Le temps est une variable naturellement ordonnée : les jours, semaines, mois, années, se suivent naturellement. Les graphiques en lignes devraient être évités lorsqu’il n’y a pas une organisation séquentielle évidente de la variable portée par l’axe des x. Ainsi, lorsque l’une des 2 variables dont on dispose est une variable numérique temporelle (des dates, des heures, etc.), on la place sur l’axe des x et la seconde variable, dont on étudiera les fluctuations au cours du temps, sur l’axe des y. On peut alors relier les valeurs grâce à l’objet géométrique geom_line() afin de créer une série temporelle. Pour illustrer cela, examinons un autre jeu de données qui contient une variable temporelle :\n\neconomics\n\n# A tibble: 574 × 6\n   date         pce    pop psavert uempmed unemploy\n   &lt;date&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 1967-07-01  507. 198712    12.6     4.5     2944\n 2 1967-08-01  510. 198911    12.6     4.7     2945\n 3 1967-09-01  516. 199113    11.9     4.6     2958\n 4 1967-10-01  512. 199311    12.9     4.9     3143\n 5 1967-11-01  517. 199498    12.8     4.7     3066\n 6 1967-12-01  525. 199657    11.8     4.8     3018\n 7 1968-01-01  531. 199808    11.7     5.1     2878\n 8 1968-02-01  534. 199920    12.3     4.5     3001\n 9 1968-03-01  544. 200056    11.7     4.1     2877\n10 1968-04-01  544  200208    12.3     4.6     2709\n# ℹ 564 more rows\n\n\nLe jeu de données economics est fourni avec le package ggplot2. Puisque vous avez chargé ce package (ou le tidyverse qui contient ce package), vous devriez pouvoir accéder à ce tableau sans difficulté. Nous nous intéresserons ici à la variable date que nous placerons sur l’axe des x et à la variable uempmed qui est la durée de chômage médiane dans la population américaine, en nombre de semaines, que nous placerons sur l’axe des y. Examinons donc comment la durée médiane du du chômage a évolué au fil du temps :\n\nggplot(economics, aes(x = date, y = uempmed)) +\n  geom_line()\n\n\n\n\nNotez que puisque la variable date du tableau economics est comprise par R comme étant du type “variable temporelle” (le type indiqué dans le tableau, juste sous le nom de variable, est &lt;date&gt;), l’axe des abscisses du graphique, qui est associé à cette variable, est correctement mis en forme : seules les années apparaissent.\nLes graphiques en lignes permettent de visualiser des progressions/évolutions lorsqu’il existe une temporalité entre les données. Sur l’exemple, traité plus haut, du lien entre masse et longueur des nageoire des manchots, relier les points n’aurait absolument aucun sens puisque toutes les observations sont indépendantes : elles correspondent à des individus différents. Soyez donc prudents lorsque vous reliez les points dur un graphique. Cela n’est possible que lorsque les données le permettent. Vous devez donc toujours vous poser la question de la pertinence de vos choix de représentations.\nComme pour les autres types de graphiques, il est possible de modifier les caractéristiques esthétiques des lignes sur un graphique, en particulier :\n\ncolor : la couleur des lignes\nsize : l’épaisseur des lignes\nlinetype : le type de ligne (continue, pointillée, tirets, etc. Essayez plusieurs valeurs entières pour comparer les types de lignes)\n\n\nggplot(economics, aes(x = date, y = uempmed)) +\n  geom_line(color = \"orange\", linetype = 2)\n\n\n\n\nL’argument linetype est également utilisable par l’objet géométrique geom_smooth() :\n\nggplot(economics, aes(x = date, y = uempmed)) +\n  geom_line() +\n  geom_smooth(se = FALSE, linetype = 4, color = \"red\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nGlobalement, la durée médiane de chômage aux USA varie de façon cyclique. La durée des cycles varie selon les période entre 5 et 10 ans environ. Depuis les années 2000, la durée de chômage a augmenté de façon très importante, pour passer de 5 à 6 semaines en 2001, à plus de 25 semaines en 2011.\n\n\n3.7.3 Les cartes\nLes latitudes et longitudes sont un autre type de variable numériques très particulières qui permettent notamment de produire des cartes. Il s’agit ici d’un domaine extrêmement vaste qui dépasse largement le cadre de ce livre. Retenez simplement qu’il est possible de produire des cartes très informatives avec ggplot2, et quelques autres packages spécialisés :\n En règle général, les cartes portent un grand nombre de variables, numériques et/ou catégorielles. Mais tout commence toujours par 2 variables numériques, les latitudes et longitude des structures/informations que l’on souhaite représenter (traits de côte, profiles bathymétriques, lieux d’observations diverses, …)."
  },
  {
    "objectID": "03-Visualization.html#deux-variables-catégorielles",
    "href": "03-Visualization.html#deux-variables-catégorielles",
    "title": "3  Visualiser des données avec ggplot2",
    "section": "3.8 Deux variables catégorielles",
    "text": "3.8 Deux variables catégorielles\nLorsque l’on souhaite examiner les relations entre deux variables catégorielles (ou facteurs), on a en général le choix entre les types de représentations graphiques suivants :\n\nles diagrammes bâtons empilés\nles diagrammes bâtons juxtaposés\nles diagrammes bâtons “facettés”\nles graphiques en mosaïque (ou mosaic plots)\n\nPour toutes ces méthodes, des données qui n’ont pas été comptées au préalable sont requises. Il est en effet beaucoup plus simple de travailler avec le tidyverse (donc avec ggplot2) lorsque chaque ligne d’un tableau correspond à une observation plutôt qu’à une somme d’observation. C’est le concept de tableau rangé, central dans le traitement de données ainsi que pour l’utilisation de tous les packages du tidyverse, et qui stipule qu’un tableau de données devrait contenir une unique ligne pour chaque observation, et une unique colonne pour chaque variable. Nous aurons l’occasion (notamment en L3) de voir des tableaux qui ne respectent pas ces règles et que nous devrons donc ré-organiser pour permettre leur analyse et les représentations graphiques.\nNous allons passer ces différentes possibilités en revue pour examiner les liens entre 2 variables catégorielles du jeu de données penguins : species et sex. La première renseigne sur l’espèce à laquelle un individu étudié appartient. La seconde renseigne sur le sexe de chaque individu. L’étude du sex-ratio est en effet souvent essentielle pour comprendre l’écologie des espèces. Les sexe-ratios sont-ils équilibrés ou non. Et s’ils ne sont pas équilibrés, sont-ils en faveur des mâles ou des femelles ?\n\n3.8.1 Diagrammes bâtons empilés\nLa façon la plus simple (mais rarement la meilleure) de procéder pour visualiser 2 facteurs conjointement est de créer un diagramme bâtons empilés :\n\nggplot(penguins, aes(x = species, fill = sex)) +\n  geom_bar()\n\n\n\n\nIci, les espèces sont associées à l’axe des x (x = species) et la couleur de remplissage des barres est associée au sexe des individus (fill = sex), à l’intérieur de la fonction aes(). Comme toujours, on peut modifier certaines caractéristiques esthétiques (couleur de contour des barres, transparence, etc.) et ré-ordonner les espèces sur l’axe des abscisses :\n\nggplot(penguins, aes(x = fct_infreq(species), fill = sex)) +\n  geom_bar(alpha = 0.6, color = \"black\")\n\n\n\n\nCe type de visualisation est utile pour se rendre compte des ordres de grandeur. On voit ici clairement que l’espèce Adélie est la plus représentée dans cette étude, suivie par l’espèce Gentoo, et enfin l’espèce Chinstrap. Pour chacune de ces 3 espèces, le sex-ratio a l’air très équilibré. Toutefois, des différences subtiles de proportions entre mâles et femelles selon les espèces pourraient être masqués par les effectifs inégaux entre espèces. Il peut donc être préférable, pour comparer des proportions, de normaliser les effectifs de toutes les espèces pour ramener chaque barre du graphique à la même hauteur :\n\nggplot(penguins, aes(x = fct_infreq(species), fill = sex)) +\n  geom_bar(alpha = 0.6, color = \"black\", position = \"fill\")\n\n\n\n\nL’argument position = \"fill\" de la fonction geom_bar() permet de transformer en proportions les abondances de chaque modalités de la variable portée par l’axe des x. L’axe des ordonnées varie maintenant entre 0 et 1 (0% et 100%), ce qui rend les comparaisons plus aisées. Ici, le fait que le sexe de quelques individus n’ait pas pu être déterminé vient gêner la lecture du graphique. On peut supprimer ces valeurs grâce à la fonction filter() du packages dplyr. Nous verrons dans le #sec-wrangling la signification du code suivant. Pour l’instant retenez simplement qu’il permet d’éliminer les individus dont le sexe est inconnu :\n\npenguins %&gt;% \n  filter(!is.na(sex)) %&gt;% \n  ggplot(aes(x = fct_infreq(species), fill = sex)) +\n  geom_bar(alpha = 0.6, color = \"black\", position = \"fill\")\n\n\n\n\nOn peut maintenant constater très facilement que le sex-ratio est parfaitement équilibré pour les espèces Adélie et Chinstrap, et qu’il est très légèrement en faveur des mâles pour l’espèce Gentoo.\n\n\n3.8.2 Diagrammes bâtons juxtaposés\nLa syntaxe permettant de produire un diagramme bâtons juxtaposé est très similaire à celle décrite ci-dessus :\n\nggplot(penguins, aes(x = fct_infreq(species), fill = sex)) +\n  geom_bar(alpha = 0.6, color = \"black\", position = \"dodge\")\n\n\n\n\nLa seule chose qui a changé est la valeur prise par l’argument position, que l’on fixe ici à dodge. L’avantage de cette représentation est qu’elle permet à la fois de visualiser les effectifs de chaque catégorie et sous-catégorie (espèce et sexe), ainsi que de comparer les proportions au sein de chaque espèce. Un inconvénient et que lorsque les catégories n’ont pas toutes le même nombre de sous-catégories, les barres ont des largeurs différentes. Ici, l’espèce Chinstrap, qui n’a que 2 sous catégories (female et male) présente des barres plus larges que les deux autres espèces qui présentent chacune 3 sous-catégories (female, male et NA). Pour y remédier, on peut :\n\nsoit retirer les données manquantes, comme précédemment :\n\n\npenguins %&gt;% \n  filter(!is.na(sex)) %&gt;% \n  ggplot(aes(x = fct_infreq(species), fill = sex)) +\n  geom_bar(alpha = 0.6, color = \"black\", position = \"dodge\")\n\n\n\n\n\nsoit imposer que toutes les sous-catégories apparaissent pour chaque catégorie :\n\n\nggplot(penguins, aes(x = fct_infreq(species), fill = sex)) +\n  geom_bar(alpha = 0.6, color = \"black\",\n           position = position_dodge(preserve = \"single\") )\n\n\n\n\nIci, l’argument position prend une valeur plus complexe puisque nous faisons appel à une fonction nommée position_dodge(). C’est l’argument preserve = \"single\" qui permet de s’assurer que toutes les sous-catégories sont bien représentées au sein de chaque catégorie, et donc, que toutes les barres ont bien la même largueur.\nLe choix d’une méthode ou de l’autre dépend de ce que l’on souhaite montrer : il n’y a pas une façon de faire meilleure ou moins bonne que l’autre. Tout dépend de l’objectif poursuivi par l’auteur du graphique.\n\n\n3.8.3 Diagrammes bâtons “facettés”\nDans le jargon de ggplot2, les facets sont simplement des sous-graphiques. Typiquement, une variable catégorielle peut être utilisée pour représenter un sous-graphique pour chaque modalité de cette variable. Ici, on peut par exemple produire un diagramme bâton pour chaque espèce, et l’axe des x de chaque graphique portera la variable sex :\n\nggplot(penguins, aes(x = sex)) +\n  geom_bar() +\n  facet_wrap(~species)\n\n\n\n\nC’est la fonction facet_wrap() qui permet de produire plusieurs sous graphiques. Examinons quelques-une de ces particularités :\n\nsa syntaxe fait appel à la notion de “formule”, utilisée pour certaines fonctions spécifiques dans le langage R. Nous en verrons des exemples en L3 pour illustrer certains tests statistiques. Le tilde ~ se lit “en fonction de”. Ici ~species signifie “crée des facets en fonction des espèces”, autrement dit, produit un sous-graphique par modalité de la variable species.\npar défaut, les axes de tous les sous graphiques sont strictement identiques, en abscisse comme en ordonnée. On peut modifier ce comportement grâce à l’un des arguments suivants : scales = \"free_x\" (pour que les axes des abscisses soient indépendants entre les sous-graphiques), scales = \"free_y\" (pour que les axes des ordonnées soient indépendants entre les sous-graphiques) ou scales = \"free\" (pour quelles deux axes soient indépendants entre les sous-graphiques)\nl’argument ncol = permet de spécifier le nombre de colonnes souhaité pour l’organisation des sous-graphiques\n\nVoici un exemple de ces syntaxes :\n\nggplot(penguins, aes(x = sex)) +\n  geom_bar() +\n  facet_wrap(~species, scales = \"free_y\", ncol = 2)\n\n\n\n\nLes 3 sous-graphiques sont maintenant disposés dans 2 colonnes, et si l’axe des x est toujours le même pour chaque sous-graphique, les axes des y sont différents pour les 3 sous-graphiques.\nPour égayer un peu ce graphique, ajoutons une couleur de remplissage pour les barres, selon l’espèce :\n\nggplot(penguins, aes(x = sex, fill = species)) +\n  geom_bar(color = \"black\", alpha = 0.7) +\n  facet_wrap(~species, scales = \"free_y\", ncol = 2)\n\n\n\n\nLa légende qui est automatiquement créée à droite est inutile puisque les sous-graphiques indiquent déjà le nom des espèces. Pour retirer une légende inutile, on peut utiliser l’argument show.legend = FALSE de la plupart des objets géométriques :\n\nggplot(penguins, aes(x = sex, fill = species)) +\n  geom_bar(color = \"black\", alpha = 0.7, show.legend = FALSE) +\n  facet_wrap(~species, scales = \"free_y\", ncol = 2)\n\n\n\n\n\n\n3.8.4 Mosaïc plots\nLes graphiques en mosaïque sont une alternative aux diagrammes bâtons en tous genre. Ils permettent de visualiser à la fois les effectifs et de comparer les proportions. La difficulté de ce genre de graphique est qu’il n’existe pas d’objet géométrique permettant de les représenter simplement dans le package ggplot2. Le package ggmosaic de Jeppson, Hofmann, et Cook (2021) est toutefois entièrement dédié à ce type de graphique. Installez ce package puis chargez-le en mémoire :\n\ninstall.packages(\"ggmosaic\")\nlibrary(ggmosaic)\n\nOn peut maintenant accéder à un nouvel objet géométrique, geom_mosaic(), dont l’utilisation est un peu différente de celle que nous avons vu jusqu’ici :\n\nggplot(penguins) +\n  geom_mosaic(aes(x = product(species), fill = sex))\n\n\n\n\nIl faut obligatoirement :\n\nspécifier aes() à l’intérieur de geom_mosaic() et non à l’intérieur de ggplot()\nutiliser la fonction product() (qui fait elle aussi partie du package ggmosaic) pour indiquer quelle variable catégorielle on souhaite associer à l’axe des x\nComme pour les diagrammes bâtons, la couleur de remplissage est associée à la seconde variable catégorielle de façon tout à fait classique\n\nComme pour les diagrammes en bâtons empilés pour lesquels on spécifie position = \"fill\", toutes les barres d’un graphique en mosaïque ont la même hauteur, ce qui permet de visualiser les proportions de chaque sexe pour chaque espèce, mais pas les effectifs. C’est ici la largueur des barres qui est proportionnelle aux effectifs de chaque espèce. Si on n’accède par directement aux valeurs absolues, on peut néanmoins effectuer des comparaisons d’ordres de grandeur. L’espèce Adélie est ainsi la plus représentée dans nos données, suivie de l’espèce Gentoo puis de l’espèce Chinstrap.\nAu final, le choix d’un graphique doit vous permettre de mettre en évidence les relations qui vous paraissent importantes de la façon la plus visuelle et évidente possible pour une personne ne connaissant pas vos données. Votre choix dépendra donc des données disponibles et de votre objectif (p. ex. comparaisons de proportions ou de valeurs absolues, nombreuses modalités ou seulement quelques unes, etc.)."
  },
  {
    "objectID": "03-Visualization.html#une-variable-de-chaque-type",
    "href": "03-Visualization.html#une-variable-de-chaque-type",
    "title": "3  Visualiser des données avec ggplot2",
    "section": "3.9 Une variable de chaque type",
    "text": "3.9 Une variable de chaque type\nLes représentations graphiques réalisables et pertinentes lorsque l’on dispose d’une variable numérique et d’un facteur sont souvent des adaptations des graphiques précédents. Globalement, trois choix s’offrent à nous :\n\nles histogrammes facettés\nles stripcharts\nles boîtes à moustaches. Nous donnerons ici un simple. La signification de tous les éléments de ces graphiques sera fournie à la Section 6.2\n\nPour illustrer ces différentes possibilités, intéressons nous maintenant à la relation qui existe entre l’épaisseur du bec des manchots (bill_depth_mm, variable numérique) et l’espèce (species, variable catégorielle ou facteur)\n\n3.9.1 Histogrammes “facettés”\nLa syntaxe est ici tout à fait classique. Pour réaliser un histogramme, on place la variable numérique sur l’axe des abscisses. La variable catégorielle nous servira à créer les sous graphiques, ici, un par espèce. Afin de faciliter les comparaisons, nous placerons les sous-graphiques les uns sous les autres en spécifiant ncol = 1. Enfin, l’aspect général sera amélioré en modifiant quelques caractéristiques esthétiques :\n\nggplot(penguins, aes(x = bill_depth_mm)) +\n  geom_histogram(fill = \"steelblue\", color = \"black\", \n                 alpha = 0.6, bins = 20) +\n  facet_wrap(~species, ncol = 1)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nOn peut aussi choisir d’utiliser une couleur pour chaque espèce (mais on n’affichera pas la légende puisque les espèces sont déjà séparées dans les sous graphiques). En outre, puisque les effectifs des Chinstrap sont bien plus faibles que pour les deux autres espèces, on a intérêt à “libérer” l’axe des y afin que l’histogramme des Chinstrap soit plus facilement lisible (il apparaît pour l’instant très “écrasé” comparé aux autres).\n\nggplot(penguins, aes(x = bill_depth_mm, fill = species)) +\n  geom_histogram(show.legend = FALSE, color = \"black\", \n                 alpha = 0.6, bins = 20) +\n  facet_wrap(~species, ncol = 1, scales = \"free_y\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\nLes Gentoo, qui ont pourtant des masses corporelles supérieures à celle des deux autres espèces (voir Figure 3.3 de la Section 3.5.2), ont visiblement des becs moins épais (entre 12 et 17 mm) que les deux autres espèces (entre 16 et 22 mm).\n\n\n\n\n\n\nImportant\n\n\n\nC’est la position des données le long de l’axe des x qui nous permet de faire des comparaisons pertinentes. Il est donc essentiel de présenter les différents histogrammes les uns sous les autres, en conservant la même échelle pour les abscisses de tous les sous-graphiques.\n\n\nIci, on peut donc discuter de la distribution de la variable numérique pour chaque modalité de la variable catégorielle (i.e. quelle distribution de l’épaisseur des becs pour chaque espèce), mais on peut en plus faire des comparaisons entre modalités (entre les espèces). Cela est beaucoup plus pertinent que de s’intéresser à la distribution de l’épaisseur des becs toutes espèces confondues.\n\n\n3.9.2 Les stripcharts\nNous avons déjà abordé ce type de graphique dans la Section 3.5.2. Contrairement à la situation où nous n’avions qu’une variable numérique et où nous devions fixer x = \"\" pour que toutes les observations se placent au même niveau de l’axe des abscisses (voir Figure 3.6), nous allons ici associer la variable catégorielle à l’axe des x. La variable numérique sera quant-à-elle toujours associée à l’axe des ordonnées :\n\nggplot(penguins, aes(x = species, y = bill_depth_mm)) +\n  geom_jitter(width = 0.20, height = 0)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nFigure 3.13: Un exemple de stripchart\n\n\n\n\nNotez que la position des points sur l’axe des y doit parfaitement correspondre aux valeurs contenues dans le jeu de données pour la variable numérique d’intérêt. Cela signifie que l’argument height doit obligatoirement être fixé à 0.\nComme pour les diagrammes bâtons, il est possible de produire des stripcharts horizontaux. Les modifications à apporter sont alors les suivantes :\n\nla variable numérique est associée à l’axe des x\nla variable catégorielle est associée à l’axe des y\nla dispersion horizontale width doit obligatoirement être fixée à 0\nla dispersion verticale height doit être comprise entre 0.1 et 0.4 pour étaler les points de chaque modalité et ainsi éviter l’overplotting\n\n\nggplot(penguins, aes(x = bill_depth_mm, y = species)) +\n  geom_jitter(width = 0, height = 0.20, alpha = 0.6)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n3.9.3 Les boîtes à moustaches ou boxplots\nVoilà à quoi ressemble un graphique de ce type pour les données qui nous intéressent (épaisseur des becs selon l’espèce) :\n\nggplot(penguins, aes(x = species, y = bill_depth_mm)) +\n  geom_boxplot()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nDans la forme, ça ressemble un à un stripchart (comparez par exemple avec la syntaxe et les résultats obtenus à la Figure 3.13). Néanmoins, ici, au lieu de visualiser tous les points du jeu de données, seules quelques valeurs caractéristiques sont utilisées pour construire le boîte à moustache de chaque espèce. Les différents éléments d’un boxplot, sont les suivants :\n\nLa limite inférieure de la boîte correspond au premier quartile : 25% des données de l’échantillon sont situées au-dessous de cette valeur.\nLa limite supérieure de la boîte correspond au troisième quartile : 25% des données de l’échantillon sont situées au-dessus de cette valeur.\nLe segment épais à l’intérieur de la boîte correspond au second quartile : c’est la médiane de l’échantillon. 50% des données de l’échantillon sont situées au-dessus de cette valeur, et 50% au-dessous.\nLa hauteur de la boîte correspond à ce que l’on appelle l’étendue inter-quartile ou Inter Quartile Range (IQR) en anglais. On trouve dans cette boîte 50% des observations de l’échantillon. C’est une mesure de la dispersion des 50% des données les plus centrales. Une boîte plus allongée indique donc une plus grande dispersion.\nLes moustaches correspondent à des valeurs qui sont en dessous du premier quartile (pour la moustache du bas) et au-dessus du troisième quartile (pour la moustache du haut). La règle utilisée dans R est que ces moustaches s’étendent jusqu’aux valeurs minimales et maximales de l’échantillon, mais elles ne peuvent en aucun cas s’étendre au-delà de 1,5 fois la hauteur de la boîte (1,5 fois l’IQR) vers le haut et le bas. Si des points apparaissent au-delà des moustaches (vers le haut ou le bas), ces points sont appelés “outliers”. On peut en observer un pour l’espèce Adélie. Ce sont des points qui s’éloignent du centre de la distribution de façon importante puisqu’ils sont au-delà de 1,5 fois l’IQR de part et d’autre du premier ou du troisième quartile. Il peut s’agir d’anomalies de mesures, d’anomalies de saisie des données, ou tout simplement, d’enregistrements tout à fait valides mais atypiques ou extrêmes. J’attire votre attention sur le fait que la définition de ces outliers est relativement arbitraire. Nous pourrions faire le choix d’étendre les moustaches jusqu’à 1,8 fois l’IQR (ou 2, ou 2,5). Nous observerions alors beaucoup moins d’outliers. D’une façons générale, la longueur des moustaches renseigne sur la variabilité des données en dehors de la zone centrale. Plus elles sont longues, plus la variabilité est importante. Et dans tous les cas, l’examen attentif des outliers est utile car il nous permet d’en apprendre plus sur le comportement extrême de certaines observations.\n\nLorsque les boîtes ont une forme à peu près symétrique de part et d’autre de la médiane (c’est le cas pour notre exemple), cela signifie qu’un histogramme des mêmes données serait symétrique également (on peut le vérifier avec les histogrammes de la Section 3.9.1).\n\n3.9.3.1 L’intervalle de confiance à 95% de la médiane\nOn peut également ajouter une encoche autour de la valeur de médiane en ajoutant l’argument notch = TRUE à la fonction geom_boxplot() :\n\nggplot(penguins, aes(x = species, y = bill_depth_mm)) +\n  geom_boxplot(notch = TRUE)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nL’encoche qui apparaît sur chaque boîte à moustache correspond à l’étendue de l’intervalle de confiance à 95% de la médiane. Pour chaque échantillon, nous espérons que la médiane calculée soit le reflet fidèle de la vraie valeur de médiane de la population générale. Mais il sera toujours impossible d’en avoir la certitude absolue. Le mieux que l’on puisse faire, c’est quantifier l’incertitude associée à l’estimation de la médiane à partir des données d’un échantillon. L’intervalle de confiance nous indique qu’il y a de bonnes chances que la vraie valeur de médiane de la population générale (qui restera à jamais inconnue) se trouve dans cet intervalle.\nNous reviendrons sur cette notion importante plus tard dans le cursus, car ce type de graphique nous permettra d’anticiper sur les résultats des tests statistiques de comparaison de moyennes.\nAu final, nous avons 3 moyens d’obtenir des informations de distribution :\n\nobserver l’ensemble des données brutes grâce à un nuage de points ou stripchart\nregrouper en partie les données brutes dans les classes d’un histogramme. On ne visualise plus l’ensemble des données individuelles, mais un résumé de ces données puisqu’on ne dispose plus que d’une unique valeur pour chaque classe de l’histogramme. L’histogramme peut donc résumer des centaines voire des milliers de points sous la forme d’un petit nombre de classes (entre 10 et 40 en général)\nregrouper très fortement les données brutes sous la forme d’une boîte à moustache. Les boîtes à moustaches permettent de résumer l’information de centaines ou milliers de points sous la forme d’un résumé statistique composé de 5 valeurs (minimum et maximum, médiane, premier et troisième quartiles), ou 7 si l’on ajoute les encoches des intervalles de confiance à 95% des médianes. On observe alors moins facilement les nuances subtiles de distribution qu’avec un histogramme ou les données brutes, mais l’avantage est qu’on peut comparer facilement les grandes tendances d’un grand nombre de séries de données (parfois plusieurs dizaines) en plaçant des boîtes à moustaches côte à côte.\n\nLa Figure 3.14 illustre ces 3 possibilités de visualisation de la distribution d’une variable numérique (ici, la distribution des masses corporelles des manchots Adélie) :\n\n\n\n\n\nFigure 3.14: Trois façons de visualiser la distribution des masses des manchots Adélie"
  },
  {
    "objectID": "03-Visualization.html#trois-variables-et-plus",
    "href": "03-Visualization.html#trois-variables-et-plus",
    "title": "3  Visualiser des données avec ggplot2",
    "section": "3.10 Trois variables (et plus !)",
    "text": "3.10 Trois variables (et plus !)\nLorsque l’on dispose de 3 variables, les situations possibles commencent à être nombreuses :\n\ntrois variables numériques\ndeux variables numériques et un facteur\nune variable numérique et deux facteurs\ntrois facteurs\n\nPour chacune de ces situations, on peut en générale reprendre les types de graphiques proposés dans les 3 sections précédentes consacrées aux situations où l’on dispose de 2 variables (), et :\n\nsoit ajouter une variable sous forme de code couleur (avec color ou fill à l’intérieur de aes())\nsoit ajouter une variable sous forme de facets (avec facet_wrap() ou avec facet_grid())\n\nLes possibilités sont très nombreuses et il ne sera pas possible d’être exhaustif ici. Je fournis néanmoins quelques exemples ci-dessous afin que vous compreniez bien la logique. Ensuite, ça sera à vous d’expérimenter selon les données dont vous disposez, les questions scientifiques que vous vous posez, et les relations que vous souhaitez explorer/visualiser.\n\n3.10.1 Trois variables numériques\nDans cette situation, on fait en général un nuage de points qui porte une variable numérique sur chaque axe, et on associe la troisième variable numérique soit à la couleur des points, soit à leur taille (soit aux deux à la fois). Par exemple, pour examiner les relations entre longueur du bec, épaisseur du bec, et masse corporelle, on peut procéder ainsi :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     size = body_mass_g)) +\n  geom_point(shape = 21, fill = \"steelblue\", color = \"black\", alpha = 0.6)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nC’est ce qu’on appelle un “bubble plot”. Ici on constate que les individus qui ont les becs les plus courts, sont aussi ceux qui ont un bec épais (groupe de points en haut à gauche). Ces individus sont parmi les plus légers (symboles de petite taille). À l’inverse, les individus ayant les becs les plus longs ont aussi des becs peu épais (groupes de points situés en bas à droite). Ces individus sont parmi les plus lourds du jeu de données (symboles de grandes taille).\nUne autre façon de visualiser ces mêmes données consiste à associer la masse des individus à la couleur de remplissage des symboles :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6, size = 2)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nCette fois, les individus les plus légers apparaissent en bleu très sombre, et les individus les plus lourds en bleu très clair. Ce choix de couleur nous est imposé, mais nous verrons plus loin comment le modifier pour rendre ce type de graphique plus facile à lire. Lorsque nous associons une variable numérique continue à la couleur des points, la légende qui est générée automatiquement pour nous par R sera toujours un gradient de couleurs. Si vous revenez en arrière au niveau des graphiques en mosaïques (Section 3.8.4), ou au niveau des diagrammes bâtons juxtaposés (Section 3.8.2), vous verrez que lorsque la couleur est associée à une variable catégorielle (ou facteur), la légende présente des couleurs distinctes, une pour chaque modalité du facteur considéré. Là encore, R choisit les couleurs pour nous. Mais là encore, nous verrons comment imposer des couleurs différentes si les choix par défaut ne nous conviennent pas.\nEnfin, il est évidemment possible de jouer à la fois sur la couleur et sur la taille des symboles :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = body_mass_g, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nIci, l’information de masse est donc associée à 2 caractéristiques esthétiques distinctes : la couleur de remplissage des points et leur taille. Cela rend la lecture plus facile dans certaines situations.\nAu final, nous avons donc associé 3 variables numériques à 4 caractéristiques esthétiques du graphique :\n\nbill_length_mm est associée à x\nbill_depth_mm est associée à y\nbody_mass_g est associé à fill\nbody_mass_g est associé à size\n\nRien ne nous empêche d’ajouter des variables et des caractéristiques esthétiques. C’est ce que nous allons voir tout de suite.\n\n\n3.10.2 Cinq variables !\nPour commencer, essayez de reproduire le graphique suivant :\n\n\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nIci, 5 variables du jeu de données (3 numériques et 2 facteurs) sont associées à 5 caractéristiques esthétiques du graphique. Le graphique est donc très riche, on peut voir par exemple :\n\nque les 3 espèces ont des morphologies de bec assez distinctes : les Gentoo ont des becs longs et fins, les Chinstrap ont des becs longs et épais, et les Adélie ont des becs courts et épais.\nqu’un dimorphisme sexuel est présent au niveau du bec : pour chaque espèce, les mâles ont des becs plus longs et épais que les femelles\nqu’un dimorphisme sexuel est présent au niveau des masses : pour chaque espèce, les mâles sont plus lourds que les femelles\n\nAu final, beaucoup d’informations sont présentées sur ce graphique et c’est presque trop. Même en améliorant l’aspect général du graphique pour le rendre plus lisible (voir ci-dessous), il vaut parfois mieux se limiter à 2 ou 3 variables et faire plusieurs graphiques, plutôt que de tout mettre sur le même. Une bonne solution consiste souvent à mettre 2 ou 3 variables sur un graphique, mais à faire plusieurs sous-graphiques pour chaque modalité d’une 4ème et/ou d’une 5ème variable catégorielle.\n\n\n\n\n\nEn particulier, sur ce graphique, il est presque impossible de déterminer la masse des individus mâles. La légende indique en effet des tailles de cercles qui correspondent à des masses spécifiques. Mais nous n’avons aucune indication pour la taille des triangles. Il vaudrait donc mieux procéder ainsi :\n\n\n\n\n\nFigure 3.15: Relation entre la morphologie du bec, la masse et le sexe chez trois espèces de manchots de l’archipel Palmer\n\n\n\n\nEn associant le sexe des individus à la couleur de remplissage plutôt qu’à la forme des points, et en faisant un sous-graphique par espèce, on élimine la difficulté de lecture liée à la taille des symboles triangulaires. L’information concernant la masse des individus est donc plus facile à visualiser. Le dimorphisme sexuel de taille des becs, présent pour chaque espèce, apparaît beaucoup plus clairement qu’avant (les mâles ont des becs plus longs et épais que les femelles). Mais les différences inter-spécifiques de morphologie des becs sont moins visibles qu’avant, notamment pour les différences de longueur des becs selon les espèces. On ne peut malheureusement pas gagner sur les tableaux à la fois. C’est la raison pour laquelle les choix de graphiques que vous ferez devront refléter les questions auxquelles vous vous intéressez, et les messages que vous souhaitez faire passer.\n\n\n3.10.3 Deux variables numériques et un facteur\nL’exemple qui suit est fondamental pour bien comprendre l’importance d’explorer en détail tous les aspects d’un jeu de données pour éviter de dire de grosses bêtises.\nImaginez que dans le jeu de données penguins, on souhaite étudier la relation qui existe entre l’épaisseur du bec des individus et la longueur des nageoires. Ces deux variables étant numériques, il semble logique de commencer par faire un nuage de points :\n\nggplot(penguins, aes(x = bill_depth_mm, y = flipper_length_mm)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nNous avons vu plus haut que pour visualiser le relations qui existent entre deux variables numériques, il est possible d’ajouter une courbe ou une droite de tendance avec la fonction geom_smooth() :\n\nggplot(penguins, aes(x = bill_depth_mm, y = flipper_length_mm)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nSi l’on s’en tient à ça, la relation semble claire : plus le bec des individus est épais, plus leurs nageoires sont courtes, et inversement. En outre, nous avons visiblement deux groupes d’individus qui présentent des caractéristiques distinctes : certains ont des becs fins et des nageoires très longues, quand d’autres ont des becs épais et des nageoires courtes. Et quasiment aucun individu ne présente de caractéristiques intermédiaires (becs d’épaisseur moyenne épais et nageoires moyennes).\nEn réalité, cette vision des choses est totalement trompeuse ! N’oubliez pas que nous avons 3 espèces distinctes dans ce jeu de données, et que ces espèces peuvent présenter des caractéristiques morphologiques très variées. Examiner la relation becs-nageoires tel que nous l’avons fait, sans considérer les espèces, n’a strictement aucun sens ! Pour s’en convaincre, il suffit d’associer la couleur (des points et des lignes) à l’espèce :\n\nggplot(penguins, aes(x = bill_depth_mm, y = flipper_length_mm,\n                     color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nLe résultat obtenu ici est à l’opposé de nos conclusions précédentes : la relation entre les deux variables numérique est en fait positive ! Au sein de chaque espèce, les individus possédant les becs les plus épais sont aussi ceux qui possèdent les nageoires les plus longues !\nEn statistiques ce phénomène (observer une relation inverse lorsque plusieurs groupes sont combinés) s’appelle le paradoxe de Simpson, et je vous encourage à consulter la page wikipédia qui y est consacrée.\nIci, si la relation entre nos deux variables numériques s’inverse lorsque l’on examine cette relation à l’échelle de chaque modalité de la variable catégorielle. Une façon encore plus nette de mettre la relation positive en évidence est la suivante :\n\nggplot(penguins, aes(x = bill_depth_mm, y = flipper_length_mm,\n                     color = species)) +\n  geom_point(show.legend = FALSE) +\n  geom_smooth(method = \"lm\", show.legend = FALSE) +\n  facet_wrap(~species, scales = \"free\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nMême sans connaître en détail le principe et les limites de la régression linéaire, vous comprenez j’espère à quel point l’exploration rigoureuse d’un jeu de données est importante. Par exemple, nous avons vu que plus tôt que, pour chaque espèce, les mâles sont plus lourds que les femelles. Est-ce que des différences morphologiques entre les sexes pourraient expliquer la relation que nous observons ici entre épaisseur du bec et longueur des nageoires ? Est-il possible qu’un second effet Simpson se cache dans ces données ? Si on distingue les deux sexes au sein de chaque espèce, la relation existe-t-elle toujours ? Et si oui, est-elle toujours positive ou s’inverse-t-elle à nouveau ? Pour le savoir, on peut utiliser une autre fonction permettant de produire des sous-graphiques, la fonction facte_grid(), qui permet de faire un sous graphique pour chaque combinaison de modalités de 2 variables catégorielles :\n\npenguins %&gt;% \n  filter(!is.na(sex)) %&gt;%    # Elimine les individus dont le sexe est inconnu\n  ggplot(aes(x = bill_depth_mm, y = flipper_length_mm,\n                     color = species, shape = sex)) +\n  geom_point(show.legend = FALSE) +\n  geom_smooth(method = \"lm\", show.legend = FALSE) +\n  facet_grid(sex ~ species, scales = \"free\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nLa syntaxe sex ~ species indique que l’on souhaite un sous-graphique pour chaque combinaison des modalités des facteurs sex et species. Les graphiques correspondant aux différents sexes apparaîtront sur des lignes distinctes, et les espèces sur des colonnes distinctes. On constate ici que si la relation semble toujours positive et assez nette pour les mâles des 3 espèces, la situation est moins tranchée pour les femelles, en particulier pour les espèces Adélie et Chinstrap.\nNous avons vu dans ce chapitre quelques exemples et des règles à suivre strictement (notamment, quels types de graphiques pour quelles types de variables). Mais les possibilités sont infinies, et je vous encourage donc à poursuivre l’exploration. Toutes les combinaisons des éléments que nous avons décrits sont possibles. Entre les facets, qui permettent de faire des sous graphiques pour chaque modalités (ou combinaisons de modalités) d’une ou deux variables catégorielles, les caractéristiques esthétiques auxquelles ont peut associer un nombre conséquent de variables numériques et/ou catégorielles, et les nombreux objets géométriques existants (nous n’avons fait qu’utiliser les plus courants, mais il en existe beaucoup d’autres), les possibilités sont infinies. À vous de faire preuve de curiosité et d’explorer d’autres types de visualisation. L’avantage de ggplot2 est que tous les graphiques se construisent sur le même modèle :\n\n\n\n\n\n\nImportant\n\n\n\n\nggplot(TABLEAU, aes(x = VAR1, y = VAR2, fill = VAR3, ...)) +\n  geom_XXX() +\n  geom_YYY() +\n  facet_ZZZ()\n\n\n\nQuand on a bien compris ce principe, on peut quasiment tout faire, les réponses aux questions qu’on se pose se trouvant presque toujours dans les fichiers d’aide des fonctions."
  },
  {
    "objectID": "03-Visualization.html#peaufiner-lapparence",
    "href": "03-Visualization.html#peaufiner-lapparence",
    "title": "3  Visualiser des données avec ggplot2",
    "section": "3.11 Peaufiner l’apparence",
    "text": "3.11 Peaufiner l’apparence\nJusqu’ici, les morceaux de code que nous avons vus permettent de produire une large gamme de graphiques exploratoires. Mais il y a une différence de taille entre des graphiques que l’on fait pour soi, afin de comprendre et explorer des données, et des graphiques que l’on fait pour communiquer à autrui des informations ou le fruit de nos découvertes.\nLes graphiques que l’on souhaite intégrer à un compte-rendu ou un rapport doivent :\n\navoir des labels corrects pour les axes (penser à toujours indiquer l’unité des variables portées par les axes)\navoir des labels corrects pour les légendes situées à droite de la plupart des graphiques (voir les nombreux exemples décrits plus haut)\navoir éventuellement un titre. Il ne sera pas toujours utile de l’intégrer à la figure car la plupart du temps, les titres sont ajoutés manuellement dans le traitement de texte que vous utilisez\nutiliser des couleurs agréables et faciles à distinguer (y compris pour les personnes atteintes de daltonisme)\nutiliser des échelles adaptées (ordres de grandeur, échelles logarithmiques, etc.)\nsi possible avoir tous le même thème (mêmes choix de couleurs, de contours, de polices de caractères, etc.)\n\nDonc, lorsqu’on obtient un graphique exploratoire parlant et que l’on souhaite l’intégrer à un rapport ou un compte-rendu, 3 étapes sont nécessaires à sa mise en forme :\n\nmodifier les légendes avec la fonction labs()\nmodifier les échelles avec les nombreuses fonctions scale_XXX_YYY()\nmodifier le thème général avec les fonctions theme_XX()\n\n\n3.11.1 Les légendes ou labels\nLe point de départ le plus évident est d’ajouter des labels de qualité. La fonction labs() du package ggplot2 permet d’ajouter plusieurs types de labels sur vos graphiques :\n\nUn titre (title =) : il doit résumer les résultats les plus importants.\nUn sous-titre (subtitle =) : il permet de donner quelques détails supplémentaires.\nUne légende (caption =) : souvent utilisée pour présenter la source des données du graphique.\nUn titre pour chaque axe (x = et y =) : permet de préciser les variables portées par les axes et leurs unités.\nUn titre pour les échelles de couleurs, de forme, de taille, etc.\n\nReprenons par exemple le graphique permettant de visualiser la relation entre épaisseur et longueur du bec selon la masse des individus :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = body_mass_g, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nNous préciser les légendes en ajoutant la fonction labs() sur une nouvelle couche du graphique :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = body_mass_g, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  labs(title = \"Forme du bec de 3 espèces de manchots et relation avec leur masse\",\n       subtitle = \"Les manchots les plus lourds ont des becs longs et fins\",\n       x = \"Longueur du bec (mm)\",\n       y = \"Épaisseur du bec (mm)\",\n       caption = \"Source :  package 'palmerpenguins'\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nPour annoter correctement les légendes situées à droite, il convient d’avoir bien compris ce qui, dans notre code, a permis à R de générer automatiquement ces légendes. Le gradient de couleur a été créé parce que nous avons tapé fill = body_mass_g. Et l’échelle de taille des symboles a été créée parce que nous avons tapé size = body_mass_g. Dans la fonction labs(), nous devons donc préciser fill = \"...\" et size = \"...\" pour modifier le titre de ces 2 légendes :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = body_mass_g, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  labs(title = \"Forme du bec de 3 espèces de manchots et relation avec leur masse\",\n       subtitle = \"Les manchots les plus lourds ont des becs longs et fins\",\n       x = \"Longueur du bec (mm)\",\n       y = \"Épaisseur du bec (mm)\",\n       caption = \"Source :  package 'palmerpenguins'\",\n       fill = \"Masse (g)\",\n       size = \"Masse (g)\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nFigure 3.16: Une figure correctement légendée\n\n\n\n\nÀ partir de maintenant, vous devriez systématiquement légender les axes de vos graphiques et annoter vos légendes correctement, en n’oubliant pas de préciser les unités lorsque c’est pertinent, pour tous les graphiques que vous intégrez dans vos rapports, compte-rendus, mémoires, présentation, etc.\n\n\n3.11.2 Les échelles\nTous les détails des graphiques que vous produisez peuvent être édités. C’est notamment le cas des échelles. Qu’il s’agisse de modifier l’étendue des axes, la densité du quadrillage, la position des tirets sur les axes, le nom des catégories figurant sur les axes ou dans les légendes ou encore les couleurs utilisées pour différentes catégories d’objets géométriques, tout est possible dans ggplot2.\nNous n’avons pas le temps ici d’aborder toutes ces questions en détail. Je vous encourage donc à consulter l’ouvrage en ligne intitulé R for data science, et en particulier son chapitre dédié aux échelles, si vous avez besoin d’apporter des modifications à vos graphiques et que vous ne trouvez pas comment faire dans cet ouvrage.\n\n3.11.2.1 La gestion des couleurs\nNous allons néanmoins examiner quelques possibilités, à commencer par la façon de procéder pour modifier les couleurs choisies par défaut par ggplot2. Reprenons la figure Figure 3.16, et changeons le gradient de couleur proposé par défaut par R. Il est possible de modifier ces couleurs de plusieurs façons :\n\nsoit en utilisant d’autres palettes de couleurs prédéfinies\nsoit en choisissant manuellement les couleurs\n\nToutes les fonctions permettant d’altérer les légendes commencent par scale_. Vient ensuite le nom de l’esthétique que l’on souhaite modifier (ici fill_) et enfin, le nom d’une fonction à appliquer. Les possibilités sont nombreuses et vous pouvez en avoir un aperçu en tapant le début du nom de la fonction et en parcourant la liste proposée par RStudio sous le curseur. Il faut toutefois distinguer 2 types d’échelles de couleurs : les échelles continues (c’est notre cas ici) et les échelles discrètes (quand l’esthétique de couleur est associée à une variable catégorielle, nous en verrons un exemple plus loin).\nPar exemple, il est possible d’utiliser la palette viridis. Selon ses auteurs :\n\n“Use [this palette] to make plots that are pretty, better represent your data, easier to read by those with colorblindness, and print well in gray scale.\n\nPour utiliser cette palette, il suffit d’ajouter une couche à notre graphique :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = body_mass_g, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  labs(title = \"Forme du bec de 3 espèces de manchots et relation avec leur masse\",\n       subtitle = \"Les manchots les plus lourds ont des becs longs et fins\",\n       x = \"Longueur du bec (mm)\",\n       y = \"Épaisseur du bec (mm)\",\n       caption = \"Source :  package 'palmerpenguins'\",\n       fill = \"Masse (g)\",\n       size = \"Masse (g)\") +\n  scale_fill_viridis_c()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nLa palette viridis est proposée pour les échelles continues (d’où le _c à la fin du nom de fonction), ou pour les échelles discrètes (scale_fill_viridis_d). Des fonctions équivalentes existent pour les couleurs de contour (scale_color_viridis_c et scale_color_viridis_d). Allez lire le fichier d’aide de cette fonction pour en apprendre plus sur son fonctionnement et ses nombreuses options.\nIci, les individus les plus lourds apparaissent en jaune, et les plus légers en bleu sombre. Si on souhaite faire le contraire, c’est possible :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = body_mass_g, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  labs(title = \"Forme du bec de 3 espèces de manchots et relation avec leur masse\",\n       subtitle = \"Les manchots les plus lourds ont des becs longs et fins\",\n       x = \"Longueur du bec (mm)\",\n       y = \"Épaisseur du bec (mm)\",\n       caption = \"Source :  package 'palmerpenguins'\",\n       fill = \"Masse (g)\",\n       size = \"Masse (g)\") +\n  scale_fill_viridis_c(direction = -1)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nD’autres palettes de couleurs sont également accessibles grâce à l’argument option = de ces fonctions :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = body_mass_g, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  labs(title = \"Forme du bec de 3 espèces de manchots et relation avec leur masse\",\n       subtitle = \"Les manchots les plus lourds ont des becs longs et fins\",\n       x = \"Longueur du bec (mm)\",\n       y = \"Épaisseur du bec (mm)\",\n       caption = \"Source :  package 'palmerpenguins'\",\n       fill = \"Masse (g)\",\n       size = \"Masse (g)\") +\n  scale_fill_viridis_c(option = \"A\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nVoici toutes les possibilités :\n\n\n\n\n\nDernière chose concernant viridis : la fonction scale_fill_viridis_b() discrétise la variable continue pour en faire une échelle discontinue. Il est en effet parfois plus facile de repérer une couleur parmi une palette de 4 ou 5 couleurs distinctes, plutôt qu’au sein d’un gradient. Voilà à quoi cela ressemble :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = body_mass_g, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  labs(title = \"Forme du bec de 3 espèces de manchots et relation avec leur masse\",\n       subtitle = \"Les manchots les plus lourds ont des becs longs et fins\",\n       x = \"Longueur du bec (mm)\",\n       y = \"Épaisseur du bec (mm)\",\n       caption = \"Source :  package 'palmerpenguins'\",\n       fill = \"Masse (g)\",\n       size = \"Masse (g)\") +\n  scale_fill_viridis_b(option = \"E\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nOutre les fonctions d’échelles proposant les palettes viridis, les fonctions se terminant par _gradient(), _gradient2() et _gradientn() permettent de spécifier manuellement les couleurs à intégrer dans un dégradé. Avec la fonction scale_fill_gradient() on indique simplement les couleurs du début de de la fin du gradient :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = body_mass_g, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  labs(title = \"Forme du bec de 3 espèces de manchots et relation avec leur masse\",\n       subtitle = \"Les manchots les plus lourds ont des becs longs et fins\",\n       x = \"Longueur du bec (mm)\",\n       y = \"Épaisseur du bec (mm)\",\n       caption = \"Source :  package 'palmerpenguins'\",\n       fill = \"Masse (g)\",\n       size = \"Masse (g)\") +\n  scale_fill_gradient(low = \"gold\", high = \"firebrick3\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nN’importe quelle nom de couleur valide, ou n’importe que code couleur hexadécimal fonctionne (voir par exemple ce site pour trouver les codes hexadécimaux dont vous avez besoin) :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = body_mass_g, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  labs(title = \"Forme du bec de 3 espèces de manchots et relation avec leur masse\",\n       subtitle = \"Les manchots les plus lourds ont des becs longs et fins\",\n       x = \"Longueur du bec (mm)\",\n       y = \"Épaisseur du bec (mm)\",\n       caption = \"Source :  package 'palmerpenguins'\",\n       fill = \"Masse (g)\",\n       size = \"Masse (g)\") +\n  scale_fill_gradient(low = \"#A0F87D\", high = \"#151197\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nAvec la fonction scale_fill_gradient2(), nous avons plus de contrôle : on indique une couleur de départ, une couleur d’arrivée, mais aussi, une couleur intermédiaire, et la valeur numérique à laquelle cette couleur doit apparaître :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = body_mass_g, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  labs(title = \"Forme du bec de 3 espèces de manchots et relation avec leur masse\",\n       subtitle = \"Les manchots les plus lourds ont des becs longs et fins\",\n       x = \"Longueur du bec (mm)\",\n       y = \"Épaisseur du bec (mm)\",\n       caption = \"Source :  package 'palmerpenguins'\",\n       fill = \"Masse (g)\",\n       size = \"Masse (g)\") +\n  scale_fill_gradient2(low = \"deeppink\", high = \"gold\", mid = \"darkslateblue\",\n                       midpoint = 4500)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nJe vous laisse explorer l’aide de cette fonction ainsi que celle de scale_fill_gradientn() pour savoir comment en utiliser toutes les possibilités.\nLorsqu’une variable catégorielle est associée à la couleur, il est évidemment possible aussi d’effectuer les choix de palettes. Voyons en exemple en associant la couleur de remplissage des points à l’espèce plutôt qu’à la masse :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = species, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  labs(title = \"Forme du bec de 3 espèces de manchots et relation avec leur masse\",\n       subtitle = \"Les manchots les plus lourds ont des becs longs et fins\",\n       x = \"Longueur du bec (mm)\",\n       y = \"Épaisseur du bec (mm)\",\n       caption = \"Source :  package 'palmerpenguins'\",\n       fill = \"Espèce\",\n       size = \"Masse (g)\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nLa version discrète de viridis peut maintenant être appliquée. Les mêmes options que pour la version continue sont disponibles :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = species, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  labs(title = \"Forme du bec de 3 espèces de manchots et relation avec leur masse\",\n       subtitle = \"Les manchots les plus lourds ont des becs longs et fins\",\n       x = \"Longueur du bec (mm)\",\n       y = \"Épaisseur du bec (mm)\",\n       caption = \"Source :  package 'palmerpenguins'\",\n       fill = \"Espèce\",\n       size = \"Masse (g)\") +\n  scale_fill_viridis_d(option = \"B\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nLes possibilités sont nombreuses, notamment grâce aux fonctions scale_fill_brewer() (pour les couleurs de remplissages associées à une variable catégorielle) et scale_color_brewer() (pour les couleurs de contour associées à une variable catégorielle). L’utilisation est simple, un précise simplement quelle palette on souhaite utiliser parmi la liste des palettes disponibles :\n\nCertaines palettes sont séquentielles (lorsque les catégories se suivent logiquement, pour les variables catégorielles ordinales en particulier), d’autres contiennent des couleurs indépendantes :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = species, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  labs(title = \"Forme du bec de 3 espèces de manchots et relation avec leur masse\",\n       subtitle = \"Les manchots les plus lourds ont des becs longs et fins\",\n       x = \"Longueur du bec (mm)\",\n       y = \"Épaisseur du bec (mm)\",\n       caption = \"Source :  package 'palmerpenguins'\",\n       fill = \"Espèce\",\n       size = \"Masse (g)\") +\n  scale_fill_brewer(palette = \"Accent\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nEnfin, il est possible de spécifier manuellement la liste des couleurs que l’on souhaite utiliser avec la fonction scale_fill_manual(). il faut bien entendu indiquer autant de couleurs que de modalités pour notre variable catégorielle (ici 3 espèces dont 3 couleurs) :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = species, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  labs(title = \"Forme du bec de 3 espèces de manchots et relation avec leur masse\",\n       subtitle = \"Les manchots les plus lourds ont des becs longs et fins\",\n       x = \"Longueur du bec (mm)\",\n       y = \"Épaisseur du bec (mm)\",\n       caption = \"Source :  package 'palmerpenguins'\",\n       fill = \"Espèce\",\n       size = \"Masse (g)\") +\n  scale_fill_manual(values = c(\"deepskyblue\", \"darkorchid3\", \"lightsalmon\"))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nDernière chose concernant les couleurs : un choix de fonction scale_XXX_XXX() inapproprié est une cause d’erreur très fréquente ! Par exemple, pour la première figure de la partie consacrée au paradoxe de Simpson (Section 3.10.3), la couleur des points et des lignes n’est pas spécifiée avec fill mais avec color. C’est donc bien une fonction qui commence par scale_color_ qu’il faut utiliser :\n\nggplot(penguins, aes(x = bill_depth_mm, y = flipper_length_mm,\n                     color = species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Épaisseur du bec (mm)\", y = \"Longueur des nageoires (mm)\",\n       color = \"Espèce\") +\n  scale_color_brewer(palette = \"Set2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nComme pour les fonctions geom_XXX(), les fonctions scale_color_XXX() et scale_fill_XXX() sont très nombreuses. Je vous encourage donc à explorer les fichiers d’aide et à faire des essais.\n\n\n3.11.2.2 Les autres échelles\nLes deux autres échelles que vous pourrez être couramment appelés à modifier sont les échelles des axes des x et des y. Les fonctions qui permettent de la faire sont construites comme ces des échelles de couleurs : scale_x_XXX() et scale_y_XXX(). La dernière partie du nom de la fonction sera, la plupart du temps, soit discrete si une variable catégorielle est associée à l’axe, soit continuous si une variable numérique y est associée.\nReprenons l’exemple du stripchart suivant :\n\nggplot(penguins, aes(x = species, y = bill_depth_mm)) +\n  geom_jitter(width = 0.20, height = 0)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nOn commence par légender les axes avec labs() :\n\nggplot(penguins, aes(x = species, y = bill_depth_mm)) +\n  geom_jitter(width = 0.20, height = 0) +\n  labs(x = \"Espèce\", y = \"Épaisseur du bec (mm)\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nSi on souhaite renommer l’espèce Adelie en Adélie (avec un accent sur le “e” donc), il faut modifier l’échelle de l’axe des x, qui porte une variable catégorielle :\n\nggplot(penguins, aes(x = species, y = bill_depth_mm)) +\n  geom_jitter(width = 0.20, height = 0) +\n  labs(x = \"Espèce\", y = \"Épaisseur du bec (mm)\") +\n  scale_x_discrete(label = c(\"Adélie\", \"Chinstrap\", \"Gentoo\"))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nPour l’axe des y, qui porte une variable continue, on peut avoir besoin de faire apparaître des graduations tous les 2 millimètres (au lieu de tous les 2,5 millimètres) :\n\nggplot(penguins, aes(x = species, y = bill_depth_mm)) +\n  geom_jitter(width = 0.20, height = 0) +\n  labs(x = \"Espèce\", y = \"Épaisseur du bec (mm)\") +\n  scale_x_discrete(label = c(\"Adélie\", \"Chinstrap\", \"Gentoo\")) +\n  scale_y_continuous(breaks = seq(from = 12, to = 22, by = 2))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nIl est également très fréquent de souhaiter étendre les axes au-delà des seules valeurs observées, pour faire apparaître le 0 par exemple. C’est tellement fréquent qu’une fonction de raccourci très facile à utiliser nous permet d’éviter le recours à une fonction scale_XXX_XXX(). Même si dans ce cas précis, ça n’est pas très pertinent, voilà un exemple :\n\nggplot(penguins, aes(x = species, y = bill_depth_mm)) +\n  geom_jitter(width = 0.20, height = 0) +\n  labs(x = \"Espèce\", y = \"Épaisseur du bec (mm)\") +\n  scale_x_discrete(label = c(\"Adélie\", \"Chinstrap\", \"Gentoo\")) +\n  expand_limits(y = 0)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nEnfin, il arrive que les valeurs prises par une variable numérique recouvrent plusieurs ordres de grandeurs (avec par exemple des valeurs de l’ordre des dizaines, des centaines et des milliers). Utiliser une échelle logarithmique permet, dans cette situation, de mieux visualiser la variabilité des données, notamment parmi les valeurs les plus faibles. Les fonctions scale_x_log10() et scale_y_log10() permettent d’effectuer ce changement d’échelle tout en conservant des valeurs normales sur les axes.\nOutre ces changements d’échelles pour les axes et les couleurs, il est possible de modifier manuellement toutes les échelles générées automatiquement par les fonction geom_XXX() (par exemple, l’échelle des tailles, ou les types de symboles utilisés pour distinguer plusieurs catégories de points). Il “suffit” pour cela de trouver la bonne fonction (par exemple scale_size_continuous(), scale_shape_manual(), … Il est evidemment impossible de faire le tour de toutes ces fonctions. Mais sachez qu’elles existent et consultez leurs fichiers d’aide le jour où vous en avez besoin.\n\n\n\n3.11.3 Les thèmes\nL’apparence de tout ce qui ne concerne pas directement les données d’un graphique est sous le contrôle d’un thème. Les thèmes contrôlent l’apparence générale du graphique : quelles polices et tailles de caractères sont utilisées, quel sera l’arrière plan du graphique, faut-il intégrer un quadrillage sous le graphique, et si oui, quelles doivent être ses caractéristiques ?\nIl est possible de spécifier chaque élément manuellement. Nous nous contenterons ici de passer en revue quelques thèmes prédéfinis qui devraient couvrir la plupart de vos besoins.\nReprenons par exemple le code suivant :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = species, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  labs(title = \"Forme du bec de 3 espèces de manchots et relation avec leur masse\",\n       subtitle = \"Les manchots les plus lourds ont des becs longs et fins\",\n       x = \"Longueur du bec (mm)\",\n       y = \"Épaisseur du bec (mm)\",\n       caption = \"Source :  package 'palmerpenguins'\",\n       fill = \"Espèce\",\n       size = \"Masse (g)\") +\n  scale_fill_viridis_d(option = \"B\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nLe thème utilisé par défaut est theme_gray(). Il est notamment responsable de l’arrière plan gris et du quadrillage blanc. Pour changer de thème, il suffit d’ajouter une couche au graphique en donnant le nom du nouveau thème :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = species, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  labs(title = \"Forme du bec de 3 espèces de manchots et relation avec leur masse\",\n       subtitle = \"Les manchots les plus lourds ont des becs longs et fins\",\n       x = \"Longueur du bec (mm)\",\n       y = \"Épaisseur du bec (mm)\",\n       caption = \"Source :  package 'palmerpenguins'\",\n       fill = \"Espèce\",\n       size = \"Masse (g)\") +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_bw()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nLe fond gris a disparu, et le quadrillage a changé de couleur. Les thèmes complets proposés par ggplot2 que vous pouvez utiliser sont les suivants :\n\ntheme_bw() : fond blanc et quadrillage.\ntheme_classic() : thème classique, avec des axes mais pas de quadrillage.\ntheme_dark() : fond sombre pour augmenter le contraste.\ntheme_gray() : thème par défaut : fond gris et quadrillage blanc.\ntheme_light() : axes et quadrillages discrets.\ntheme_linedraw() : uniquement des lignes noires.\ntheme_minimal() : pas d’arrière plan, pas d’axes, quadrillage discret.\ntheme_void() : thème vide, seuls les objets géométriques restent visibles.\n\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = species, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  labs(title = \"Forme du bec de 3 espèces de manchots et relation avec leur masse\",\n       subtitle = \"Les manchots les plus lourds ont des becs longs et fins\",\n       x = \"Longueur du bec (mm)\",\n       y = \"Épaisseur du bec (mm)\",\n       caption = \"Source :  package 'palmerpenguins'\",\n       fill = \"Espèce\",\n       size = \"Masse (g)\") +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_minimal()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nTous les thèmes possèdent la même liste d’argument. L’un d’entre eux est l’argument base_family, qui permet de spécifier une police de caractères différente de celle utilisée par défaut. Évidemment, vous ne pourrez utiliser que des polices qui sont disponibles sur l’ordinateur que vous utilisez. Un bon tutoriel expliquant comment indiquer à R les polices qui sont disponibles sur votre ordinateur est disponible ici. N’hésitez pas à revenir vers moi pour toute question à ce sujet.\nDans l’exemple ci-dessous, j’utilise la police “Gill Sans”. Si cette police n’est pas disponible sur votre ordinateur, ce code produira une erreur (ou R prendra simplement la police par défaut. Si c’est le cas, remplacez-la par une police de votre ordinateur. Attention, son nom exact doit être utilisé. Cela signifie bien sûr le respect des espaces, majuscules, etc.\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = species, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  labs(title = \"Forme du bec de 3 espèces de manchots et relation avec leur masse\",\n       subtitle = \"Les manchots les plus lourds ont des becs longs et fins\",\n       x = \"Longueur du bec (mm)\",\n       y = \"Épaisseur du bec (mm)\",\n       caption = \"Source :  package 'palmerpenguins'\",\n       fill = \"Espèce\",\n       size = \"Masse (g)\") +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_minimal(base_family = \"Gill Sans\")\n\n\nIl est également possible de spécifier la taille de police qui devrait être utilisée par défaut. On spécifie la taille de base avec l’argument base_size =, et toutes les autres tailles de polices seront mises à jour pour refléter le changement. Ainsi, les différences de tailles entre titre, sous-titres, légendes des axes, etc, seront maintenues :\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm,\n                     fill = species, size = body_mass_g)) +\n  geom_point(shape = 21, color = \"black\", alpha = 0.6) +\n  labs(title = \"Forme du bec des manchots et relation avec leur masse\",\n       subtitle = \"Les manchots les plus lourds ont des becs longs et fins\",\n       x = \"Longueur du bec (mm)\",\n       y = \"Épaisseur du bec (mm)\",\n       caption = \"Source :  package 'palmerpenguins'\",\n       fill = \"Espèce\",\n       size = \"Masse (g)\") +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_minimal(base_family = \"Gill Sans\", base_size = 14)\n\n\nLe choix d’un thème et d’une police adaptés doivent vous permettre de faire des graphiques originaux et clairs. Rappelez-vous toujours que vos choix en matière de graphiques doivent avoir pour objectif principal de rendre les tendances plus faciles à décrypter pour un lecteur non familier de vos données. C’est un outil de communication au même titre que n’importe quel paragraphe d’un rapport ou compte-rendu. Et comme pour un paragraphe, la première version d’un graphique est rarement la bonne.\nVous devriez donc maintenant être bien armés pour produire 95% des graphiques dont vous aurez besoin tout au long de votre cursus universitaire. Toutefois, un point important a pour l’instant été omis : l’ajout de barres d’erreurs sur vos graphiques. Nous verrons comment faire cela au Chapitre 8, après avoir appris à manipuler efficacement des tableaux de données avec les packages tidyr et dplyr dans les chapitres 4 et 5 respectivement.\nQuoi qu’il en soit, il est maintenant attendu de vous que vous utilisez R et ce que vous avez appris de ggplot2 pour produire tous les graphiques que vous serez amenés à intégrer à vos comptes-rendus de TP et à vos rapports."
  },
  {
    "objectID": "03-Visualization.html#pour-conclure",
    "href": "03-Visualization.html#pour-conclure",
    "title": "3  Visualiser des données avec ggplot2",
    "section": "3.12 Pour conclure…",
    "text": "3.12 Pour conclure…\nQuoi que vous en pensiez, nous n’avons fait qu’effleurer ce qu’il est possible de faire avec R en matière de visualisation de données. Par exemple, il est possible de se servir du logiciel et de ses possibilités graphiques très puissantes comme d’un SIG. Par ailleurs, de très nombreux types de visualisations originales permettent, entre autres, d’explorer différemment des distributions statistiques, ou de visualiser des relations complexes entre variables. Par exemple, essayez de voir à quoi ressemble un violin plot, un lollipop plot ou un cleveland dot plot et comment les faire dans R. Ou encore, jetez un œil au package ggridges et tentez de comprendre à quoi il sert et dans quelle situation il peut être intéressant de l’utiliser.\nEnfin, je vous encourage vivement à faire preuve de curiosité et à explorer de votre côté. C’est comme cela que vous trouverez des idées originales vous permettant de mettre vos données en valeur. De très nombreux exemples (avec le code !) sont notamment disponibles sur le site de la R graph gallery. Je ne peux que vous encourager à aller y faire un tour de temps en temps…"
  },
  {
    "objectID": "03-Visualization.html#exercices",
    "href": "03-Visualization.html#exercices",
    "title": "3  Visualiser des données avec ggplot2",
    "section": "3.13 Exercices",
    "text": "3.13 Exercices\n\nAvec le jeu de données diamonds, du packages ggplot2, tapez les commandes suivantes pour créer un nouveau tableau diams contenant moins de lignes (3000 au lieu de près de 54000) :\n\n\nlibrary(dplyr)\nset.seed(4532) # Afin que tout le monde récupère les mêmes lignes\ndiams &lt;- diamonds %&gt;%\n  sample_n(3000)\n\n\nAvec ce nouveau tableau diams, tapez le code permettant de créer le graphique ci-dessous. Indice : affichez le tableau diams dans la console afin de voir quelles sont les variables disponibles.\n\n\n\n\n\n\nPrix de 3000 diamants en fonction de leur taille en carats et de leur clarté.\n\n\n\n\n\nSelon vous, à quoi sont dues les bandes verticales que l’on observe sur ce graphique ?\nInstallez et chargez en mémoire le package nycflights13\nExaminez le tableau flights de ce package, et lisez son fichier d’aide pour comprendre à quoi correspondent ces données\nCréer un nouveau jeu de données en exécutant ces commandes :\n\n\nset.seed(1234)\nsmall_flights &lt;- flights %&gt;%\n  filter(!is.na(arr_delay),\n         distance &lt; 3000)  %&gt;%\n  sample_n(1000)\n\n\nCe nouveau jeu de données de petite taille (1000 lignes) est nommé small_flights. Il contient les mêmes variables que le tableau flights mais ne contient qu’une petite fraction de ses lignes. Les lignes retenues ont été choisies au hasard. Vous pouvez visualiser son contenu en tapant son nom dans la console ou en utilisant la fonction View().\n\nEn vous appuyant sur les fonctions et les principes de la grammaire des graphiques que vous avez découverts dans ce chapitre, et en vous servant de ce nouveau jeu de données, tapez les commandes qui permettent de produire le graphique ci-dessous :\n\n\n\n\n\nQuelques indices :\n\nLes couleurs utilisées sont celles de la palette Set1 du package RColorBrewer.\nLes variables utilisées sont origin, air_time et distance.\nLa transparence des symboles est fixée à 0.8.\n\n\nToujours avec ce jeu de données small-flights, tapez les commandes permettant de produire le graphique ci-dessous :\n\n\n\n\n\n\nQuelques indices :\n\nLes couleurs utilisées sont celles de la palettes Accent du package RColorBrewer.\nLes variables utilisées sont month, carrier et origin.\nLa variable month est codée sous forme numérique dans le tableau de données. Il faudra la transformer en facteur avec la fonction factor(month) au moment de l’associer à un axe du graphique.\n\n\n\n\n\n\nJeppson, Haley, Heike Hofmann, et Di Cook. 2021. ggmosaic: Mosaic Plots in the ggplot2 Framework. https://github.com/haleyjeppson/ggmosaic.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, et Teun van den Brand. 2024. ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWilkinson, Leland. 2005. The grammar of graphics. 2nd éd. New-York: Springer-Verlag. https://www.springer.com/us/book/9780387245447."
  },
  {
    "objectID": "04-TidyData.html#préambule",
    "href": "04-TidyData.html#préambule",
    "title": "4  (Ar)ranger des données avec tidyr",
    "section": "4.1 Préambule",
    "text": "4.1 Préambule\nDans la Section 1.3.4.1, nous avons introduit le concept de tableaux de données ou data.frame dans R. Il s’agit d’une représentation rectangulaire des données, à la manière d’un tableur, dans laquelle les lignes correspondent aux observations et les colonnes correspondent à des variables décrivant chaque observation.\nDans ce chapitre, nous allons aller plus loin en présentant le concept de tidy data, ou “données nettes/rangées/soignées/ordonnées”. Vous verrez que l’idée d’avoir des données stockées dans un format “net” va plus loin que la simple définition usuelle que le terme “rangé” peut avoir lorsque les données sont simplement bien organisées dans un tableur. Nous définirons le terme “tidy data” de manière plus rigoureuse, en établissant un ensemble de règles permettant de stocker les données correctement afin de rendre plus aisées les analyses statistiques et les représentations graphiques.\nJusqu’à maintenant, vous avez utilisé des données qui étaient déjà dans ce format (c’est le cas des données contenues dans penguins, ou dans diamonds par exemple). Pourtant, la plupart du temps, les données que vous manipulerez dans R seront importées depuis un tableur dans lequel vous ou vos collaborateurs en aurez fait la saisie. S’assurer que les données importées manuellement dans R sont correctement “nettoyées” et mises en forme de “tidy data” est indispensable pour éviter les problèmes lors de la réalisation de graphiques (voir Chapitre 3) comme lors de la manipulation des données pour en tirer de l’information statistique pertinente (ce que nous verrons au Chapitre 5)."
  },
  {
    "objectID": "04-TidyData.html#sec-prerek",
    "href": "04-TidyData.html#sec-prerek",
    "title": "4  (Ar)ranger des données avec tidyr",
    "section": "4.2 Pré-requis",
    "text": "4.2 Pré-requis\nDans ce chapitre, nous aurons besoin des packages suivants :\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(readxl)  # la dernière lettre est un \"L\" minuscule, pas le chiffre 1...\n\nComme d’habitude, si vous recevez des messages d’erreur, c’est probablement parce que le package que vous essayez de charger en mémoire n’a pas été installé au préalable. Consultez la Section 1.4 si vous ne savez plus comment procéder.\nOutre ces packages classiques, nous aurons aussi besoin du package EDAWR qui n’est pas disponible sur les serveurs habituels de R. Pour l’installer, on procède de la façon suivante :\n\nInstallez et chargez en mémoire le package remotes :\n\n\ninstall.packages(\"remotes\")\nlibrary(remotes)\n\n\nInstallez le package EDAWR grâce à la fonction install_github() du package remotes qui va chercher le package sur le site https://github.com :\n\n\ninstall_github(\"rstudio/EDAWR\")\n\nAttention, sur les ordinateurs de l’université cette procédure ne fonctionne pas toujours. Si vous rencontrez des difficultés, suivez les instructions décrites à la fin de cette Section 4.2.\n\nChargez le package EDAWR de la façon habituelle :\n\n\nlibrary(EDAWR)\n\nLe package EDAWR contient plusieurs jeux de données dont nous allons nous servir pour illustrer les questions liées au format des tableaux de données. Pour en avoir la liste, vous pouvez taper :\n\ndata(package = \"EDAWR\")\n\nEn cas de problème pour installer le package EDAWR sur les ordinateurs de l’université.\nVous pouvez télécharger manuellement les 4 jeux de données dont nous aurons besoin grâce à ces 4 liens :\n\ncases\npopulation\nrates\nstorms\n\nUne fois téléchargés, les données contenues dans ces 4 fichiers peuvent être importées dans RStudio en cliquant sur File &gt; Open File..., puis en sélectionnant un à un chacun des fichiers. Pour chaque fichier un nouvel objet doit apparaître dans votre environnement de travail (onglet Environnement, dans le panneau en haut à droite de RStudio). L’inconvénient de cette méthode est que les fichiers d’aide de ces jeux de données ne seront pas disponibles dans RStudio. Vous pouvez toutefois en consulter une version brute (non mise en forme) en cliquant ici."
  },
  {
    "objectID": "04-TidyData.html#cest-quoi-des-tidy-data",
    "href": "04-TidyData.html#cest-quoi-des-tidy-data",
    "title": "4  (Ar)ranger des données avec tidyr",
    "section": "4.3 C’est quoi des “tidy data” ?",
    "text": "4.3 C’est quoi des “tidy data” ?\nLes “tidy data” (nous les appellerons “données rangées” dans la suite de ce livre), sont des données qui respectent un format standardisé. En particulier :\n\nChaque variable est dans une colonne unique.\nChaque colonne contient une unique variable.\nChaque ligne correspond à une observation pour chaque variable.\nLes cellules du tableau représentent les valeurs de chaque observation pour chaque variable.\n\n\n\n\n\n\nFigure 4.1: La définition des ‘données rangées’, d’après http://r4ds.had.co.nz/tidy-data.html.\n\n\n\n\nMalheureusement, les données peuvent être présentées sous de nombreux formats qui ne respectent pas ces règles de base. La modification des tableaux est donc souvent un préambule nécessaire à toute analyse statistique ou représentation graphique.\nPar exemple, examinez le tableau cases du package EDAWR, qui présente le nombre de cas de tuberculose dans 3 pays en 2011, 2012 et 2013.\n\ncases\n\n  country  2011  2012  2013\n1      FR  7000  6900  7000\n2      DE  5800  6000  6200\n3      US 15000 14000 13000\n\n\nDans ce tableau, essayez d’identifier quelles sont les variables en présence. Indice, vous devriez en trouver 3.\nEssayez d’identifier également où se trouvent ces variables.\nPour ma part, je compte les 3 variables suivantes :\n\ncountry : qui indique les pays dans lesquels les cas de tuberculose ont été dénombrés. Cette variable occupe la première colonne du tableau.\nLa seconde variable est l’année, qui peut prendre les valeurs 2011, 2012 ou 2013. Cette variable occupe la ligne des titres des 3 colonnes de droite du tableau.\nEt enfin, la troisième variable est le nombre de cas de tuberculose observés dans chaque pays et chaque année. Cette troisième variable occupe 3 lignes et 3 colonnes du tableau.\n\nAutrement dit, les variables peuvent être visualisées de la façon suivante :\n\n\n\n\n\nFigure 4.2: Position des variables dans le tableau cases du package EDAWR.\n\n\n\n\nDonc même si nous disposons ici d’un tableau rectangulaire classique, nous sommes bien loin du format des données rangées.\n\n4.3.1 La fonction pivot_longer()\nAfin de transformer les données non rangées du tableau cases en données rangées, nous allons utiliser la fonction pivot_longer() du package tidyr. Avant d’aller plus loin, essayez d’imaginer à quoi le tableau rangé devrait ressembler.\nLa fonction pivot_longer() prend 4 arguments :\n\ndata : le nom du tableau de données que l’on souhaite “ranger”.\ncols : La liste des colonnes du tableau initial que l’on souhaite rassembler en 2 nouvelles variables. Ici, les colonnes 2, 3 et 4 (on pourra les noter 2:4 ou, en utilisant leur nom, \"2011\":\"2013\").\nnames_to : le nom d’une nouvelle variable qui contiendra les en-têtes des colonnes qui constituent la seconde variable. Ici, nous nommerons cette seconde variable year car elle devra contenir les années 2011, 2012 et 2013.\nvalues_to : le nom d’une nouvelle variable qui contiendra les informations correspondant à la troisième variable identifiée plus haut. Nous appellerons cette variables n_cases car elle contiendra les nombres de cas de tuberculose (7000, 5800, 15000, etc).\n\n\npivot_longer(data = cases, \n             cols = `2011`:`2013`, \n             names_to = \"year\", \n             values_to = \"n_cases\")\n\n# A tibble: 9 × 3\n  country year  n_cases\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n1 FR      2011     7000\n2 FR      2012     6900\n3 FR      2013     7000\n4 DE      2011     5800\n5 DE      2012     6000\n6 DE      2013     6200\n7 US      2011    15000\n8 US      2012    14000\n9 US      2013    13000\n\n\nNous avons bien transformé le tableau de départ en un “tableau rangé” : chacune de nos 3 variables se trouve dans une unique colone, et chaque ligne correspond à une observation pour chacune de ces 3 variables. Comme d’habitude, si nous souhaitons pouvoir utiliser ce nouveau tableau, il faut lui donner un nom :\n\ncases_tidy &lt;- pivot_longer(data = cases, \n                           cols = `2011`:`2013`, \n                           names_to = \"year\", \n                           values_to = \"n_cases\")\n\nIl nous est maintenant plus facile de manipuler ces données pour en tirer de l’information, grâce à des analyses statistiques ou des représentations graphiques :\n\nggplot(cases_tidy, aes(x = country, y = n_cases, fill = year)) +\n  geom_col(position = \"dodge\", color = \"black\") +\n  scale_fill_brewer(palette = \"Accent\") +\n  theme_minimal() +\n  labs(x = \"Pays\",\n       y = \"Nombre de cas\",\n       fill = \"Année\",\n       title = \"Évolution du nombre de cas de tuberculose entre 2011 et 2013\",\n       subtitle = \"DE : Allemagne, FR : France, US : États-Unis\")\n\n\n\n\nFigure 4.3: Évolution du nombre de cas de tuberculose dans 3 pays, de 2011 à 2013.\n\n\n\n\nOn constate ici qu’entre 2011 et 2013, le nombre de cas de tuberculose a légèrement augmenté en Allemagne, est resté stable en France, et a diminué aux États-Unis.\nNotez ici que la variable year de notre nouveau tableau est considérée comme une variable de type “chaîne de caractères” et non comme une variable numérique. On peut le voir en affichant notre tableau en tapant son nom, ou en utilisant la fonction str() déjà décrite plus tôt :\n\nstr(cases_tidy)\n\ntibble [9 × 3] (S3: tbl_df/tbl/data.frame)\n $ country: chr [1:9] \"FR\" \"FR\" \"FR\" \"DE\" ...\n $ year   : chr [1:9] \"2011\" \"2012\" \"2013\" \"2011\" ...\n $ n_cases: num [1:9] 7000 6900 7000 5800 6000 6200 15000 14000 13000\n\n\nC’est le comportement par défaut de la fonction pivot_longer() : les anciens titres de colonnes sont convertis en chaînes de caractères. Si ce comportement n’est pas souhaitable, il y a 2 alternatives possibles :\n\nutiliser les arguments names_transform et/ou values_transform de la fonction pivot_longer(). Cela permet de spécifier comment transformer les variables nouvellement créées au moment de leur création.\nutiliser les fonctions mutate() et as.numeric() ou as.integer() après avoir modifié le tableau de départ avec pivot_longer(). Cette façon de faire sera décrite dans la Section 5.6.\n\n\n# On commence par afficher `cases`\ncases \n\n  country  2011  2012  2013\n1      FR  7000  6900  7000\n2      DE  5800  6000  6200\n3      US 15000 14000 13000\n\n# On utilise ensuite pivot_longer() avec l'argument \n# names_transform pour transformer year en variable entière\npivot_longer(data = cases, \n             cols = `2011`:`2013`, \n             names_to = \"year\", \n             values_to = \"n_cases\",\n             names_transform = list(year = as.integer))\n\n# A tibble: 9 × 3\n  country  year n_cases\n  &lt;chr&gt;   &lt;int&gt;   &lt;dbl&gt;\n1 FR       2011    7000\n2 FR       2012    6900\n3 FR       2013    7000\n4 DE       2011    5800\n5 DE       2012    6000\n6 DE       2013    6200\n7 US       2011   15000\n8 US       2012   14000\n9 US       2013   13000\n\n\nOn voit ici que la variable year est maintenant une colonne numérique (&lt;int&gt; : nombres entiers), et non plus une variable de type “character”. En utilisant as.numeric() au lieu de as.integer(), on aurait transformé la variable year en &lt;dbl&gt; (nombre réel au lieu de nombre entier), ce qui ici, reviendrait exactement au même.\nDe la même façon, on peut avoir besoin de présenter la colonne year sous la forme d’un facteur :\n\npivot_longer(data = cases, \n             cols = `2011`:`2013`, \n             names_to = \"year\", \n             values_to = \"n_cases\",\n             names_transform = list(year = as.factor))\n\n# A tibble: 9 × 3\n  country year  n_cases\n  &lt;chr&gt;   &lt;fct&gt;   &lt;dbl&gt;\n1 FR      2011     7000\n2 FR      2012     6900\n3 FR      2013     7000\n4 DE      2011     5800\n5 DE      2012     6000\n6 DE      2013     6200\n7 US      2011    15000\n8 US      2012    14000\n9 US      2013    13000\n\n\n\n\n4.3.2 La fonction pivot_wider()\nLa fonction pivot_wider() permet de réaliser l’opération inverse de pivot_longer(). Elle “disperse” une unique colonne catégorielle en plusieurs colonnes, le tableau obtenu est donc plus large (“wider”) que le tableau de départ.\nReprenons par exemple notre tableau cases_tidy :\n\ncases_tidy\n\n# A tibble: 9 × 3\n  country year  n_cases\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n1 FR      2011     7000\n2 FR      2012     6900\n3 FR      2013     7000\n4 DE      2011     5800\n5 DE      2012     6000\n6 DE      2013     6200\n7 US      2011    15000\n8 US      2012    14000\n9 US      2013    13000\n\n\nLa fonction pivot_wider() prend 3 arguments :\n\nLe nom du tableau contenant les données (ici, cases_tidy).\nnames_from : le nom de la variable contenant les catégories qui devront être transformées en colonnes (ici, year).\nvalues_from : le nom de la variable contenant les valeurs qui devront remplir les nouvelles colonnes (ici, n_cases).\n\n\npivot_wider(data = cases_tidy, \n            names_from = year, \n            values_from = n_cases)\n\n# A tibble: 3 × 4\n  country `2011` `2012` `2013`\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 FR        7000   6900   7000\n2 DE        5800   6000   6200\n3 US       15000  14000  13000\n\n\nCette fonction sera donc rarement utilisée puisqu’elle ne permet pas d’obtenir des “tableaux rangés”. Toutefois, elle pourra vous être utile dans 2 situations :\n\npour mettre en forme des données appariées afin de réaliser certains tests statistiques et graphiques spécifiques (plus de détails à ce sujet dans le Chapitre 10)\npour présenter des résultats sous forme synthétique. Prenons un exemple avec le jeu de données penguins. Imaginons que vous deviez créer un tableau resum présentant les effectifs de mâles et de femelles des 3 espèces de manchots. Une possibilité serait de taper ceci :\n\n\nresum &lt;- penguins |&gt; \n  group_by(species, sex) |&gt; \n  count()\nresum\n\n# A tibble: 8 × 3\n# Groups:   species, sex [8]\n  species   sex        n\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n\nLes commandes permettant de produire ce tableau seront expliquées dans le Chapitre 5. On peut cependant constater ici que ce tableau contient 8 lignes et 3 colonnes. Il s’agit bien d’un “tableau rangé” parfaitement adapté pour faire des statistiques et des visualisations graphiques, mais son format n’est pas terrible si notre objectif est de le faire figurer dans un rapport. La solution : utiliser pivot_wider() :\n\nresum_large &lt;- pivot_wider(resum, \n            names_from = sex, \n            values_from = n)\n\nresum_large\n\n# A tibble: 3 × 4\n# Groups:   species [3]\n  species   female  male  `NA`\n  &lt;fct&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 Adelie        73    73     6\n2 Chinstrap     34    34    NA\n3 Gentoo        58    61     5\n\n\nCe nouveau tableau contient maintenant 3 lignes (une par espèce), et 4 colonnes : une pour la variable species, et 3 pour la variable sex, soit une colonne pour les femelles, une colonne pour les mâles, et une pour les individus dont le sexe est inconnu (`NA`). On parle de tableau au format large (par opposition au “tableau rangé”, dit “format long”). Cela rend la présentation dans un rapport plus aisée.\nNotez également que pour l’espèce Chinstrap, tous les individus ont été sexés correctement. Le tableau au format large devrait donc indiquer 0 au lieu de NA dans la indique quatrième colonne de la deuxième ligne. En effet, NA signifie “Not Available”, autrement dit : données manquantes. Ici, il ne s’agit pas du tout d’une données manquantes : cela signifie simplement qu’aucun individu de l’espèce Chinstrap ne possède un sexe inconnu. Nous pouvons donc indiquer à R quelle valeur utiliser pour les catégories qui ne sont pas représentées dans le tableau de départ grâce à l’argument values_fill :\n\npivot_wider(resum, \n            names_from = sex, \n            values_from = n, \n            values_fill = 0)\n\n# A tibble: 3 × 4\n# Groups:   species [3]\n  species   female  male  `NA`\n  &lt;fct&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 Adelie        73    73     6\n2 Chinstrap     34    34     0\n3 Gentoo        58    61     5\n\n\nD’autres arguments existent. Je vous encourage vivement à consulter l’aide des fonctions pivot_longer() et pivot_wider() et à faire des essais.\n\n\n4.3.3 Les fonctions separate() et unite()\nCes fonctions sont complémentaires : tout comme pivot_longer() et pivot_wider(), elles effectuent 2 opérations opposées. Reprenons le jeu de données cases_tidy :\n\ncases_tidy\n\n# A tibble: 9 × 3\n  country year  n_cases\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n1 FR      2011     7000\n2 FR      2012     6900\n3 FR      2013     7000\n4 DE      2011     5800\n5 DE      2012     6000\n6 DE      2013     6200\n7 US      2011    15000\n8 US      2012    14000\n9 US      2013    13000\n\n\nImaginons que nous ayons besoin de séparer les données de la colonne year en 2 variables : le siècle d’une part, et l’année d’autre part. La fonction separate() permet de faire exactement cela :\n\nseparate(cases_tidy, year, into = c(\"century\", \"year\"), sep = 2)\n\n# A tibble: 9 × 4\n  country century year  n_cases\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n1 FR      20      11       7000\n2 FR      20      12       6900\n3 FR      20      13       7000\n4 DE      20      11       5800\n5 DE      20      12       6000\n6 DE      20      13       6200\n7 US      20      11      15000\n8 US      20      12      14000\n9 US      20      13      13000\n\n\n\nLe premier argument est le nom du tableau de données.\nLe second argument est la variable que l’on souhaite scinder en plusieurs morceaux.\ninto est un vecteur qui contient le nom des nouvelles colonnes à créer\nsep peut prendre plusieurs formes. Lorsqu’on utilise un nombre, ce nombre correspond à la position de la coupure dans la variable d’origine. Ici, la variable d’origine a été coupée après le second caractère. Il est aussi possible d’utiliser des nombres négatifs : R compte alors à partir de la droite au lieu de compter à partir de la gauche. Enfin, il est aussi possible d’utiliser un symbole. Par exemple, certaines variables contiennent des tirets - ou des slashs \\. Utiliser ces caractères en guise de séparateur permet de couper les variables à ce niveau là. Nous en verrons un exemple plus tard.\n\nNotez ici que les 2 nouvelles variables sont de type &lt;chr&gt;. Si nous souhaitons que ces variables soient considérées comme numériques, nous devons ajouter un argument lorsque nous utilisons separate() :\n\ncases_split &lt;- separate(cases_tidy, \n                        year, \n                        into = c(\"century\", \"year\"), \n                        sep = 2, \n                        convert = TRUE)\ncases_split\n\n# A tibble: 9 × 4\n  country century  year n_cases\n  &lt;chr&gt;     &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;\n1 FR           20    11    7000\n2 FR           20    12    6900\n3 FR           20    13    7000\n4 DE           20    11    5800\n5 DE           20    12    6000\n6 DE           20    13    6200\n7 US           20    11   15000\n8 US           20    12   14000\n9 US           20    13   13000\n\n\nNotre nouvel objet cases_split contient maintenant 2 nouvelles colonnes de nombres entiers, l’une contenant le siècle, l’autre contenant l’année.\nLa fonction unite() fait exactement le contraire : elle fusionne 2 colonnes existantes en accolant leurs contenus (et en ajoutant un séparateur) :\n\nunite(cases_split, new, century, year)\n\n# A tibble: 9 × 3\n  country new   n_cases\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n1 FR      20_11    7000\n2 FR      20_12    6900\n3 FR      20_13    7000\n4 DE      20_11    5800\n5 DE      20_12    6000\n6 DE      20_13    6200\n7 US      20_11   15000\n8 US      20_12   14000\n9 US      20_13   13000\n\n\nLa colonne new a été créée par la fusion des colonnes century et year du tableau cases_split. Si l’on souhaite supprimer le tiret, il nous faut le spécifier explicitement :\n\nunite(cases_split, new, century, year, sep = \"\")\n\n# A tibble: 9 × 3\n  country new   n_cases\n  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n1 FR      2011     7000\n2 FR      2012     6900\n3 FR      2013     7000\n4 DE      2011     5800\n5 DE      2012     6000\n6 DE      2013     6200\n7 US      2011    15000\n8 US      2012    14000\n9 US      2013    13000\n\n\n\n\n4.3.4 Exercices\nExaminez les tableaux rates, storms et population du package EDAWR.\n\nCes tableaux sont-ils des “tableaux rangés” (tidy data) ?\nSi oui, quelles sont les variables représentées ?\nSi non, transformez-les en “tableaux rangés”."
  },
  {
    "objectID": "04-TidyData.html#importer-des-données-depuis-un-tableur",
    "href": "04-TidyData.html#importer-des-données-depuis-un-tableur",
    "title": "4  (Ar)ranger des données avec tidyr",
    "section": "4.4 Importer des données depuis un tableur",
    "text": "4.4 Importer des données depuis un tableur\n\n4.4.1 Les règles de base\nJusqu’à maintenant, nous avons travaillé exclusivement avec des jeux de données déjà disponibles dans R. La plupart du temps, les données sur lesquelles vous devrez travailler devront au préalable être importées dans R ou RStudio, à partir de fichiers issus de tableurs. De tels fichiers se présentent généralement sous l’un des 2 formats suivants :\n\nFichiers au format .csv : il s’agit d’un format de fichier dit “texte brut”, c’est à dire qu’il peut être ouvert avec n’importe quel éditeur de texte, y compris le bloc notes de Windows. L’extension .csv est l’abréviation de Comma Separated Values, autrement dit, dans ce type de fichiers, les colonnes sont séparées par des virgules. Cela peut poser problème en France puisque le symbole des décimales est souvent aussi la virgule (et non le point comme dans les pays anglo-saxons). Le séparateur de colonnes utilisé en France dans les fichiers .csv est alors souvent le point-virgule. Il est possible de créer des fichiers .csv à partir de n’importe quel tableur en choisissant Fichier &gt; Exporter... ou Fichier &gt; Enregistrer sous... puis en sélectionnant le format approprié (les dénominations sont variables selon les logiciels : format texte brut, format csv, plain text, etc…).\nFichiers au format tableur : .xls ou .xlsx pour Excel, .calc pour Open Office.\n\nDans les 2 cas, pour que R puisse importer les données contenues dans ces fichiers, un certain nombre de règles doivent être respectées :\n\nLa première chose à laquelle il faut veiller est la présentation des données. Les variables doivent être en colonnes et les observations en lignes. Dans l’idéal, les données doivent donc être “rangées”.\nLes cases vides qui correspondent à des données manquantes doivent contenir les lettres NA en majuscule. Il est important de bien faire la distinction entre les vrais zéros (i.e. les grandeurs mesurées pour lesquelles un zéro a été obtenu), et les valeurs manquantes, c’est à dire pour lesquelles aucune valeur n’a pu être obtenue (e.g. variable non mesurée pour un individu donné ou à une station donnée).\nIl est généralement conseillé d’utiliser la première ligne du tableau pour stocker le nom des variables.\nPour les noms de fichiers, de colonnes ou dans le contenu des colonnes, il vaut mieux éviter les caractères spéciaux tels que #, $, %, ^, &, *, (, ), {, }, [, ], \\, /, les accents, cédilles, guillemets ou apostrophes… Cela pourrait causer des erreurs dans R. Si votre fichier en contient, faites une recherche (via le menu Edition &gt; Rechercher et remplacer...) pour remplacer chaque instance par un caractère qui ne posera pas de problème.\nÉvitez les espaces dans vos noms de variables, d’observations ou de catégories et remplacez-les par des points ou, de préférence, des tirets bas _.\nSi des noms de lignes sont présents dans votre tableau, chaque ligne doit avoir un nom unique (il ne faut pas que plusieurs lignes portent le même nom).\nDes noms courts pour les variables sont généralement plus faciles à manipuler par la suite.\nLa première valeur de votre tableau devrait toujours se trouver dans la cellule A1 du tableur. Autrement dit, il ne devrait jamais y avoir de lignes incomplètes ou de lignes de commentaires au-dessus des données, ou de colonne vide à gauche de votre tableau. D’ailleurs, il ne devrait jamais y avoir de commentaires à droite ou en dessous de vos données non plus.\n\n\n\n\n\n\n\nDifférence de philosophie\n\n\n\nQuand on travaille dans un tableur, on double clique sur le nom du fichier pour l’ouvrir dans le logiciel approprié (Excel par exemple), puis on en modifie ensuite le contenu en ajoutant ou supprimant des cellules, en modifiant des données déjà existantes ou en créant des graphiques par exemple. Les modifications sont en général sauvegardées au fur et à mesure, manuellement ou automatiquement. Au final, le fichier de départ a été modifié : son contenu n’est plus le même après votre séance de travail et il est virtuellement impossible de garder la trace de toutes les opérations qui ont été effectuées lors de la séance de travail.\nLorsqu’on importe des données issues d’un tableur dans R ou RStudio, la philosophie est totalement différente : les données sont copiées dans la mémoire vive de l’ordinateur au moment de l’importation, et on ne travaille donc pas directement dans le fichier de données. On travaille dans RStudio pour mettre en forme les données, ajouter ou modifier des variables, faire des graphiques et des tests statistiques, en tapant des commandes dans un script. À aucun moment le fichier tableur de départ n’est modifié, à aucun moment son contenu n’est altéré. Tout le travail que l’on effectue sur les données importées n’existe que :\n\ndans la mémoire vive de l’ordinateur,\net sous la forme de commandes tapées dans un script.\n\nPour sauvegarder notre travail, on enregistre notre script. À la prochaine session de travail, nous n’aurons plus qu’à ré-ouvrir notre script dans RStudio, puis à ré-envoyer dans la console la totalité des commandes qu’il contient. Ainsi, les données du fichier tableur seront ré-importées puis toutes les analyses seront exécutées à nouveau et on pourra reprendre le travail là où on l’avait laissé. Cela présente le très gros avantage de ne jamais modifier les données brutes contenues dans le fichier de départ. En outre, un script ne contient en général que quelques centaines de lignes de code au format texte brut. Cela en fait des fichiers très peu volumineux qu’il est facile de dupliquer, modifier, échanger, etc. Ils contiennent la trace de toutes les modifications et opérations que nous faisons subir aux données, ce qui en fait un outil indispensable pour la reproductibilité des analyses.\n\n\n\n\n4.4.2 Fichiers au format tableur (.xls ou .xlsx)\nÀ titre d’exemple, téléchargez le fichier dauphin.xls et placez-le dans votre répertoire de travail. Ce jeu de données contient des résultats de dosages de différents métaux lourds (cadmium, cuivre et mercure) dans différents organes (foie et rein) de plusieurs dauphins communs Delphinus delphis. Les informations de taille, d’âge et de statut reproducteur sont également précisées. Ouvrez ce fichier dans un tableur. Vous constaterez que son format ne permet pas de l’importer tel quel dans R :\n\nIl contient des lignes vides inutiles au-dessus des données.\nIl contient des commentaires inutiles au-dessus des données.\nLes titres de colonnes sont complexes et contiennent des caractères spéciaux.\nDans le tableau, les données manquantes sont représentées soit par des “*”, soit par des cellules vides.\n\nImporter un tel jeu de données dans R par les méthodes classiques (c’est-à-dire sans utiliser RStudio et uniquement grâce aux fonctions de base de R) demanderait donc un gros travail de mise en forme préalable. Heureusement, RStudio et le package readxl facilitent grandement le processus.\nDans RStudio, localisez l’onglet Files situé dans le panneau en bas à droite de l’interface du logiciel. Dans ce panneau, naviguez jusqu’à votre répertoire de travail, qui doit maintenant contenir le fichier daupin.xls que vous avez téléchargé. Cliquez sur son nom, puis, dans le menu qui s’affiche, choisissez Import Dataset... :\n\n\n\n\n\nFigure 4.4: L’option Import Dataset... dans la fenêtre Files de RStudio.\n\n\n\n\nLa nouvelle fenêtre qui s’ouvre est celle de l’assistant d’importation (Figure 4.5).\n\n\n\n\n\nFigure 4.5: L’assistant d’importation de RStudio.\n\n\n\n\nCette fenêtre contient plusieurs zones importantes :\n\nFile/URL (en haut) : lien vers le fichier contenant les données, sur votre ordinateur ou en ligne.\nData Preview : zone principale affichant les 50 premières lignes du fichier que l’on souhaite importer.\nImport Options (en bas à gauche) : zone dans laquelle des options permettant d’importer les données correctement peuvent être spécifiées.\nCode Preview (en bas à droite) : les lignes de codes que vous pourrez copier-coller dans votre script une fois les réglages corrects effectués.\n\nIci, nous constatons que les données ne sont pas au bon format. La première chose que nous pouvons faire est d’indiquer à R que nous souhaitons ignorer les 9 premières lignes du fichier. Ensuite, nous précisons à RStudio que l’étoile “*” a été utilisée pour indiquer des données manquantes (Figure 4.6) :\n\n\n\n\n\nFigure 4.6: Les bons réglages pour ce fichier.\n\n\n\n\nNotez qu’à chaque fois que vous modifiez une valeur dans la zone Import Options, 2 choses se produisent simultanément :\n\nLa zone Data Preview est mise à jour. Cela permet de s’assurer que les changements effectués ont bien les effets escomptés.\nLa zone Code Preview est mise à jour. Cela permet de copier-coller dans un script les commandes permettant d’importer correctement les données. Ici, voilà le code que nous devons ajouter à notre script :\n\n\ndauphin &lt;- read_excel(\"data/dauphin.xls\", na = \"*\", skip = 9)\n\nLa commande library(readxl) est inutile puisque nous l’avons déjà saisie au début de ce chapitre. Nous disposons maintenant d’un nouvel objet nommé dauphin. Il est stocké sous la forme d’un tibble :\n\ndauphin\n\n# A tibble: 93 × 9\n   `N°`      Sexe  `Statut reproducteur` `Taille en cm` `Age en années`\n   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;                          &lt;dbl&gt;           &lt;dbl&gt;\n 1 Numéro 1  f     imm                              315               3\n 2 Numéro 2  f     imm                              357               4\n 3 Numéro 3  f     pnl                              439              34\n 4 Numéro 4  f     imm                              316               4\n 5 Numéro 5  f     l                                435              26\n 6 Numéro 6  f     pnl                              388               6\n 7 Numéro 7  f     mat                              410              NA\n 8 Numéro 8  m     imm                              355              NA\n 9 Numéro 9  m     imm                              222              NA\n10 Numéro 10 m     imm                              412               9\n# ℹ 83 more rows\n# ℹ 4 more variables: `Cd (mg.kg-1)` &lt;dbl&gt;, `Cu (mg.kg-1)` &lt;dbl&gt;,\n#   `Hg (mg.kg-1)` &lt;dbl&gt;, Organe &lt;chr&gt;\n\n\nNotez toutefois que les noms de colonnes complexes sont toujours présents. Avec de tels noms, les variables ne seront pas faciles à manipuler et les risques d’erreurs de frappes seront nombreux. Nous avons tout intérêt à les modifier à l’aide de la fonction names() :\n\nnames(dauphin) &lt;- c(\"ID\", \"Sexe\", \"Statut\", \"Taille\",\n                    \"Age\", \"Cd\", \"Cu\", \"Hg\", \"Organe\")\ndauphin\n\n# A tibble: 93 × 9\n   ID        Sexe  Statut Taille   Age     Cd    Cu    Hg Organe\n   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n 1 Numéro 1  f     imm       315     3  29.6   3.24 NA    rein  \n 2 Numéro 2  f     imm       357     4  55.1   4.42 NA    rein  \n 3 Numéro 3  f     pnl       439    34 129.    5.01  9.02 rein  \n 4 Numéro 4  f     imm       316     4  71.2   4.33 NA    rein  \n 5 Numéro 5  f     l         435    26 192     5.15 NA    rein  \n 6 Numéro 6  f     pnl       388     6  NA     4.12  4.53 rein  \n 7 Numéro 7  f     mat       410    NA  76     5.1  33.9  foie  \n 8 Numéro 8  m     imm       355    NA  74.4   4.72 13.3  foie  \n 9 Numéro 9  m     imm       222    NA   0.09  9.5   2.89 foie  \n10 Numéro 10 m     imm       412     9  85.6   5.42 NA    rein  \n# ℹ 83 more rows\n\n\nEnfin, vous pouvez également noter que certaines variables devraient être modifiées :\n\nLes variables Sexe, Statut (qui contient l’information de statut reproducteur des dauphins) et Organe (qui indique dans quel organe les métaux ont été dosés) sont de type &lt;chr&gt;. L’idéal serait de disposer de facteurs puisqu’ils s’agit de variables catégorielles.\nLa variable ID est totalement inutile puisqu’elle est parfaitement redondante avec le numéro de ligne. Nous pourrions donc la supprimer.\nCertaines catégories (ou niveaux) de la variable Statut devraient être ordonnées puisqu’elles reflètent une progression logique : imm (immature), mat (mature), pnl (pregnant not lactating), pl (pregnant lactating), l (lactating), repos (repos somatique).\n\nNous verrons dans le Chapitre 5 comment effectuer simplement ces différentes opérations.\n\n\n4.4.3 Fichiers au format texte brut (.csv)\nNous allons utiliser les mêmes données que précédemment, mais cette fois-ci, elles sont contenues dans un fichier au format .csv. Téléchargez le fichier dauphin.csv (pour cela, faites un clic droit sur le lien et choisissez Enregistrez la cible du lien sous... ou une mention équivalente), placez-le dans votre répertoire de travail, et ouvrez-le avec le bloc notes Windows ou tout autre éditeur de texte brut disponible sur votre ordinateur. Attention : Microsoft Word n’est pas un éditeur de texte brut. Un fichier au format .doc ou .docx est illisible dans un éditeur de texte brut car outre le texte, ces formats de documents contiennent toutes les informations concernant la mise en forme du texte (polices de caractères, tailles, couleurs et autres attributs, présence de figures, de tableaux dans le document, etc.).\nÀ l’inverse, les fichiers au format .txt, .csv, .tsv et même .R (vos scripts !) sont des fichiers au format texte brut. Vous pouvez d’ailleurs essayer d’ouvrir dauphin.csv depuis RStudio, en allant dans la fenêtre Files puis en cliquant sur le nom du fichier et en choisissant View File. RStudio ouvre un nouvel onglet à côté de votre script vous permettant d’inspecter le contenu de ce fichier. Par rapport au fichier Excel, vous pouvez noter un certain nombre de différences :\n\nLes colonnes sont séparées par des tabulations (il aurait pu s’agir de virgules, de points-virgules, d’un caractère spécial ou de simples espaces).\nLes nombres décimaux utilisent la virgule (et non le point comme dans les pays anglo-saxons).\nLes noms de colonnes ont déjà été corrigés/simplifiés par rapport au tableau d’origine.\nLes valeurs manquantes sont toutes codées par des NAs.\n\nUn travail d’édition du fichier .xls de départ a donc été réalisé en amont de l’enregistrement au format .csv.\nAttention, à ce stade, vous avez ouvert un fichier au format texte brut dans RStudio, mais les données contenues dans ce fichier n’ont pas été importées dans R pour autant. Pour les importer, on procède comme pour les fichiers au format tableur (voir Section 4.4.2 ci-dessus).\nOn commence par cliquer sur dauphin.csv dans l’onglet Files de RStudio. On sélectionne ensuite Import Dataset... :\n\n\n\n\n\nFigure 4.7: Importer un fichier .csv depuis l’onglet Files de RStudio.\n\n\n\n\nLa fenêtre qui s’ouvre est en tous points identique à celle obtenue pour l’importation de fichiers tableurs (Figure 4.8).\n\n\n\n\n\nFigure 4.8: Importer un fichier .csv depuis l’onglet Files de RStudio.\n\n\n\n\nNous voyons ici que par défaut, RStudio considère qu’une unique colonne est présente. En effet, les fichiers .csv utilisent généralement la virgule pour séparer les colonnes. Ce n’est pas le cas ici. Il nous faut donc sélectionner, dans le champ Delimiter, l’option Tab (tabulation) et non Comma (virgule).\nÀ ce stade, chaque variable est maintenant reconnue comme telle, chaque variable occupe donc une colonne distincte. Mais les colonnes Cd, Cu et Hg ne contiennent pas les bonnes valeurs (vous pouvez le vérifier en consultant l’onglet dauphin.csv que vous avez ouvert un peu plus tôt à côté de votre script). La cause est simple : R s’attend à ce que les nombres décimaux utilisent le point en guise de symbole des décimales. Or, notre fichier .csv utilise la virgule. C’est une convention qui dépend du pays dans lequel vous vous trouvez, et de la langue de votre système d’exploitation (en langage technique, on parle de Locale). Le fichier dauphin.csv ayant été créé sur un ordinateur français, la virgule a été utilisée en guise de symbole des décimales. Pour l’indiquer à R, cliquez sur Locale &gt; Configure..., changez le . en , dans le champ Decimal Mark et validez en cliquant sur Configure.\n\n\n\n\n\nFigure 4.9: Changement du symbole utilisé pour les décimales.\n\n\n\n\nLes données sont maintenant au bon format, prêtes à être importées dans RStudio. Afin de ne pas écraser l’objet dauphin que nous avons créé à partir du fichier tableur un peu plus tôt, nous stockerons ces nouvelles données dans un objet nommé dauphin2. Pour cela, ajoutez un 2 au nom dauphin dans le champ Name en bas à gauche :\n\n\n\n\n\nFigure 4.10: Les données, dans un format correct permettant l’importation.\n\n\n\n\nNous n’avons plus qu’à copier-coller dans notre script le code généré automatiquement en bas à droite de la fenêtre. Comme précédemment, la ligne library(readr) est inutile : ce fait package fait partie du tidyverse et nous l’avons donc déjà chargé en début de chapitre.\n\ndauphin2 &lt;- read_delim(\"data/dauphin.csv\", \n                       \"\\t\", escape_double = FALSE, \n                       locale = locale(decimal_mark = \",\"), \n                       trim_ws = TRUE)\n\nRows: 93 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (3): Sexe, Statut, Organe\ndbl (6): Id, Taille, Age, Cd, Cu, Hg\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNotez que :\n\nC’est le package readr et non plus readxl qui est utilisé.\nLa fonction read_delim() a remplacé la fonction read_excel(). Il existe beaucoup d’autres fonctions selon le format de vos données (par exemple read_csv() et read_csv2()). Il est inutile de toutes les connaître dans la mesure où généralement, RStudio vous propose automatiquement la plus appropriée.\nR indique de quelle façon les colonnes ont été “parsées”, autrement dit, R indique quelles fonctions ont été utilisées pour reconnaître le type des données présentes dans chaque colonne.\n\nToutes les fonctions permettant d’importer des données n’ont pas nécessairement le même comportement. Ainsi, si l’on compare les objets importés depuis le fichier tableur (dauphin) et depuis le fichier texte brut (dauphin2), le type de certaines variables peut être différent :\n\ndauphin\n\n# A tibble: 93 × 9\n   ID        Sexe  Statut Taille   Age     Cd    Cu    Hg Organe\n   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n 1 Numéro 1  f     imm       315     3  29.6   3.24 NA    rein  \n 2 Numéro 2  f     imm       357     4  55.1   4.42 NA    rein  \n 3 Numéro 3  f     pnl       439    34 129.    5.01  9.02 rein  \n 4 Numéro 4  f     imm       316     4  71.2   4.33 NA    rein  \n 5 Numéro 5  f     l         435    26 192     5.15 NA    rein  \n 6 Numéro 6  f     pnl       388     6  NA     4.12  4.53 rein  \n 7 Numéro 7  f     mat       410    NA  76     5.1  33.9  foie  \n 8 Numéro 8  m     imm       355    NA  74.4   4.72 13.3  foie  \n 9 Numéro 9  m     imm       222    NA   0.09  9.5   2.89 foie  \n10 Numéro 10 m     imm       412     9  85.6   5.42 NA    rein  \n# ℹ 83 more rows\n\ndauphin2\n\n# A tibble: 93 × 9\n      Id Sexe  Statut Taille   Age     Cd    Cu    Hg Organe\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n 1     1 f     imm       315     3  29.6   3.24 NA    rein  \n 2     2 f     imm       357     4  55.1   4.42 NA    rein  \n 3     3 f     pnl       439    34 129.    5.01  9.02 rein  \n 4     4 f     imm       316     4  71.2   4.33 NA    rein  \n 5     5 f     l         435    26 192     5.15 NA    rein  \n 6     6 f     pnl       388     6  NA     4.12  4.53 rein  \n 7     7 f     mat       410    NA  76     5.1  33.9  foie  \n 8     8 m     imm       355    NA  74.4   4.72 13.3  foie  \n 9     9 m     imm       222    NA   0.09  9.5   2.89 foie  \n10    10 m     imm       412     9  85.6   5.42 NA    rein  \n# ℹ 83 more rows\n\n\nEn particulier selon la version des packages que vous utilisez et les réglages spécifiques de vos systèmes d’exploitation, les variables Taille et Age sont parfois considérées comme réelles dans dauphin (&lt;dbl&gt;) mais comme entières dans dauphin2 (&lt;int&gt;, ce n’est pas le cas ici). Afin d’éviter les confusions dans la suite du document, nous allons supprimer dauphin2 en tapant :\n\nrm(dauphin2)\n\nTaper dauphin2 dans la console devrait maintenant produire une erreur :\n\ndauphin2\n\nError in eval(expr, envir, enclos): objet 'dauphin2' introuvable\n\n\n\n\n4.4.4 En cas de problème…\nIl arrive parfois que l’importation de fichiers textes bruts par la méthode décrite ci-dessus échoue en raison d’un bug du package readr qui gère mal la présence de caractères spéciaux (accents, cédilles, etc) dans le chemin des fichiers que l’on tente d’importer. À l’heure où j’écris ces lignes, ce bug déjà ancien est toujours présent sur certains systèmes. Il est donc utile de connaître une méthode alternative pour importer de tels fichiers dans R. Cette méthode repose sur “la mère de toutes les fonctions d’importation” : read.table().\nLa fonction read.table() est à la base de la plupart des fonctions d’importation décrites dans ce chapitre. Il est donc important d’en connaître la syntaxe et les arguments les plus courants. Cette fonction requiert en général les arguments suivants :\n\nLe chemin du fichier texte contenant les données à importer. Si le fichier se trouve dans votre répertoire de travail, il suffit de donner son nom. S’il est dans un sous-dossier de votre répertoire de travail, il faut donner le nom complet : \"sous_dossier/nom_du_fichier.csv\".\nsep : la spécification du symbole utilisé en guise de séparateur de colonnes dans le fichier texte. Cela peut-être la virgule (sep = \",\"), le point virgule (sep = \";\") ou encore la tabulation (sep = \"\\t\") selon les fichiers importés.\ndec : la spécification du symbole utilisé en guise de symbole pour les décimales. Il n’est pas nécessaire de spécifier cet argument lorsque le symbole dans le fichier source est le point. Mais si c’est une virgule (comme c’est souvent le cas dans les pays francophones), il faut alors préciser dec = \",\".\nheader : la première ligne du fichier source contient-elle des noms de variables. Si oui, il faut indiquer header = TRUE.\n\nAinsi, par exemple, pour le fichier dauphin.csv que j’ai placé dans un sous-dossier de mon répertoire de travail nommé data, on peut taper ceci :\n\ndauph &lt;- read.table(\"data/dauphin.csv\",\n                    sep = \"\\t\",\n                    dec = \",\",\n                    header = TRUE)\ndauph\n\n   Id Sexe Statut Taille Age     Cd    Cu     Hg Organe\n1   1    f    imm    315   3  29.60  3.24     NA   rein\n2   2    f    imm    357   4  55.10  4.42     NA   rein\n3   3    f    pnl    439  34 129.30  5.01   9.02   rein\n4   4    f    imm    316   4  71.20  4.33     NA   rein\n5   5    f      l    435  26 192.00  5.15     NA   rein\n6   6    f    pnl    388   6     NA  4.12   4.53   rein\n7   7    f    mat    410  NA  76.00  5.10  33.90   foie\n8   8    m    imm    355  NA  74.40  4.72  13.30   foie\n9   9    m    imm    222  NA   0.09  9.50   2.89   foie\n10 10    m    imm    412   9  85.60  5.42     NA   rein\n11 11    m    imm    310   4  39.80  3.62     NA   rein\n12 12    f    pnl    452  28 193.90  6.34   8.70   rein\n13 13    m    imm    299   3  25.80  5.10   7.40   foie\n14 14    f     pl    432  14  77.30  8.75 128.80   foie\n15 15    m    imm    392   4  36.70  5.10  13.70   foie\n16 16    f    pnl    445  22  88.40  8.40 141.90   foie\n17 17    f    imm    348  NA  49.80  6.30  32.40   foie\n18 18    m    imm    210  NA   0.05  5.50   3.09   foie\n19 19    f     pl    430  NA  57.90  7.00  14.10   foie\n20 20    m    imm    264  NA   4.40  3.40   1.25   foie\n21 21    f    pnl    433  23 155.90  8.51     NA   rein\n22 22    f    imm    447  27  43.40  3.56     NA   rein\n23 23    m    mat    548  21  57.90  3.21     NA   rein\n24 24    m    imm    308   2  29.70  3.93     NA   rein\n25 25    f    pnl    435  21  55.40  4.35     NA   rein\n26 26    f      l    465  14 146.80  5.78     NA   rein\n27 27    m    imm    334   1   1.55  3.35     NA   rein\n28 28    f      l    434  22  55.00  3.53     NA   rein\n29 29    f    pnl    387   6  90.10  4.17     NA   rein\n30 30    f  repos    444  40 107.30  5.01     NA   rein\n31 31    m    mat    581  18 164.30  5.69     NA   rein\n32 32    f    imm    359  11     NA  3.97     NA   rein\n33 33    f    imm    245  NA   0.07  5.80   1.30   foie\n34 34    m    imm    346   4  34.40  2.65  21.60   foie\n35 35    f    imm    370  NA  36.40  3.80  15.70   foie\n36 36    f      l    432  27  80.10  3.96     NA   rein\n37 37    m    imm    279   2   7.84  3.63     NA   rein\n38 38    f    imm    316   3  34.20  3.21     NA   rein\n39 39    m    imm    315   2  16.50  3.35     NA   rein\n40 40    f    pnl    363   8  56.20  4.00     NA   rein\n41 41    f    pnl    457  14 123.30  5.86   4.23   rein\n42 42    m    imm    472   9 109.60  4.50     NA   rein\n43 43    f     pl    442  16 193.10  5.25   6.38   rein\n44 44    m    imm    422  10  75.10  6.90  53.15   foie\n45 45    f    imm    193   1     NA  5.70     NA   rein\n46 46    m    imm    324   4  31.60  4.50   7.90   foie\n47 47    f    pnl    478  36  82.20  6.70 243.60   foie\n48 48    f    pnl    451  NA 584.70  5.26     NA   rein\n49 49    m    imm    245  NA   0.07  6.00   5.70   foie\n50 50    f    pnl    405  NA  70.00  6.90  28.70   foie\n51 51    m    imm    433  NA  55.20  7.50  52.50   foie\n52 52    m    imm    326  NA  19.90  3.70  11.00   foie\n53 53    f    pnl    440  22  84.00  9.10 207.40   foie\n54 54    f    pnl    431  NA  90.30  4.64     NA   rein\n55 55    f     pl    428  12 108.30  4.47   3.93   rein\n56 56    f    imm    308  NA  50.80  5.00  27.00   foie\n57 57    f    pnl    422  14  70.40  7.30  62.90   foie\n58 58    f     pl    426  12  94.40  8.00  85.00   foie\n59 59    f    imm    216  NA   0.01  4.20   3.53   foie\n60 60    f    imm    314  NA  37.10  4.40  15.20   foie\n61 61    f    pnl    394   9  78.50  6.20  55.20   foie\n62 62    m    imm    400   5  89.10  4.83     NA   rein\n63 63    f    pnl    416  NA  67.80  7.50  95.90   foie\n64 64    f    imm    275   2   2.30  3.00   1.20   foie\n65 65    m    imm    382  10  89.10  4.62     NA   rein\n66 66    f    imm    320   6     NA  4.65     NA   rein\n67 67    f     pl    418  12  89.10  4.26   5.44   rein\n68 68    f    pnl    423  12  71.90  7.20  72.50   foie\n69 69    f      l    407  11  64.40  5.10  39.00   foie\n70 70    f     pl    459  22  76.20 10.70 178.20   foie\n71 71    m    imm    215  NA   0.04  4.40   2.74   foie\n72 72    m    imm    354   5  31.80  5.40 172.10   foie\n73 73    m    imm    237  NA   0.07  7.10   1.29   foie\n74 74    m    mat    513  18  44.30  5.80  49.80   foie\n75 75    f     pl    431  15  80.80  9.40 145.20   foie\n76 76    m    mat    519  15  71.30  4.41     NA   rein\n77 77    f    pnl    386   8 918.80  7.38     NA   rein\n78 78    f    imm    378   5 129.20  6.38     NA   rein\n79 79    f    imm    342   4  89.40  4.68     NA   rein\n80 80    m    mat    582  21 127.10  5.76     NA   rein\n81 81    f      l    453  24  59.40  7.90 141.90   foie\n82 82    f    imm    242   1   7.20  4.30   3.48   foie\n83 83    f    imm    292   3  33.20  3.60     NA   rein\n84 84    m    mat    545  25  89.30  2.67     NA   rein\n85 85    f    imm    185  NA   0.01  5.50   2.58   foie\n86 86    m    imm    340  NA  42.00  4.20   9.90   foie\n87 87    m    imm    232   5   0.08  6.40   1.10   foie\n88 88    m    mat    560  24  35.70  5.70  69.70   foie\n89 89    m    imm    308  NA  38.20  4.40  11.50   foie\n90 90    m    imm    227  NA   3.40  3.70   3.08   foie\n91 91    f     pl    460  19  98.90  3.63     NA   rein\n92 92    f      l    436  35  94.00  4.01     NA   rein\n93 93    m    mat    582  27  82.20  3.39     NA   rein\n\n\nPuisque la fonction read.table() importe les données sous la forme d’un data.frame, il est nécessaire de transformer le tableau obtenu en tibble grâce à la fonction as_tibble() afin de bénéficier de tous les avantages de ce format d’objet.\n\ndauph &lt;- as_tibble(dauph)\ndauph\n\n# A tibble: 93 × 9\n      Id Sexe  Statut Taille   Age     Cd    Cu    Hg Organe\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n 1     1 f     imm       315     3  29.6   3.24 NA    rein  \n 2     2 f     imm       357     4  55.1   4.42 NA    rein  \n 3     3 f     pnl       439    34 129.    5.01  9.02 rein  \n 4     4 f     imm       316     4  71.2   4.33 NA    rein  \n 5     5 f     l         435    26 192     5.15 NA    rein  \n 6     6 f     pnl       388     6  NA     4.12  4.53 rein  \n 7     7 f     mat       410    NA  76     5.1  33.9  foie  \n 8     8 m     imm       355    NA  74.4   4.72 13.3  foie  \n 9     9 m     imm       222    NA   0.09  9.5   2.89 foie  \n10    10 m     imm       412     9  85.6   5.42 NA    rein  \n# ℹ 83 more rows\n\n\n\n\n4.4.5 Exercices\n\nL’objet dauphin est-il “tidy” (autrement dit, s’agit-il de “données rangées”) ? Justifiez.\nProduisez le graphique ci-dessous :\n\n\n\n\n\n\nFigure 4.11: Figure à reproduire.\n\n\n\n\nIndice : les droites de régression avec les intervalles de confiance sont ajoutés grâce à la fonction geom_smooth(method = \"lm\").\n\nImportez dans R le jeu de données whoTB.csv. Ce jeu de données contient les cas de tuberculose (TB) rapportés par l’Organisation Mondiale de la Santé (OMS, ou WHO en anglais : World Health Organization). Les cas sont répertoriés par année, pays, âge, sexe, type de tuberculose et méthode de diagnostique. Selon vous, ce jeu de données est-il “rangé” ? Pourquoi ?\nSi ce jeu de données n’est pas rangé, rangez-le en utilisant les fonctions du packages tidyr que nous avons découvertes dans ce chapitre : pivot_longer(), pivot_wider(), separate() et unite(). Vous n’aurez pas nécessairement besoin d’utiliser ces 4 fonctions, et à l’inverse, certaines devront peut-être être utilisées plusieurs fois.\n\nPour vous aider, l’OMS donne la signification des codes utilisés en guise de noms pour la plupart des colonnes. Ainsi :\n\nnew indique des nouveaux cas, old des anciens (ici, seuls des nouveaux cas sont rapportés).\nLe type de cas est précisé ensuite :\n\nsp signifie “Smear Positive” (tuberculose pulmonaire à frottis positif).\nsn signifie “Smear Negative” (tuberculose pulmonaire à frottis négatif).\nrel signifie “relapse” (rechute).\nep signifie “Extra Pulmonary” (tuberculose extra-pulmonaire).\n\nLe sexe est codé par m (male) ou f (female).\nEnfin, les chiffres correspondent à des tranches d’âges : 014 signifie “de 0 à 14 ans”, “1524” signifie “de 15 à 24 ans”, etc.\n\nDans ces colonnes aux noms composés, les nombres de cas de tuberculose sont rapportés."
  },
  {
    "objectID": "05-DataWrangling.html#pré-requis",
    "href": "05-DataWrangling.html#pré-requis",
    "title": "5  Manipuler des tableaux avec dplyr",
    "section": "5.1 Pré-requis",
    "text": "5.1 Pré-requis\nNous abordons ici une étape essentielle de toute analyse de données : la manipulation de tableaux, la sélection de lignes, de colonnes, la création de nouvelles variables, etc. Bien souvent, les données brutes que nous importons dans R ne sont pas utiles en l’état. Il nous faut parfois sélectionner seulement certaines lignes pour travailler sur une petite partie du jeu de données. Il nous faut parfois modifier des variables existantes (pour modifier les unités par exemple) ou en créer de nouvelles à partir des variables existantes. Nous avons aussi très souvent besoin de constituer des groupes et d’obtenir des statistiques descriptives pour chaque groupe (moyenne, écart-type, erreur type, etc). Nous verrons dans ce chapitre comment faire tout cela grâce au package dplyr qui fournit un cadre cohérent et des fonctions simples permettant d’effectuer tous les tripatouillages de données dont nous pourrons avoir besoin.\nDans ce chapitre, nous aurons besoin des packages suivants :\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(nycflights13)"
  },
  {
    "objectID": "05-DataWrangling.html#le-pipe",
    "href": "05-DataWrangling.html#le-pipe",
    "title": "5  Manipuler des tableaux avec dplyr",
    "section": "5.2 Le pipe |>",
    "text": "5.2 Le pipe |&gt;\nAvant d’entrer dans le vif du sujet, je souhaite introduire ici la notion de “pipe” (prononcer à l’anglo-saxonne). Le pipe est un opérateur que nous avons déjà vu apparaître à plusieurs reprises dans les chapitres précédents sans expliquer son fonctionnement.\n\n\n\n\n\n\nFigure 5.1: Pour que ces raccourcis fonctionnent, assurez-vous que l’option Use native pipe operator est bien cochée dans les préférences de RStudio.\n\n\n\nLe pipe, noté |&gt;, peut être obtenu en pressant les touches ctrl + shift + M de votre clavier (ou command + shift + M sous macOS). Il peut aussi être noté de la façon suivante : |&gt;. Historiquement, c’est d’ailleurs ce symbole qui était systématiquement utilisé et qui apparaissait en pressant les raccourcis claviers décrits plus haut (voir Figure 5.1 ci-contre). Ce pipe est apparu dans le package magrittr qui fait partie du tidyverse. Cet opérateur s’est avéré tellement utile et a permis de rendre les scripts tellement plus faciles à lire, que depuis la version 4.1.0 de R, un pipe “natif” (|&gt;, disponible par défaut, sans avoir besoin de charger le moindre package) a été rendu disponible.\nIl permet d’enchaîner logiquement des actions les unes à la suite des autres. Globalement, le pipe prend l’objet situé à sa gauche, et le transmet à la fonction situé à sa droite. En d’autres termes, les 2 expressions suivantes sont strictement équivalentes :\n\n# Ici, \"f\" est une fonction quelconque, \n# \"x\" et \"y\" sont 2 objets dont la fonction a besoin.\n\n# Il s'agit d'un exemple fictif : ne tapez pas ceci dans votre script !\nf(x, y)\nx |&gt; f(y)\n\nTravailler avec le pipe est très intéressant car toutes les fonctions de dplyr que nous allons décrire ensuite sont construites autour de la même syntaxe : on leur fournit un data.frame (ou encore mieux, un tibble), elles effectuent une opération et renvoient un nouveau data.frame (ou un nouveau tibble). Il est ainsi possible de créer des groupes de commandes cohérentes qui permettent, grâce à l’enchaînement d’étapes simples, d’aboutir à des résultats complexes.\nDe la même façon que le + permet d’ajouter une couche supplémentaire à un graphique ggplot2, le pipe |&gt; permet d’ajouter une opération supplémentaire dans un groupe de commandes.\nPour reprendre un exemple de la Section 3.8.1 sur les diagrammes bâtons empilés, nous avions utilisé ce code :\n\npenguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  ggplot(aes(x = fct_infreq(species), fill = sex)) +\n  geom_bar(alpha = 0.6, color = \"black\", position = \"fill\")\n\n\n\n\nLigne par ligne, voilà la signification de ce code :\n\n“Prend le tableau penguins, puis…”\n“transmets-le à la fonction filter() pour éliminer les lignes pour lequel le sexe est inconnu, puis…”\n“transmets le résultat à la fonction ggplot() pour en faire un graphique”\n\nOn aurait pu faire la même chose ainsi :\n\npenguins_clean &lt;- filter(penguins, !is.na(sex))\nggplot(penguins_clean, aes(x = fct_infreq(species), fill = sex)) +\n    geom_bar(alpha = 0.6, color = \"black\", position = \"fill\")\n\n\n\n\nC’est strictement équivalent. La deuxième méthode à l’inconvénient de nous obliger à créer un objet intermédiaire (que j’ai ici nommé penguins_clean). Lorsque l’on a de nombreuses fonctions à enchaîner, il faut donc créer de nombreux objets intermédiaires dont nous n’avons besoin qu’une seule fois, ce qui peut être source de nombreuses erreurs.\nUne troisième façon de procéder est la suivante :\n\nggplot(filter(penguins, !is.na(sex)), \n       aes(x = fct_infreq(species), fill = sex)) +\n    geom_bar(alpha = 0.6, color = \"black\", position = \"fill\")\n\n\n\n\nCette fois, on ne crée plus d’objet intermédiaire, mais on intègre directement la fonction filter() à l’intérieur de la fonction ggplot(). Le code devient un peu moins lisible, et quand ça n’est pas deux fonctions mais 4, 5 ou plus que nous devons enchaîner, procéder ainsi est la garantie que des erreurs seront commises et qu’elles seront très difficiles à corriger.\nOn préfère donc toujours utiliser le pipe qui a le mérite de placer chaque fonction sur une nouvelle ligne, et de permettre une lecture plus simple du code, ligne par ligne, étape par étape, et non de façon imbriquée, de l’intérieur d’une commande vers l’extérieur :\n\npenguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  ggplot(aes(x = fct_infreq(species), fill = sex)) +\n  geom_bar(alpha = 0.6, color = \"black\", position = \"fill\")\n\nNotez bien qu’avec le pipe, le premier argument des fonctions filter() et ggplot() a disparu : le pipe a fourni automatiquement à filter() les données du tableau penguins. Il a ensuite fourni automatiquement à ggplot() les données modifiées par la fonction filter().\nComme pour le + de ggplot2, il est conseillé de placer un seul pipe par ligne, toujours à la fin, et de revenir à la ligne pour préciser l’étape suivante.\nToutes les commandes que nous utiliserons à partir de maintenant reposeront sur le pipe puisqu’il permet de rendre le code plus lisible."
  },
  {
    "objectID": "05-DataWrangling.html#les-verbes-du-tripatouillage-de-données",
    "href": "05-DataWrangling.html#les-verbes-du-tripatouillage-de-données",
    "title": "5  Manipuler des tableaux avec dplyr",
    "section": "5.3 Les verbes du tripatouillage de données",
    "text": "5.3 Les verbes du tripatouillage de données\nNous allons ici nous concentrer sur les fonctions les plus couramment utilisées pour manipuler et résumer des données. Nous aborderons ici une dizaine des principaux verbes de la manipulation des données, chacun correspondant à une fonction précise de dplyr. Chaque section de ce chapitre sera consacrée à la présentation d’un exemple utilisant un ou plusieurs de ces verbes.\nLes 6 verbes sont :\n\nfilter() : choisir des lignes dans un tableau à partir de conditions spécifiques (filtrer).\nselect() : sélectionner des colonnes d’un tableau.\nmutate() : créer de nouvelles variables en transformant et combinant des variables existantes (muter).\narrange() : trier les lignes d’un tableau selon un ou plusieurs critères (arranger).\nsummarise() et reframe() : calculer des résumés statistiques des données (résumer). Souvent utilisé en combinaison avec group_by() (grouper par), qui permet de constituer des groupes au sein des données.\nleft_join() et inner_join() : associer, fusionner 2 data.frames en faisant correspondre les éléments d’une colonne commune entre les 2 tableaux (joindre). Il y a de nombreuses façons de joindre des tableaux, et donc, de nombreuses fonctions de jointure (left_join(), right_join(), inner_join(), full_join(), outer_join(), cross_join(), nest_join()…). Nous nous contenterons d’examiner les fonctions les plus basiques et qui devraient couvrir l’essentiel de vos besoins.\n\nToutes ces fonctions, tous ces verbes, sont utilisés de la même façon : on prend un data.frame, grâce au pipe, on le transmet à l’une de ces fonctions dont on précise les arguments entre parenthèses, la fonction nous renvoie un nouveau tableau modifié. Évidemment, on peut enchaîner les actions pour modifier plusieurs fois le même tableau, c’est tout l’intérêt du pipe.\nEnfin, gardez en tête qu’il existe beaucoup plus de fonctions dans dplyr que la dizaine que nous allons détailler ici. Nous verrons parfois quelques variantes, mais globalement, maîtriser ces fonctions simples devrait vous permettre de conduire une très large gamme de manipulations de données, et ainsi vous faciliter la vie pour la production de graphiques et l’analyse statistique de vos données."
  },
  {
    "objectID": "05-DataWrangling.html#sec-filter",
    "href": "05-DataWrangling.html#sec-filter",
    "title": "5  Manipuler des tableaux avec dplyr",
    "section": "5.4 Filtrer des lignes avec filter()",
    "text": "5.4 Filtrer des lignes avec filter()\n\n5.4.1 Principe\n\n\n\n\n\nFigure 5.2: Schéma de la fonction filter() tiré de la ‘cheatsheet’ de dplyr et tidyr.\n\n\n\n\nComme son nom l’indique, filter() permet de filtrer des lignes en spécifiant un ou des critères de tri portant sur une ou plusieurs variables. Nous pouvons ainsi créer un nouveau tableau ne contenant que les données de l’espèce Adélie :\n\npeng_adelie &lt;- penguins |&gt; \n  filter(species == \"Adelie\")\n\nLa première ligne de code nous permet :\n\nd’indiquer le nom du nouvel objet dans lequel les données modifiées seront stockées (ici, peng_adelie)\nd’indiquer de quel objet les données doivent être extraites (penguins)\nde passer cet objet à la fonction suivante avec un pipe |&gt;\n\nLe premier argument de la fonction filter() doit être le nom d’un data.frame ou d’un tibble. Ici, puisque nous utilisons le pipe, il est inutile de spécifier cet argument : c’est ce qui est placé à gauche du pipe qui est utilisé comme premier argument de la fonction filter(). Les arguments suivants constituent la ou les conditions qui doivent être respectées par les lignes du tableau de départ afin d’être intégrées au nouveau tableau de données.\n\n\n5.4.2 Exercice\nCréez un objet nommé adelie_light qui contiendra uniquement les données de l’espèce Adélie, et uniquement pour les individus pesant 3700 grammes ou moins. Indice : relisez la Section 1.3.4.2\nVérifiez que cet objet contient bien 81 lignes.\n\n\n5.4.3 Les conditions logiques\nDans la Section 1.3.4.2, nous avons présenté en détail le fonctionnement des opérateurs de comparaison dans R. Relisez cette section si vous ne savez plus de quoi il s’agit. Les opérateurs de comparaison permettent de vérifier l’égalité ou l’inégalité entre des éléments. Ils renvoient TRUE ou FALSE et seront particulièrement utiles pour filtrer des lignes dans un tableau. Voici à nouveau la liste des opérateurs de comparaison usuels :\n\n== : égal à\n!= : différent de\n&gt; : supérieur à\n&lt; : inférieur à\n&gt;= : supérieur ou égal à\n&lt;= : inférieur ou égal à\n\nÀ cette liste, nous pouvons ajouter quelques éléments utiles :\n\nis.na() : renvoie TRUE en cas de données manquantes.\n! : permet de tester le contraire d’une expression logique. Par exemple !is.na() renvoie TRUE s’il n’y a pas de données manquantes.\n%in% : permet de tester si l’élément de gauche est contenu dans la série d’éléments fournie à droite. Par exemple 2 %in% 1:5 renvoie TRUE, mais 2 %in% 5:10 renvoie FALSE.\n| : opérateur logique OU. Permet de tester qu’une condition OU une autre est remplie.\n& : opérateur logique ET. Permet de tester qu’une condition ET une autre sont remplies.\n\nVoyons comment utiliser ces opérateurs avec la fonction filter().\nDans le tableau penguins, quels sont les individus pour lesquels la masse n’a pas été mesurée ? Une bonne façon de le savoir est de regarder si, pour la variable body_mass_g, des données manquantes sont présentes :\n\npenguins |&gt; \n  filter(is.na(body_mass_g))\n\n# A tibble: 2 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen             NA            NA                NA          NA\n2 Gentoo  Biscoe                NA            NA                NA          NA\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nSeules les lignes contenant NA dans la colonne body_mass_g sont retenues. Il y a donc 2 individus dont la masse est inconnue. D’ailleurs, pour ces individu, aucune mesure biométrique n’est disponible. il s’agit d’un manchot Adélie, et d’un manchot Gentoo, tous les deux de sexe inconnu.\nDans le même ordre d’idée, y a t-il des individus dont on ne connait pas le sexe mais dont on connait les mesures biométriques (au moins la masse) ? Là encore, une façon d’obtenir cette information est de sélectionner les individus dont le sexe est manquant, mais pour lesquels la masse n’est pas manquante :\n\npenguins |&gt; \n  filter(is.na(sex),\n         !is.na(body_mass_g))\n\n# A tibble: 9 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           34.1          18.1               193        3475\n2 Adelie  Torgersen           42            20.2               190        4250\n3 Adelie  Torgersen           37.8          17.1               186        3300\n4 Adelie  Torgersen           37.8          17.3               180        3700\n5 Adelie  Dream               37.5          18.9               179        2975\n6 Gentoo  Biscoe              44.5          14.3               216        4100\n7 Gentoo  Biscoe              46.2          14.4               214        4650\n8 Gentoo  Biscoe              47.3          13.8               216        4725\n9 Gentoo  Biscoe              44.5          15.7               217        4875\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nNotez l’utilisation du ! pour la seconde condition. Nous récupérons ici les lignes pour lesquelles body_mass_g n’est pas NA et pour lesquelles sex est NA. Seules les lignes qui respectent cette double condition sont retenues. Cette syntaxe est équivalente à :\n\npenguins |&gt; \n  filter(is.na(sex) & !is.na(body_mass_g))\n\n# A tibble: 9 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           34.1          18.1               193        3475\n2 Adelie  Torgersen           42            20.2               190        4250\n3 Adelie  Torgersen           37.8          17.1               186        3300\n4 Adelie  Torgersen           37.8          17.3               180        3700\n5 Adelie  Dream               37.5          18.9               179        2975\n6 Gentoo  Biscoe              44.5          14.3               216        4100\n7 Gentoo  Biscoe              46.2          14.4               214        4650\n8 Gentoo  Biscoe              47.3          13.8               216        4725\n9 Gentoo  Biscoe              44.5          15.7               217        4875\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nDans la fonction filter(), séparer plusieurs conditions par des virgules signifie que seules les lignes qui remplissent toutes les conditions seront retenues. C’est donc l’équivalent du ET logique.\n\nEnfin, pour illustrer l’utilisation de | (le OU logique) et de %in%, imaginons que nous souhaitions extraire les informations des individus de l’espèce Adélie qui vivent soit sur l’île Biscoe, soit sur l’île Dream, et dont le bec mesure moins de 42 mm de longueur :\n\nadel_small &lt;- penguins |&gt; \n  filter(species == \"Adelie\", \n         island == \"Biscoe\" | island == \"Dream\", \n         bill_length_mm &lt; 42)\nadel_small\n\n# A tibble: 91 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Biscoe           37.8          18.3               174        3400\n 2 Adelie  Biscoe           37.7          18.7               180        3600\n 3 Adelie  Biscoe           35.9          19.2               189        3800\n 4 Adelie  Biscoe           38.2          18.1               185        3950\n 5 Adelie  Biscoe           38.8          17.2               180        3800\n 6 Adelie  Biscoe           35.3          18.9               187        3800\n 7 Adelie  Biscoe           40.6          18.6               183        3550\n 8 Adelie  Biscoe           40.5          17.9               187        3200\n 9 Adelie  Biscoe           37.9          18.6               172        3150\n10 Adelie  Biscoe           40.5          18.9               180        3950\n# ℹ 81 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nExaminez ce tableau avec View() pour vérifier que la variable island contient bien uniquement les valeurs Biscoe et Dream correspondant aux 2 îles qui nous intéressent. Nous avons extrait ici les individus des îles Biscoe et Dream, pourtant, il nous a fallu utiliser le OU logique. Car chaque individu n’est issu que d’une unique île, or nous souhaitons récupérer toutes les lignes pour lesquelles l’île est soit Biscoe, soit Dream (l’une ou l’autre). Pour chaque ligne, les deux conditions ne peuvent pas être vraies l’une et l’autre en même temps. En revanche, on retient chaque ligne qui remplit la première condition ou la seconde.\nUne autre solution pour obtenir le même tableau est de remplacer l’expression contenant | par une expression contenant %in% :\n\nadel_small2 &lt;- penguins |&gt; \n  filter(species == \"Adelie\", \n         island %in% c(\"Biscoe\", \"Dream\"), \n         bill_length_mm &lt; 42)\nadel_small2\n\n# A tibble: 91 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Biscoe           37.8          18.3               174        3400\n 2 Adelie  Biscoe           37.7          18.7               180        3600\n 3 Adelie  Biscoe           35.9          19.2               189        3800\n 4 Adelie  Biscoe           38.2          18.1               185        3950\n 5 Adelie  Biscoe           38.8          17.2               180        3800\n 6 Adelie  Biscoe           35.3          18.9               187        3800\n 7 Adelie  Biscoe           40.6          18.6               183        3550\n 8 Adelie  Biscoe           40.5          17.9               187        3200\n 9 Adelie  Biscoe           37.9          18.6               172        3150\n10 Adelie  Biscoe           40.5          18.9               180        3950\n# ℹ 81 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nIci, toutes les lignes du tableau dont la variable island est égale à un élément du vecteur c(\"Biscoe\", \"Dream\") sont retenues. L’utilisation du OU logique peut être source d’erreur. Je préfère donc utiliser %in% qui me semble plus parlant. La fonction identical() nous confirme que les deux façons de faire produisent exactement le même résultat. Libre à vous de privilégier la méthode qui vous convient le mieux :\n\nidentical(adel_small, adel_small2)\n\n[1] TRUE"
  },
  {
    "objectID": "05-DataWrangling.html#sélectionner-des-variables-avec-select",
    "href": "05-DataWrangling.html#sélectionner-des-variables-avec-select",
    "title": "5  Manipuler des tableaux avec dplyr",
    "section": "5.5 Sélectionner des variables avec select()",
    "text": "5.5 Sélectionner des variables avec select()\n\n\n\n\n\nFigure 5.3: Schéma de la fonction select() tiré de la ‘cheatsheet’ de dplyr et tidyr.\n\n\n\n\nIl n’est pas rare de travailler avec des tableaux contenant des centaines, voir des milliers de colonnes. Dans de tels cas, il peut être utile de réduire le jeu de données aux variables qui vous intéressent. Le rôle de la fonction select() est de retenir uniquement les colonnes dont on a spécifié le nom, afin de recentrer l’analyse sur les variables utiles.\nselect() n’est pas particulièrement utile pour le jeu de données penguins puisqu’il ne contient que 8 variables. Toutefois, on peut malgré tout ces données pour comprendre le fonctionnement général de select(). Ainsi, pour sélectionner uniquement les colonnes species, sex et body_mass_g, on tape :\n\n# Sélection de variables par leur nom\npenguins |&gt;\n  select(species, sex, body_mass_g)\n\n# A tibble: 344 × 3\n   species sex    body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;        &lt;int&gt;\n 1 Adelie  male          3750\n 2 Adelie  female        3800\n 3 Adelie  female        3250\n 4 Adelie  &lt;NA&gt;            NA\n 5 Adelie  female        3450\n 6 Adelie  male          3650\n 7 Adelie  female        3625\n 8 Adelie  male          4675\n 9 Adelie  &lt;NA&gt;          3475\n10 Adelie  &lt;NA&gt;          4250\n# ℹ 334 more rows\n\n\nPour retenir des colonnes qui sont côte à côte dans le tableau de départ, on peut utiliser l’opérateur : pour les sélectionner :\n\n# Sélection de toutes les variables entre `island` et `bill_depth_mm` (inclues)\npenguins |&gt;\n  select(island:bill_depth_mm)\n\n# A tibble: 344 × 3\n   island    bill_length_mm bill_depth_mm\n   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;\n 1 Torgersen           39.1          18.7\n 2 Torgersen           39.5          17.4\n 3 Torgersen           40.3          18  \n 4 Torgersen           NA            NA  \n 5 Torgersen           36.7          19.3\n 6 Torgersen           39.3          20.6\n 7 Torgersen           38.9          17.8\n 8 Torgersen           39.2          19.6\n 9 Torgersen           34.1          18.1\n10 Torgersen           42            20.2\n# ℹ 334 more rows\n\n\nÀ l’inverse, si on veut supprimer certaines colonnes, on peut utiliser la notation - :\n\n# Sélection de toutes les variables de `penguins` à l'exception\n# de celles comprises entre `island` et `bill_depth_mm` (inclues)\npenguins |&gt;\n  select(-(island:bill_depth_mm))\n\n# A tibble: 344 × 5\n   species flipper_length_mm body_mass_g sex     year\n   &lt;fct&gt;               &lt;int&gt;       &lt;int&gt; &lt;fct&gt;  &lt;int&gt;\n 1 Adelie                181        3750 male    2007\n 2 Adelie                186        3800 female  2007\n 3 Adelie                195        3250 female  2007\n 4 Adelie                 NA          NA &lt;NA&gt;    2007\n 5 Adelie                193        3450 female  2007\n 6 Adelie                190        3650 male    2007\n 7 Adelie                181        3625 female  2007\n 8 Adelie                195        4675 male    2007\n 9 Adelie                193        3475 &lt;NA&gt;    2007\n10 Adelie                190        4250 &lt;NA&gt;    2007\n# ℹ 334 more rows\n\n\nIl y a beaucoup de fonctions permettant de sélectionner des variables dont les noms respectent certains critères. Par exemple :\n\nstarts_with(\"abc\") : renvoie toutes les variables dont les noms commencent par “abc”\nends_with(\"xyz\") : renvoie toutes les variables dont les noms se terminent par “xyz”\ncontains(\"ijk\") : renvoie toutes les variables dont les noms contiennent “ijk”\n\nIl en existe beaucoup d’autres. Vous pouvez consulter l’aide de ?select() pour en savoir plus.\nAinsi, il est par exemple possible d’extraire toutes les variables contenant le mot “mm” ainsi :\n\npenguins |&gt;\n  select(contains(\"mm\"))\n\n# A tibble: 344 × 3\n   bill_length_mm bill_depth_mm flipper_length_mm\n            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n 1           39.1          18.7               181\n 2           39.5          17.4               186\n 3           40.3          18                 195\n 4           NA            NA                  NA\n 5           36.7          19.3               193\n 6           39.3          20.6               190\n 7           38.9          17.8               181\n 8           39.2          19.6               195\n 9           34.1          18.1               193\n10           42            20.2               190\n# ℹ 334 more rows\n\n\nÉvidemment, le tableau penguins n’est pas modifié par cette opération : il contient toujours les 8 variables de départ. Pour travailler avec ces tableaux de données contenant moins de variables, il faut les stocker dans un nouvel objet en leur donnant un nom :\n\nmeasures &lt;- penguins |&gt;\n  select(contains(\"mm\"))\n\nEnfin, on peut utiliser select() pour renommer des variables. Mais ce n’est que rarement utile car select() élimine toutes les variables qui n’ont pas été explicitement nommées :\n\npenguins |&gt;\n  select(species:island,\n         b_length = bill_length_mm,\n         flipper = flipper_length_mm)\n\n# A tibble: 344 × 4\n   species island    b_length flipper\n   &lt;fct&gt;   &lt;fct&gt;        &lt;dbl&gt;   &lt;int&gt;\n 1 Adelie  Torgersen     39.1     181\n 2 Adelie  Torgersen     39.5     186\n 3 Adelie  Torgersen     40.3     195\n 4 Adelie  Torgersen     NA        NA\n 5 Adelie  Torgersen     36.7     193\n 6 Adelie  Torgersen     39.3     190\n 7 Adelie  Torgersen     38.9     181\n 8 Adelie  Torgersen     39.2     195\n 9 Adelie  Torgersen     34.1     193\n10 Adelie  Torgersen     42       190\n# ℹ 334 more rows\n\n\nIl est donc généralement préférable d’utiliser rename() pour renommer certaines variables sans en éliminer aucune :\n\npenguins |&gt;\n  rename(b_length = bill_length_mm,\n         flipper = flipper_length_mm)\n\n# A tibble: 344 × 8\n   species island    b_length bill_depth_mm flipper body_mass_g sex     year\n   &lt;fct&gt;   &lt;fct&gt;        &lt;dbl&gt;         &lt;dbl&gt;   &lt;int&gt;       &lt;int&gt; &lt;fct&gt;  &lt;int&gt;\n 1 Adelie  Torgersen     39.1          18.7     181        3750 male    2007\n 2 Adelie  Torgersen     39.5          17.4     186        3800 female  2007\n 3 Adelie  Torgersen     40.3          18       195        3250 female  2007\n 4 Adelie  Torgersen     NA            NA        NA          NA &lt;NA&gt;    2007\n 5 Adelie  Torgersen     36.7          19.3     193        3450 female  2007\n 6 Adelie  Torgersen     39.3          20.6     190        3650 male    2007\n 7 Adelie  Torgersen     38.9          17.8     181        3625 female  2007\n 8 Adelie  Torgersen     39.2          19.6     195        4675 male    2007\n 9 Adelie  Torgersen     34.1          18.1     193        3475 &lt;NA&gt;    2007\n10 Adelie  Torgersen     42            20.2     190        4250 &lt;NA&gt;    2007\n# ℹ 334 more rows"
  },
  {
    "objectID": "05-DataWrangling.html#sec-mutate",
    "href": "05-DataWrangling.html#sec-mutate",
    "title": "5  Manipuler des tableaux avec dplyr",
    "section": "5.6 Créer de nouvelles variables avec mutate()",
    "text": "5.6 Créer de nouvelles variables avec mutate()\n\n5.6.1 Principe\n\n\n\n\n\nFigure 5.4: Schéma de la fonction mutate() tiré de la ‘cheatsheet’ de dplyr et tidyr.\n\n\n\n\nLa fonction mutate() permet de créer de nouvelles variables à partir des variables existantes, ou de modifier des variables déjà présentes dans un jeu de données. Il est en effet fréquent d’avoir besoin de calculer de nouvelles variables, souvent plus informatives que les variables disponibles.\nVoyons un exemple. À partir de penguins, nous allons calculer une nouvelle variable et en modifier une autre :\n\nratio : le rapport entre la longueur du bec et son épaisseur. Cela nous donnera un indice de la compacité du bec. Des valeurs faibles de ce ratio indiqueront un bec très trapu, alors que des valeurs fortes indiqueront un bec très effilé\nmass_kg : la masse, qui est ici exprimée en grammes sera transformée en kilogrammes par une simple division par 1000\n\n\npenguins |&gt;\n  mutate(ratio = bill_length_mm / bill_depth_mm,\n         mass_kg = body_mass_g / 1000)\n\n# A tibble: 344 × 10\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 4 more variables: sex &lt;fct&gt;, year &lt;int&gt;, ratio &lt;dbl&gt;, mass_kg &lt;dbl&gt;\n\n\nSi on ne souhaite conserver que les variables nouvellement créées par mutate() et éliminer toutes les autres, on peut utiliser transmute() :\n\npenguins |&gt;\n  transmute(ratio = bill_length_mm / bill_depth_mm,\n            mass_kg = body_mass_g / 1000)\n\n# A tibble: 344 × 2\n   ratio mass_kg\n   &lt;dbl&gt;   &lt;dbl&gt;\n 1  2.09    3.75\n 2  2.27    3.8 \n 3  2.24    3.25\n 4 NA      NA   \n 5  1.90    3.45\n 6  1.91    3.65\n 7  2.19    3.62\n 8  2       4.68\n 9  1.88    3.48\n10  2.08    4.25\n# ℹ 334 more rows\n\n\nEt comme toujours, pour pouvoir réutiliser ces données, on leur donne un nom :\n\npengu_ratio &lt;-  penguins |&gt;\n  transmute(ratio = bill_length_mm / bill_depth_mm,\n            mass_kg = body_mass_g / 1000)\n\n\n\n5.6.2 Transformer des variables en facteurs\nIl n’est pas rare que les tableaux de données que nous importons contiennent des colonnes numériques ou de chaînes de caractères qui devraient en réalité être reconnues en tant que facteurs. La fonction mutate() nous permet de changer rapidement le type d’une variable afin qu’elle soit reconnue comme un facteur. Plusieurs variables du tableau dauphin, importé plus tôt, devrait être transformées en facteur :\n\ndauphin\n\n# A tibble: 93 × 9\n   ID        Sexe  Statut Taille   Age     Cd    Cu    Hg Organe\n   &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n 1 Numéro 1  f     imm       315     3  29.6   3.24 NA    rein  \n 2 Numéro 2  f     imm       357     4  55.1   4.42 NA    rein  \n 3 Numéro 3  f     pnl       439    34 129.    5.01  9.02 rein  \n 4 Numéro 4  f     imm       316     4  71.2   4.33 NA    rein  \n 5 Numéro 5  f     l         435    26 192     5.15 NA    rein  \n 6 Numéro 6  f     pnl       388     6  NA     4.12  4.53 rein  \n 7 Numéro 7  f     mat       410    NA  76     5.1  33.9  foie  \n 8 Numéro 8  m     imm       355    NA  74.4   4.72 13.3  foie  \n 9 Numéro 9  m     imm       222    NA   0.09  9.5   2.89 foie  \n10 Numéro 10 m     imm       412     9  85.6   5.42 NA    rein  \n# ℹ 83 more rows\n\n\nC’est le cas des variables Sexe, Statut et Organe. Par ailleurs, la variable ID pourrait être supprimée puisqu’elle n’apporte aucune information est est parfaitement redondante avec les numéros de ligne du tableau. Voyons comment réaliser toutes ces actions :\n\ndauphin_clean &lt;- dauphin |&gt; \n  select(-ID) |&gt;                 # Suppression de la colonne ID, puis\n  mutate(Sexe = factor(Sexe),     # Transformation de Sexe en facteur\n         Organe = factor(Organe), # Transformation d'Organe en facteur\n         Statut = factor(Statut,  # Transformation de Statut en facteur\n                         levels = c(\"imm\", \"mat\", \"pnl\", \"pl\", \"l\", \"repos\")))\n\nL’objet dauphin_clean contient les résultats de nos manipulations :\n\ndauphin_clean\n\n# A tibble: 93 × 8\n   Sexe  Statut Taille   Age     Cd    Cu    Hg Organe\n   &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; \n 1 f     imm       315     3  29.6   3.24 NA    rein  \n 2 f     imm       357     4  55.1   4.42 NA    rein  \n 3 f     pnl       439    34 129.    5.01  9.02 rein  \n 4 f     imm       316     4  71.2   4.33 NA    rein  \n 5 f     l         435    26 192     5.15 NA    rein  \n 6 f     pnl       388     6  NA     4.12  4.53 rein  \n 7 f     mat       410    NA  76     5.1  33.9  foie  \n 8 m     imm       355    NA  74.4   4.72 13.3  foie  \n 9 m     imm       222    NA   0.09  9.5   2.89 foie  \n10 m     imm       412     9  85.6   5.42 NA    rein  \n# ℹ 83 more rows\n\n\nVous notez que ID a disparu et que les 3 variables modifiées sont maintenant bel et bien des facteurs. Vous avez probablement remarqué également que pour la variable Statut, la syntaxe que j’ai utilisée est légèrement différente de celle des variables Sexe et Organe. Pour en comprendre la raison, tapez ceci pour afficher le contenu de ces facteurs :\n\ndauphin_clean$Sexe\n\n [1] f f f f f f f m m m m f m f m f f m f m f f m m f f m f f f m f f m f f m f\n[39] m f f m f m f m f f m f m m f f f f f f f f f m f f m f f f f f m m m m f m\n[77] f f f m f f f m f m m m m m f f m\nLevels: f m\n\ndauphin_clean$Organe\n\n [1] rein rein rein rein rein rein foie foie foie rein rein rein foie foie foie\n[16] foie foie foie foie foie rein rein rein rein rein rein rein rein rein rein\n[31] rein rein foie foie foie rein rein rein rein rein rein rein rein foie rein\n[46] foie foie rein foie foie foie foie foie rein rein foie foie foie foie foie\n[61] foie rein foie foie rein rein rein foie foie foie foie foie foie foie foie\n[76] rein rein rein rein rein foie foie rein rein foie foie foie foie foie foie\n[91] rein rein rein\nLevels: foie rein\n\ndauphin_clean$Statut\n\n [1] imm   imm   pnl   imm   l     pnl   mat   imm   imm   imm   imm   pnl  \n[13] imm   pl    imm   pnl   imm   imm   pl    imm   pnl   imm   mat   imm  \n[25] pnl   l     imm   l     pnl   repos mat   imm   imm   imm   imm   l    \n[37] imm   imm   imm   pnl   pnl   imm   pl    imm   imm   imm   pnl   pnl  \n[49] imm   pnl   imm   imm   pnl   pnl   pl    imm   pnl   pl    imm   imm  \n[61] pnl   imm   pnl   imm   imm   imm   pl    pnl   l     pl    imm   imm  \n[73] imm   mat   pl    mat   pnl   imm   imm   mat   l     imm   imm   mat  \n[85] imm   imm   imm   mat   imm   imm   pl    l     mat  \nLevels: imm mat pnl pl l repos\n\n\nPour les 2 premiers facteurs, les niveaux des facteurs (ou modalités) sont classés par ordre alphabétique. Ainsi, pour le facteur Sexe, la catégorie f (femelle) apparaît avant m (mâles) dans la liste des niveaux (Levels: ...). Pour le facteur Organe, la modalité foie apparaît avant la modalité rein. L’ordre des modalités d’un facteur est celui qui sera utilisé par défaut pour ordonner les catégories sur les axes d’un graphique ou dans les légendes. L’ordre alphabétique convient parfaitement pour le Sexe ou l’Organe puisqu’il n’y a pas, pour ces facteurs, d’ordre dans les modalités.\n\nlevels(dauphin_clean$Sexe)\n\n[1] \"f\" \"m\"\n\nlevels(dauphin_clean$Organe)\n\n[1] \"foie\" \"rein\"\n\n\nPour le facteur Statut en revanche, l’ordre importe, car il reflète des stades qui se succèdent logiquement au cours de la vie des individus (et des femelles plus particulièrement). Sur un graphique, on souhaite donc que ces catégories apparaissent dans un ordre bien précis, différent de l’ordre alphabétique. C’est la raison pour laquelle, lorsque l’on crée un facteur avec la fonction factor(), on peut spécifier explicitement un ordre pour les catégories grâce à l’argument levels =. Il suffit ensuite de fournir un vecteur contenant le nom de chaque catégorie, dans l’ordre souhaité.\nIl existe de nombreuses façons de ré-ordonner les modalités d’un facteur le long des axes d’un graphique. Voyons un exemple avec la Figure 5.5 :\n\ndauphin_clean |&gt; \n  ggplot(aes(x = Organe, y = Cu, fill = Sexe)) +\n  geom_boxplot(notch = TRUE) +\n  scale_fill_brewer(palette = \"Accent\") +\n  theme_bw()\n\n\n\n\nFigure 5.5: Un exemple de figure avec 2 facteurs et des modalités ordonnées automatiquement par ordre alphabétique\n\n\n\n\nImaginons que je souhaite faire apparaître les concentrations en cuivre dans les reins à gauche, et les concentrations en cuivre dans le foie à droite, et que je souhaite inverser l’ordre des catégories pour les sexes (les mâles avant les femelles). Une première possibilité consiste à modifier l’ordre des catégories de façon explicite lorsque je crée les facteurs Sexe et Organe grâce à l’argument levels de la fonction factor() :\n\ndauphin_clean |&gt; \n  mutate(Sexe = factor(Sexe, levels = c(\"m\", \"f\")),\n         Organe = factor(Organe, levels = c(\"rein\", \"foie\"))) |&gt; \n  ggplot(aes(x = Organe, y = Cu, fill = Sexe)) +\n  geom_boxplot(notch = TRUE) +\n  scale_fill_brewer(palette = \"Accent\") +\n  theme_bw()\n\n\n\n\nFigure 5.6: La même figure mais avec les modalités des 2 facteurs ordonnées manuellement\n\n\n\n\nRemarquez que l’ordre des catégories a changé sur l’axe des abscisses, mais que les couleurs de remplissage ne sont plus associées aux mêmes sexes non plus.\nUne autre solution est de faire appel au package forcats (c’est un anagramme de factors) qui est automatiquement chargé en mémoire avec le tidyverse. Ce package contient de nombreuses fonctions permettant de manipuler les facteurs, et toutes commencent par fct_. Par exemple, pour inverser l’ordre des catégories d’un facteur (et donc pour arriver au même résultat que précédemment), on peut utiliser fct_rev() :\n\ndauphin_clean |&gt; \n  ggplot(aes(x = fct_rev(Organe), y = Cu, fill = fct_rev(Sexe))) +\n  geom_boxplot(notch = TRUE) +\n  scale_fill_brewer(palette = \"Accent\") +\n  theme_bw()\n\n\n\n\nFigure 5.7: La même figure mais avec les modalités des 2 facteurs inversées avec fct_rev()\n\n\n\n\nIl conviendrait ici de changer les légendes de l’axe des x et de l’échelle de couleurs avec la fonction labs() (voir Section 3.11.1).\nIl existe de nombreuses autres fonctions très utiles dans le package forcats. L’une d’entre elles est la fonction fct_recode(), qui permet de changer le nom des modalités d’un facteur. Par exemple :\n\ndauphin |&gt; \n  mutate(Sexe = fct_recode(Sexe, \n                           \"Femelle\" = \"f\",\n                           \"Mâle\" = \"m\"))\n\n# A tibble: 93 × 9\n   ID        Sexe    Statut Taille   Age     Cd    Cu    Hg Organe\n   &lt;chr&gt;     &lt;fct&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n 1 Numéro 1  Femelle imm       315     3  29.6   3.24 NA    rein  \n 2 Numéro 2  Femelle imm       357     4  55.1   4.42 NA    rein  \n 3 Numéro 3  Femelle pnl       439    34 129.    5.01  9.02 rein  \n 4 Numéro 4  Femelle imm       316     4  71.2   4.33 NA    rein  \n 5 Numéro 5  Femelle l         435    26 192     5.15 NA    rein  \n 6 Numéro 6  Femelle pnl       388     6  NA     4.12  4.53 rein  \n 7 Numéro 7  Femelle mat       410    NA  76     5.1  33.9  foie  \n 8 Numéro 8  Mâle    imm       355    NA  74.4   4.72 13.3  foie  \n 9 Numéro 9  Mâle    imm       222    NA   0.09  9.5   2.89 foie  \n10 Numéro 10 Mâle    imm       412     9  85.6   5.42 NA    rein  \n# ℹ 83 more rows\n\n\nCela permet de transformer la catégorie f en Femelle et la catégorie m en Mâle, et ainsi de rendre plus clair la signification des catégories sur un graphique :\n\ndauphin |&gt; \n  mutate(Sexe = fct_recode(Sexe, \n                           \"Femelle\" = \"f\",\n                           \"Mâle\" = \"m\")) |&gt; \n  ggplot(aes(x = fct_rev(Organe), y = Cu, fill = fct_rev(Sexe))) +\n  geom_boxplot(notch = TRUE) +\n  scale_fill_brewer(palette = \"Accent\") +\n  labs(x = \"Organe\", fill = \"Sexe\", y = \"Concentration en cuivre (µg/g de poids sec)\") +\n  theme_bw()\n\n\n\n\nFigure 5.8: La même figure mais avec les modalités plus claires pour le facteur Sexe\n\n\n\n\nEnfin, il existe une autre façon de procéder lorsque toutes les variables &lt;chr&gt; d’un tableau doivent être transformées en facteur. mutate_if() permet en effet d’appliquer la même fonction à toutes les variables respectant une condition précise. Ici, toutes les colonnes possédant le type &lt;chr&gt; seront transformées en facteur. Nous pouvons donc taper ceci :\n\ndauphin |&gt; \n  mutate_if(is.character, as.factor)\n\n# A tibble: 93 × 9\n   ID        Sexe  Statut Taille   Age     Cd    Cu    Hg Organe\n   &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; \n 1 Numéro 1  f     imm       315     3  29.6   3.24 NA    rein  \n 2 Numéro 2  f     imm       357     4  55.1   4.42 NA    rein  \n 3 Numéro 3  f     pnl       439    34 129.    5.01  9.02 rein  \n 4 Numéro 4  f     imm       316     4  71.2   4.33 NA    rein  \n 5 Numéro 5  f     l         435    26 192     5.15 NA    rein  \n 6 Numéro 6  f     pnl       388     6  NA     4.12  4.53 rein  \n 7 Numéro 7  f     mat       410    NA  76     5.1  33.9  foie  \n 8 Numéro 8  m     imm       355    NA  74.4   4.72 13.3  foie  \n 9 Numéro 9  m     imm       222    NA   0.09  9.5   2.89 foie  \n10 Numéro 10 m     imm       412     9  85.6   5.42 NA    rein  \n# ℹ 83 more rows\n\n\nUn inconvénient de cette fonction est qu’il est impossible de changer manuellement l’ordre des catégories d’un facteur en même temps. On est alors obligé de procéder en deux temps :\n\ndauphin_clean &lt;- dauphin |&gt; \n  select(-ID) |&gt;                            # Suppression de la colonne ID, puis\n  mutate_if(is.character, as.factor) |&gt;     # Transformation en facteur de toutes les variables &lt;chr&gt;, puis\n  mutate(Sexe = fct_recode(Sexe,            # Changement des modalités du facteur Sexe, puis\n                           \"Femelle\" = \"f\",\n                           \"Mâle\" = \"m\"),\n         Organe = fct_rev(Organe),          # Inversion des modalités du facteur Organe\n         Statut = fct_relevel(Statut,       # Ré-agencement de l'ordre des modalités du facteur Statut\n                              \"imm\", \"mat\", \"pnl\", \"l\", \"pl\", \"repos\"))\n\nDans le code ci-dessus, la fonction fct_relevel() joue le même rôle que factor(..., levels = c(...)).\nAu final, toutes les transformations que nous avons fait subir à ce jeu de données n’ont qu’un seul objectif : “ranger” ce jeu de données. Nous avons importé dauphin depuis un fichier externe, puis nous avons supprimé les variables inutiles et modifié celles qui devaient l’être. Toutes ces étapes peuvent être enchaînées grâce au pipe, de la façon suivante :\n\n# Importation et mise en forme du jeu de données `dauphin`\nlibrary(readxl)\ndauphin &lt;- read_excel(\"data/dauphin.xls\", \n                      na = \"*\", skip = 9) |&gt;   # Importer, puis\n  rename(ID = `N°`,                 # Raccourcir les noms, puis\n         Statut = `Statut reproducteur`,\n         Taille = `Taille en cm`,\n         Age = `Age en années`,\n         Cd = `Cd (mg.kg-1)`,\n         Cu = `Cu (mg.kg-1)`,\n         Hg = `Hg (mg.kg-1)`) |&gt; \n  select(-ID) |&gt;                   # Supprimer la variable `ID`, puis\n  mutate_if(is.character,           # 'Factoriser' les variables &lt;chr&gt;, puis\n            as.factor) |&gt;          \n  mutate(Sexe = fct_recode(Sexe,    # Modifier les modalités des facteurs\n                           \"Female\" = \"f\",\n                           \"Male\" = \"m\"),\n         Organe = fct_rev(Organe), \n         Statut = fct_relevel(Statut,\n                              \"imm\", \"mat\", \"pnl\", \"l\", \"pl\", \"repos\"))\n\ndauphin\n\n# A tibble: 93 × 8\n   Sexe   Statut Taille   Age     Cd    Cu    Hg Organe\n   &lt;fct&gt;  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; \n 1 Female imm       315     3  29.6   3.24 NA    rein  \n 2 Female imm       357     4  55.1   4.42 NA    rein  \n 3 Female pnl       439    34 129.    5.01  9.02 rein  \n 4 Female imm       316     4  71.2   4.33 NA    rein  \n 5 Female l         435    26 192     5.15 NA    rein  \n 6 Female pnl       388     6  NA     4.12  4.53 rein  \n 7 Female mat       410    NA  76     5.1  33.9  foie  \n 8 Male   imm       355    NA  74.4   4.72 13.3  foie  \n 9 Male   imm       222    NA   0.09  9.5   2.89 foie  \n10 Male   imm       412     9  85.6   5.42 NA    rein  \n# ℹ 83 more rows\n\n\nÉvidemment, je ne vous demande pas d’être capable de produire un code tel que celui-ci du premier coup. D’ailleurs, ça n’est jamais comme ça qu’on construit ce type de bloc d’instructions. On procède étape par étape, et quand la première étape fonctionne, alors on passe à la suivante en ajoutant un pipe. Mais on s’assure bien que chaque étape fonctionne avant de passer à la suivante.\nOutre les fonctions fct_rev(), fct_recode() et fct_relevel() abordées ici, on peut aussi noter :\n\nfct_reorder() et fct_reorder2(), pour ordonner automatiquement les niveaux d’un facteur en fonction d’une autre variable numérique (pour avoir par exemple des séries rangées par ordre de moyennes croissantes sur un graphique).\nfct_infreq(), pour ordonner automatiquement les niveaux d’un facteur par ordre de fréquence croissante, ce qui est notamment utile pour faire des diagrammes bâtons ordonnés.\nfct_collapse(), pour fusionner deux ou plusieurs niveaux d’un facteur.\n\nNous n’avons pas le temps de développer ici des exemples pour chacune de ces fonctions, mais sachez que ces fonctions existent. Vous trouverez des exemples détaillés dans le chapitre consacré aux facteurs de l’ouvrage en ligne R for Data Science. C’est en anglais, mais les exemples sont très parlants. N’hésitez pas à consulter cet ouvrage et à faire des essais de mise en application avec les jeux de données vus ici (e.g. dauphin ou squid).\n\n\n5.6.3 Exercices\n\nDans ggplot2 le jeu de données mpg contient des informations sur 234 modèles de voitures. Examinez ce jeu de données avec la fonction View() et consultez l’aide pour savoir à quoi correspondent les différentes variables. Quelle(s) variable(s) nous renseignent sur la consommation des véhicules ? À quoi correspond la variable disp ?\nLa consommation sur autoroute est donnée en miles par gallon. Créez une nouvelle variable conso qui contiendra la consommation sur autoroute exprimée en nombre de litres pour 100 kilomètres.\nFaites un graphique présentant la relation entre la cylindrée en litres et la consommation sur autoroute exprimée en nombre de litres pour 100 kilomètres. Vous exclurez de ce graphique les véhicules dont la classe est 2seater (il s’agit de voitures de sports très compactes qu’il est difficile de mesurer aux autres). Sur votre graphique, la couleur devrait représenter le type de véhicule. Vous ajouterez une droite de régression en utilisant geom_smooth(method = \"lm\"). Votre graphique devrait ressembler à ceci :\n\n\n\n\n\n\nFigure 5.9: Consommation en fonction de la cylindrée\n\n\n\n\n\nCe graphique présente-t-il correctement l’ensemble des données de ces 2 variables ? Pourquoi ? Comparez la Figure 5.9 de la question 3 ci-dessus et la Figure 5.10 présentée ci-dessous. Selon vous, quels arguments et/ou fonctions ont été modifiés pour arriver à ce nouveau graphique ? Quels sont les avantages et les inconvénients de ce graphique par rapport au précédent ?\n\n\n\n\n\n\nFigure 5.10: Consommation en fonction de la cylindrée"
  },
  {
    "objectID": "05-DataWrangling.html#sec-arrange",
    "href": "05-DataWrangling.html#sec-arrange",
    "title": "5  Manipuler des tableaux avec dplyr",
    "section": "5.7 Trier des lignes avec arrange()",
    "text": "5.7 Trier des lignes avec arrange()\n\n\n\n\n\nFigure 5.11: Schéma de la fonction arrange() tiré de la ‘cheatsheet’ de dplyr et tidyr.\n\n\n\n\nLa fonction arrange() permet de trier des tableaux en ordonnant les éléments d’une ou plusieurs colonnes. Les tris peuvent être en ordre croissants (c’est le cas par défaut) ou décroissants (grâce à la fonction desc(), abréviation de “descending”).\narrange() fonctionne donc comme filter(), mais au lieu de sélectionner des lignes, cette fonction change leur ordre. Il faut lui fournir le nom d’un tableau et au minimum le nom d’une variable selon laquelle le tri doit être réalisé. Si plusieurs variables sont fournies, chaque variable supplémentaire permet de résoudre les égalités. Ainsi, pour ordonner le tableau penguins par ordre croissant d’épaisseur de bec (bill_depth_mm), on tape :\n\npenguins |&gt;\n  arrange(bill_depth_mm)\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           42.9          13.1               215        5000\n 2 Gentoo  Biscoe           46.1          13.2               211        4500\n 3 Gentoo  Biscoe           44.9          13.3               213        5100\n 4 Gentoo  Biscoe           43.3          13.4               209        4400\n 5 Gentoo  Biscoe           46.5          13.5               210        4550\n 6 Gentoo  Biscoe           42            13.5               210        4150\n 7 Gentoo  Biscoe           44            13.6               208        4350\n 8 Gentoo  Biscoe           40.9          13.7               214        4650\n 9 Gentoo  Biscoe           45.5          13.7               214        4650\n10 Gentoo  Biscoe           42.6          13.7               213        4950\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nNotez que la variable dbill_depth_mm est maintenant triée en ordre croissant. Notez également que 2 individus ont un bec dont l’épaisseur vaut exactement 13,5 mm. Comparez le tableau précédent avec celui-ci :\n\npenguins |&gt;\n  arrange(bill_depth_mm, bill_length_mm)\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           42.9          13.1               215        5000\n 2 Gentoo  Biscoe           46.1          13.2               211        4500\n 3 Gentoo  Biscoe           44.9          13.3               213        5100\n 4 Gentoo  Biscoe           43.3          13.4               209        4400\n 5 Gentoo  Biscoe           42            13.5               210        4150\n 6 Gentoo  Biscoe           46.5          13.5               210        4550\n 7 Gentoo  Biscoe           44            13.6               208        4350\n 8 Gentoo  Biscoe           40.9          13.7               214        4650\n 9 Gentoo  Biscoe           42.6          13.7               213        4950\n10 Gentoo  Biscoe           42.7          13.7               208        3950\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nLes lignes des 2 individus dont l’épaisseur du bec vaut 13,5 mm ont été inversées : la variable bill_length_mm a été utilisée pour ordonner les lignes en cas d’égalité de la variable bill_depth_mm.\nComme indiqué plus haut, il est possible de trier les données par ordre décroissant :\n\npenguins |&gt;\n  arrange(desc(bill_depth_mm))\n\n# A tibble: 344 × 8\n   species   island   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;     &lt;fct&gt;             &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie    Torgers…           46            21.5               194        4200\n 2 Adelie    Torgers…           38.6          21.2               191        3800\n 3 Adelie    Dream              42.3          21.2               191        4150\n 4 Adelie    Torgers…           34.6          21.1               198        4400\n 5 Adelie    Dream              39.2          21.1               196        4150\n 6 Adelie    Biscoe             41.3          21.1               195        4400\n 7 Chinstrap Dream              54.2          20.8               201        4300\n 8 Adelie    Torgers…           42.5          20.7               197        4500\n 9 Adelie    Biscoe             39.6          20.7               191        3900\n10 Chinstrap Dream              52            20.7               210        4800\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nCela est particulièrement utile après l’obtention de résumés groupés (obtenus avec la fonction count()) pour connaître la catégorie la plus représentée. Par exemple, si nous souhaitons connaître l’espèce et le sexe les plus fréquemment observés, on peut procéder ainsi :\n\nprendre le tableau penguins, puis,\ncompter le nombre d’observation par espèce et sexe avec la fonction count, puis,\ntrier les données par effectif décroissant.\n\n\npenguins |&gt;\n  count(species, sex) |&gt;\n  arrange(desc(n))\n\n# A tibble: 8 × 3\n  species   sex        n\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Gentoo    male      61\n4 Gentoo    female    58\n5 Chinstrap female    34\n6 Chinstrap male      34\n7 Adelie    &lt;NA&gt;       6\n8 Gentoo    &lt;NA&gt;       5\n\n\nDeux catégories sont aussi fréquemment observées l’une que l’autre : les mâles et femelles de l’espèce Adélie, pour lesquels 73 individus ont été observés."
  },
  {
    "objectID": "05-DataWrangling.html#sec-summarise",
    "href": "05-DataWrangling.html#sec-summarise",
    "title": "5  Manipuler des tableaux avec dplyr",
    "section": "5.8 Créer des résumés avec summarise()",
    "text": "5.8 Créer des résumés avec summarise()\nDans cette partie, nous allons en réalité traiter un peu plus que de la simple fonction summarise(). Nous aborderons :\n\nsummarise() : pour créer des résumés de données simples à partir des colonnes d’un tableau\ncount() : pour compter le nombre d’observations pour chaque niveau d’un facteur (ou modalité d’une variable catégorielle)\ngroup_by() : pour effectuer des opérations pour chaque niveau d’un facteur (ou modalité d’une variable catégorielle). Cette dernière fonction a été rendue presque obsolète par une mise à jour récente du package dplyr qui introduit un nouvel argument pour plusieurs fonctions, dont summarise() (mais aussi mutate(), filter() et quelques autres) : l’argument .by. Un peu comme group-by(), ce nouvel argument permet d’effectuer des opérations pour chaque niveau d’un facteur (ou modalité d’une variable catégorielle). À notre niveau, les différences entre la fonction group_by() et l’argument .by ne sont pas importantes. Nous utiliserons donc de préférence la notation la plus simple, celle de l’argument .by.\n\nLa fonction reframe() est très proche de la fonction summarise() car elle permet de créer des résumés de données plus élaborés à partir des colonnes d’un tableau. Nous verrons comment l’utiliser dans le chapitre dédié aux statistiques descriptives (Chapitre 6).\n\n\n\n\n\n\nLien avec les statistiques descriptives\n\n\n\nCette section est importante car elle permet de faire un premier lien avec les statistiques. La plupart des fonctions décrites ici servent en effet à produire des résumés statistiques pour des variables de tous types, ou pour des modalités spécifiques de facteurs d’intérêt.\n\n\n\n5.8.1 Principe de la fonction summarise()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.12: Schéma de la fonction summarise() tiré de la ‘cheatsheet’ de dplyr et tidyr.\n\n\n\n\nLa Figure 5.12 ci-dessus indique comment travaille la fonction summarise() : elle prend plusieurs valeurs (potentiellement, un très grand nombre) et les réduit à une unique valeur qui les résume. La valeur qui résume les données est choisie par l’utilisateur. Il peut s’agir par exemple d’un calcul de moyenne, de quartile ou de variance, il peut s’agir de calculer une somme, ou d’extraire la valeur maximale ou minimale, ou encore, il peut tout simplement s’agir de déterminer un nombre d’observations. Mais le fonctionnement est toujours le même : la fonction summarise() ne renvoie qu’une unique valeur pour une variable donnée (ou pour chaque modalité d’une variable catégorielle).\nAinsi, pour connaître la moyenne de la longueur du bec des manchots de l’île de Palmer, il suffit d’utiliser le tableau penguins du package palmerpenguins et sa variable bill_length_mm que nous avons déjà utilisée plus tôt :\n\npenguins |&gt;\n  summarise(moyenne = mean(bill_length_mm))\n\n# A tibble: 1 × 1\n  moyenne\n    &lt;dbl&gt;\n1      NA\n\n\nLa fonction mean() permet de calculer une moyenne. Ici, la valeur retournée est NA car 2 individus n’ont pas été mesurés, et le tableau contient donc des valeurs manquantes :\n\npenguins |&gt;\n  filter(is.na(bill_length_mm))\n\n# A tibble: 2 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen             NA            NA                NA          NA\n2 Gentoo  Biscoe                NA            NA                NA          NA\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nPour obtenir la valeur souhaitée, il faut indiquer à R d’exclure les valeurs manquantes lors du calcul de moyenne :\n\npenguins |&gt;\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  moyenne\n    &lt;dbl&gt;\n1    43.9\n\n\nLa longueur moyenne du bec des manchots (toutes espèces confondues) est donc de 43.9 millimètres.\nDe la même façon, on peut demander plusieurs calculs d’indices à la fois, par exemple la moyenne et l’écart-type (avec la fonction sd()) de la longueur des becs :\n\npenguins |&gt;\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE),\n            ecart_type = sd(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 1 × 2\n  moyenne ecart_type\n    &lt;dbl&gt;      &lt;dbl&gt;\n1    43.9       5.46\n\n\nIci, l’écart-type vaut 5.5 millimètres.\nLa fonction summarise() permet donc de calculer des indices statistiques variés, et permet aussi d’accéder à plusieurs variables à la fois. Par exemple. pour calculer les moyennes, médianes, minima et maxima des longueurs de nageoires et de masses corporelles, on peut procéder ainsi :\n\npenguins |&gt; \n  summarise(moy_flip = mean(flipper_length_mm, na.rm = TRUE),\n            med_flip = median(flipper_length_mm, na.rm = TRUE),\n            min_flip = min(flipper_length_mm, na.rm = TRUE),\n            max_flip = max(flipper_length_mm, na.rm = TRUE),\n            moy_mass = mean(body_mass_g, na.rm = TRUE),\n            med_mass = median(body_mass_g, na.rm = TRUE),\n            min_mass = min(body_mass_g, na.rm = TRUE),\n            max_mass = max(body_mass_g, na.rm = TRUE))\n\n# A tibble: 1 × 8\n  moy_flip med_flip min_flip max_flip moy_mass med_mass min_mass max_mass\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;\n1     201.      197      172      231    4202.     4050     2700     6300\n\n\nLa fonction summarise() est donc très utile pour produire des résumés informatifs des données, mais nos exemples ne sont ici pas très pertinents puisque nous avons jusqu’ici calculé des indices sans distinguer les espèces. Si les 3 espèces de manchots ont des caractéristiques très différentes, calculer des moyennes toutes espèces confondues n’a pas de sens. Voyons maintenant comment obtenir ces même indices pour chaque espèce.\n\n\n5.8.2 Intérêt de l’argument .by\nLa fonction summarise() devient particulièrement puissante lorsqu’on y ajoute l’argument .by :\n\n\n\n\n\nFigure 5.13: Fonctionnement de l’argument .by travaillant de concert avec summarise(), tiré de la ‘cheatsheet’ de dplyr et tidyr\n\n\n\n\nComme son nom l’indique, l’argument .by permet de créer des sous-groupes dans un tableau, afin que le résumé des données soit calculé pour chacun des sous-groupes plutôt que sur l’ensemble du tableau. En ce sens, son fonctionnement est analogue à celui des facets de ggplot2 qui permettent de scinder les données d’un graphique en plusieurs sous-groupes.\nPour revenir à l’exemple de la longueur du bec des manchots, imaginons que nous souhaitions calculer les moyennes et les écart-types pour chacune des trois espèces. Voilà comment procéder :\n\npenguins |&gt;\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE),\n            ecart_type = sd(bill_length_mm, na.rm = TRUE),\n            .by = species)\n\n# A tibble: 3 × 3\n  species   moyenne ecart_type\n  &lt;fct&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 Adelie       38.8       2.66\n2 Gentoo       47.5       3.08\n3 Chinstrap    48.8       3.34\n\n\nIci, les étapes sont les suivantes :\n\nOn prend le tableau penguins, puis\nOn résume les données sous la forme de moyennes et d’écart-types\nOn demande un calcul pour chaque modalité de la variable species\n\nLà où nous avions auparavant une seule valeur de moyenne et d’écart-type pour l’ensemble des individus du tableau de données, nous avons maintenant une valeur de moyenne et d’écart-type pour chaque modalité de la variable espèce. Puisque le facteur species contient 3 modalités (Adelie, Chinstrap et Gentoo), le résumé des données contient maintenant 3 lignes.\nCette syntaxe très simple est presque équivalente à celle de la fonction group_by() :\n\npenguins |&gt;\n  group_by(species) |&gt; \n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE),\n            ecart_type = sd(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 3 × 3\n  species   moyenne ecart_type\n  &lt;fct&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 Adelie       38.8       2.66\n2 Chinstrap    48.8       3.34\n3 Gentoo       47.5       3.08\n\n\nLes valeurs obtenues sont les mêmes, mais d’une part, les commandes sont fournies avec une syntaxe et dans un ordre différents :\n\nOn prend le tableau penguins, puis\nOn groupe les données par espèce, puis\nOn résume les données sous la forme de moyennes et d’écart-types\n\nEt l’objet obtenu au final n’est pas strictement identique : avec la fonction group_by(), et dans certaines situations, le tibble obtenu conserve l’information du regroupement effectué, ce qui peut être utile dans certaines situations, mais peut parfois poser problème et causer l’affichage de messages d’avertissements dans la console. Ce comportement n’est pas observé avec l’argument .by qui ne groupe les données qu’au moment du calcul des indices dans la fonction summarise() et n’en conserve pas la trace ensuite. C’est la raison pour laquelle nous privilégierons cette méthode.\nPour aller plus loin, ajoutons à ce résumé 2 variables supplémentaires : le nombre de mesures et l’erreur standard (notée \\(se\\)), qui peut être calculée de la façon suivante :\n\\[se \\approx \\frac{s}{\\sqrt{n}}\\]\navec \\(s\\), l’écart-type de l’échantillon et \\(n\\), la taille de l’échantillon (plus d’informations sur cette statistique très importante dans la Chapitre 7). Nous allons donc calculer ici ces résumés, et nous donnerons un nom au tableau créé pour pouvoir ré-utiliser ces statistiques descriptives :\n\nstats_esp &lt;- penguins |&gt;\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE),\n            ecart_type = sd(bill_length_mm, na.rm = TRUE),\n            nb_obs = n(),\n            erreur_std = ecart_type / sqrt(nb_obs),\n            .by = species)\n\nstats_esp\n\n# A tibble: 3 × 5\n  species   moyenne ecart_type nb_obs erreur_std\n  &lt;fct&gt;       &lt;dbl&gt;      &lt;dbl&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 Adelie       38.8       2.66    152      0.216\n2 Gentoo       47.5       3.08    124      0.277\n3 Chinstrap    48.8       3.34     68      0.405\n\n\nVous constatez ici que nous avons 4 statistiques descriptives pour chaque espèce. Deux choses sont importantes à retenir ici :\n\non peut obtenir le nombre d’observations dans chaque sous-groupe d’un tableau groupé en utilisant la fonction n(). Cette fonction n’a besoin d’aucun argument : elle détermine automatiquement la taille des groupes créés par .by (ou par la fonction group_by()).\non peut créer de nouvelles variables en utilisant le nom de variables créées auparavant. Ainsi, nous avons créé la variable erreur_std en utilisant deux variables créées au préalable : ecart-type et nb_obs\n\n\n\n5.8.3 Grouper par plus d’une variable\nJusqu’ici, nous avons groupé les données par espèce. Il est tout à fait possible de grouper les données par plus d’une variable, par exemple, par espèce et par sexe :\n\nstats_esp_sex &lt;- penguins |&gt;\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE),\n            ecart_type = sd(bill_length_mm, na.rm = TRUE),\n            nb_obs = n(),\n            erreur_std = ecart_type / sqrt(nb_obs),\n            .by = c(species, sex))\n\nstats_esp_sex\n\n# A tibble: 8 × 6\n  species   sex    moyenne ecart_type nb_obs erreur_std\n  &lt;fct&gt;     &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 Adelie    male      40.4       2.28     73      0.267\n2 Adelie    female    37.3       2.03     73      0.237\n3 Adelie    &lt;NA&gt;      37.8       2.80      6      1.14 \n4 Gentoo    female    45.6       2.05     58      0.269\n5 Gentoo    male      49.5       2.72     61      0.348\n6 Gentoo    &lt;NA&gt;      45.6       1.37      5      0.615\n7 Chinstrap female    46.6       3.11     34      0.533\n8 Chinstrap male      51.1       1.56     34      0.268\n\n\nEn plus de la variable species, la tableau stats_esp_sex contient une variable sex. Les statistiques que nous avons calculées plus tôt sont maintenant disponibles pour chaque espèce et chaque sexe. D’ailleurs, puisque le sexe de certains individus est inconnu, nous avons également des lignes pour lesquelles le sexe affiché est NA. Pour les éliminer, il suffit de retirer les lignes du tableau pour lesquelles le sexe des individus est inconnu avant de recalculer les mêmes indices :\n\nstats_esp_sex2 &lt;- penguins |&gt;\n  filter(!is.na(sex)) |&gt; \n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE),\n            ecart_type = sd(bill_length_mm, na.rm = TRUE),\n            nb_obs = n(),\n            erreur_std = ecart_type / sqrt(nb_obs),\n            .by = c(species, sex))\n\nstats_esp_sex2\n\n# A tibble: 6 × 6\n  species   sex    moyenne ecart_type nb_obs erreur_std\n  &lt;fct&gt;     &lt;fct&gt;    &lt;dbl&gt;      &lt;dbl&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 Adelie    male      40.4       2.28     73      0.267\n2 Adelie    female    37.3       2.03     73      0.237\n3 Gentoo    female    45.6       2.05     58      0.269\n4 Gentoo    male      49.5       2.72     61      0.348\n5 Chinstrap female    46.6       3.11     34      0.533\n6 Chinstrap male      51.1       1.56     34      0.268\n\n\nSi vous ne comprenez pas la commande filter(!is.na(sex)), je vous encourage vivement à relire la Section 5.4.\nEnfin, lorsque nous groupons par plusieurs variables, il peut être utile de présenter les résultats sous la forme d’un tableau large (grâce à la fonction pivot_wider()) pour l’intégration dans un rapport par exemple (voir la Section 4.3.2). La fonction pivot_wider() permet de passer d’un tableau qui possède ce format :\n\npenguins |&gt;\n  filter(!is.na(sex)) |&gt; \n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE),\n            .by = c(species, sex))\n\n# A tibble: 6 × 3\n  species   sex    moyenne\n  &lt;fct&gt;     &lt;fct&gt;    &lt;dbl&gt;\n1 Adelie    male      40.4\n2 Adelie    female    37.3\n3 Gentoo    female    45.6\n4 Gentoo    male      49.5\n5 Chinstrap female    46.6\n6 Chinstrap male      51.1\n\n\nà un tableau sous ce format :\n\npenguins |&gt;\n  filter(!is.na(sex)) |&gt; \n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE),\n            .by = c(species, sex)) |&gt; \n  pivot_wider(names_from = sex,\n              values_from = moyenne)\n\n# A tibble: 3 × 3\n  species    male female\n  &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 Adelie     40.4   37.3\n2 Gentoo     49.5   45.6\n3 Chinstrap  51.1   46.6\n\n\nSous cette forme, les données ne sont plus “rangées”, c’est à dire que nous n’avons plus une observation par ligne et une variable par colonne. En effet ici, la variable sex est maintenant “étalée” dans 2 colonnes distinctes : chaque modalité du facteur de départ (female et male) est utilisée en tant que titre de nouvelles colonnes, et la variable moyenne est répartie dans deux colonnes. Ce format de tableau n’est pas idéal pour les statistiques ou les représentations graphiques, mais il est plus synthétique, et donc plus facile à inclure dans un rapport ou un compte-rendu.\n\n\n5.8.4 Un raccourci pratique pour compter des effectifs\nIl est extrêmement fréquent d’avoir à grouper des données en fonction d’une variable catégorielle puis d’avoir à compter le nombre d’observations de chaque modalité avec n() :\n\npenguins |&gt; \n  summarise(effectif = n(), \n            .by = species)\n\n# A tibble: 3 × 2\n  species   effectif\n  &lt;fct&gt;        &lt;int&gt;\n1 Adelie         152\n2 Gentoo         124\n3 Chinstrap       68\n\n\nou encore :\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarise(effectif = n())\n\n# A tibble: 3 × 2\n  species   effectif\n  &lt;fct&gt;        &lt;int&gt;\n1 Adelie         152\n2 Chinstrap       68\n3 Gentoo         124\n\n\nCes deux opérations sont tellement fréquentes (regrouper puis compter) que le package dplyr nous fournit un raccourci : la fonction count().\nLe code ci-dessus est équivalent à celui-ci :\n\npenguins |&gt; \n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nNotez qu’avec la fonction count(), la colonne qui contient les comptages s’appelle toujours n par défaut. Comme avec .by et group_by(), il est bien sûr possible d’utiliser count() avec plusieurs variables :\n\npenguins |&gt; \n  count(species, sex)\n\n# A tibble: 8 × 3\n  species   sex        n\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n\n\npenguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  count(species, sex)\n\n# A tibble: 6 × 3\n  species   sex        n\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Chinstrap female    34\n4 Chinstrap male      34\n5 Gentoo    female    58\n6 Gentoo    male      61\n\n\nEt il est évidemment possible de présenter le résultats sous un format de tableau large :\n\npenguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  count(species, sex) |&gt; \n  pivot_wider(names_from = sex,\n              values_from = n)\n\n# A tibble: 3 × 3\n  species   female  male\n  &lt;fct&gt;      &lt;int&gt; &lt;int&gt;\n1 Adelie        73    73\n2 Chinstrap     34    34\n3 Gentoo        58    61\n\n\nVous connaissez maintenant plusieurs méthodes pour calculer à la main des statistiques descriptives pour des variables entières, ou pour des sous-groupes de lignes (par espèce, par sexe, par sexe et par espèce…). Globalement, toutes les fonctions de R qui prennent une série de chiffres en guise d’argument, et qui renvoient une valeur unique, peuvent être utilisées avec la fonction summarise(). En particulier, vous pouvez utiliser les fonctions suivantes pour faire des analyses exploratoires :\n\nmean() : calcul de la moyenne\nmedian() : calcul de la médiane\nmin() : affichage de la valeur minimale\nmax() : affichage de la valeur minimale\nn_distinct() : calcul du nombre de valeurs différentes\nn() : calcul du nombre d’observations\nvar() : calcul de la variance\nsd() : calcul de l’écart-type\nIQR() : calcul de l’intervalle inter-quartiles\n\nEt la liste n’est bien sûr pas exhaustive\n\n\n5.8.5 Exercices\n\nAvec le tableau diamonds du package ggplot2, faites un tableau indiquant combien de diamants de chaque couleur on dispose. Vous devriez obtenir le tableau suivant :\n\n\n\n# A tibble: 7 × 2\n  color     n\n  &lt;ord&gt; &lt;int&gt;\n1 D      6775\n2 E      9797\n3 F      9542\n4 G     11292\n5 H      8304\n6 I      5422\n7 J      2808\n\n\n\nExaminez le tableau weather du package nycflights13 et lisez son fichier d’aide pour comprendre à quoi correspondent les données et comment elles ont été acquises.\nÀ partir du tableau weather faites un tableau indiquant les vitesses de vents minimales, maximales et moyennes, enregistrées chaque mois dans chaque aéroport de New York. Indice : les 3 aéroports de New York sont Newark, LaGuardia Airport et John F. Kennedy, notés respectivement EWR, LGA et JFK dans la variable origin. Votre tableau devrait ressembler à ceci :\n\n\n\n# A tibble: 36 × 5\n   origin month max_wind min_wind moy_wind\n   &lt;chr&gt;  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 EWR        1     42.6        0     9.87\n 2 EWR        2   1048.         0    12.2 \n 3 EWR        3     29.9        0    11.6 \n 4 EWR        4     25.3        0     9.63\n 5 EWR        5     33.4        0     8.49\n 6 EWR        6     34.5        0     9.55\n 7 EWR        7     20.7        0     9.15\n 8 EWR        8     21.9        0     7.62\n 9 EWR        9     23.0        0     8.03\n10 EWR       10     26.5        0     8.32\n# ℹ 26 more rows\n\n\n\nSachant que les vitesses du vent sont exprimées en miles par heure, certaines valeurs sont-elles surprenantes ? À l’aide de la fonction filter(), éliminez la ou les valeurs aberrantes. Vous devriez obtenir ce tableau :\n\n\n\n# A tibble: 36 × 5\n   origin month max_wind min_wind moy_wind\n   &lt;chr&gt;  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 EWR        1     42.6        0     9.87\n 2 EWR        2     31.1        0    10.7 \n 3 EWR        3     29.9        0    11.6 \n 4 EWR        4     25.3        0     9.63\n 5 EWR        5     33.4        0     8.49\n 6 EWR        6     34.5        0     9.55\n 7 EWR        7     20.7        0     9.15\n 8 EWR        8     21.9        0     7.62\n 9 EWR        9     23.0        0     8.03\n10 EWR       10     26.5        0     8.32\n# ℹ 26 more rows\n\n\n\nEn utilisant les données de vitesse de vent du tableau weather, produisez le graphique suivant :\n\n\n\n\n\n\nIndications :\n\nles vitesses de vent aberrantes ont été éliminées grâce à la fonction filter()\nla fonction geom_jitter() a été utilisée avec l’argument height = 0\nla transparence des points est fixée à 0.2\n\n\nÀ votre avis :\n\n\npourquoi les points sont-ils organisés en bandes horizontales ?\npourquoi n’y a-t-il jamais de vent entre 0 et environ 3 miles à l’heure (mph) ?\nSachant qu’en divisant des mph par 1.151 on obtient des vitesses en nœuds, que nous apprend cette commande :\n\n\nsort(unique(weather$wind_speed)) / 1.151\n\n [1]   0.000000   2.999427   3.999235   4.999044   5.998853   6.998662\n [7]   7.998471   8.998280   9.998089  10.997897  11.997706  12.997515\n[13]  13.997324  14.997133  15.996942  16.996751  17.996560  18.996368\n[19]  19.996177  20.995986  21.995795  22.995604  23.995413  24.995222\n[25]  25.995030  26.994839  27.994648  28.994457  29.994266  30.994075\n[31]  31.993884  32.993692  33.993501  34.993310  36.992928 910.825873"
  },
  {
    "objectID": "05-DataWrangling.html#utiliser-.by-avec-dautres-fonctions",
    "href": "05-DataWrangling.html#utiliser-.by-avec-dautres-fonctions",
    "title": "5  Manipuler des tableaux avec dplyr",
    "section": "5.9 Utiliser .by avec d’autres fonctions",
    "text": "5.9 Utiliser .by avec d’autres fonctions\nOutre la fonction summarise(), de nombreuses autres fonctions du tidyverse possèdent un argument .by. C’est par exemple le cas des fonctions filter() et mutate() décrites plus haut, ou de la fonction reframe() que nous découvrirons dans la Section 6.2.\n\n5.9.1 Un exemple avec filter()\nPour vous montrer à quel point cet argument peut-être utile dans d’autres contextes que celui du calcul de résumés statistiques, prenons un premier exemple. Imaginez que nous souhaitions identifier, dans le tableau penguins, les 10% des individus les plus lourds pour chaque espèce. Une première façon de faire serait de procéder espèce par espèce, en tapant ceci :\n\n# Pour l'espèce Adélie\nadelie &lt;- penguins |&gt; \n  filter(species == \"Adelie\")\n\nadelie_lourds &lt;- adelie |&gt; \n  filter(body_mass_g &gt; quantile(body_mass_g, 0.9, na.rm = TRUE))\n\nadelie_lourds \n\n# A tibble: 15 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.2          19.6               195        4675\n 2 Adelie  Torgersen           34.6          21.1               198        4400\n 3 Adelie  Torgersen           42.5          20.7               197        4500\n 4 Adelie  Dream               39.8          19.1               184        4650\n 5 Adelie  Dream               44.1          19.7               196        4400\n 6 Adelie  Dream               39.6          18.8               190        4600\n 7 Adelie  Biscoe              41.3          21.1               195        4400\n 8 Adelie  Torgersen           41.8          19.4               198        4450\n 9 Adelie  Torgersen           42.9          17.6               196        4700\n10 Adelie  Dream               39.6          18.1               186        4450\n11 Adelie  Dream               40.3          18.5               196        4350\n12 Adelie  Biscoe              41            20                 203        4725\n13 Adelie  Biscoe              43.2          19                 197        4775\n14 Adelie  Biscoe              45.6          20.3               191        4600\n15 Adelie  Dream               37.5          18.5               199        4475\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nIci, j’ai commencé par créer un tableau adelie qui ne contient que les individus de cette espèce. L’expression quantile(body_mass_g, 0.9, na.rm = TRUE) permet d’identifier le quatre vingt dixième percentile des masses corporelles en grammes pour cette espèce, et la fonction filter() me permet de ne récupérer que les individus dont la masse est supérieure à cette valeur. Ici, nous récupérons 15 individus dans le tableau adelie_lourds, qui correspondent donc aux 10% des individus les plus lourds pour cette espèce. Pour obtenir la même choses pour les 3 espèces, il me faut reproduire ce code pour les manchots Chinstrap et Gentoo, puis il faut que je fusionne les 3 tableaux (voir les détails de cette fonction dans la Section 5.10.4) :\n\n# Pour l'espèce Chinstrap\nchinstrap &lt;- penguins |&gt; \n  filter(species == \"Chinstrap\")\n\nchinstrap_lourds &lt;- chinstrap |&gt; \n  filter(body_mass_g &gt; quantile(body_mass_g, 0.9, na.rm = TRUE))\n\nchinstrap_lourds \n\n# A tibble: 7 × 8\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Chinstrap Dream            49.2          18.2               195        4400\n2 Chinstrap Dream            52.8          20                 205        4550\n3 Chinstrap Dream            54.2          20.8               201        4300\n4 Chinstrap Dream            52            20.7               210        4800\n5 Chinstrap Dream            53.5          19.9               205        4500\n6 Chinstrap Dream            50.8          18.5               201        4450\n7 Chinstrap Dream            49            19.6               212        4300\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# Pour l'espèce Gentoo\ngentoo &lt;- penguins |&gt; \n  filter(species == \"Gentoo\")\n\ngentoo_lourds &lt;- gentoo |&gt; \n  filter(body_mass_g &gt; quantile(body_mass_g, 0.9, na.rm = TRUE))\n\ngentoo_lourds \n\n# A tibble: 12 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           48.4          14.6               213        5850\n 2 Gentoo  Biscoe           49.3          15.7               217        5850\n 3 Gentoo  Biscoe           49.2          15.2               221        6300\n 4 Gentoo  Biscoe           59.6          17                 230        6050\n 5 Gentoo  Biscoe           49.5          16.2               229        5800\n 6 Gentoo  Biscoe           48.6          16                 230        5800\n 7 Gentoo  Biscoe           51.1          16.3               220        6000\n 8 Gentoo  Biscoe           45.2          16.4               223        5950\n 9 Gentoo  Biscoe           49.8          15.9               229        5950\n10 Gentoo  Biscoe           55.1          16                 230        5850\n11 Gentoo  Biscoe           48.8          16.2               222        6000\n12 Gentoo  Biscoe           50.4          15.7               222        5750\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n# Fusion des 3 tableaux\nheavy_penguins &lt;- bind_rows(adelie_lourds, chinstrap_lourds, gentoo_lourds)\nheavy_penguins\n\n# A tibble: 34 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.2          19.6               195        4675\n 2 Adelie  Torgersen           34.6          21.1               198        4400\n 3 Adelie  Torgersen           42.5          20.7               197        4500\n 4 Adelie  Dream               39.8          19.1               184        4650\n 5 Adelie  Dream               44.1          19.7               196        4400\n 6 Adelie  Dream               39.6          18.8               190        4600\n 7 Adelie  Biscoe              41.3          21.1               195        4400\n 8 Adelie  Torgersen           41.8          19.4               198        4450\n 9 Adelie  Torgersen           42.9          17.6               196        4700\n10 Adelie  Dream               39.6          18.1               186        4450\n# ℹ 24 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nNous récupérons ainsi une unique table contenant 34 que nous pourrions ensuite utiliser pour des statistiques et/ou des représentations graphiques. L’utilisation de l’argument .by pourrait toutefois nous éviter de taper beaucoup de code pour aboutir au même résultat :\n\npenguins |&gt; \n  filter(body_mass_g &gt; quantile(body_mass_g, 0.9, na.rm = TRUE),\n         .by = species)\n\n# A tibble: 34 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.2          19.6               195        4675\n 2 Adelie  Torgersen           34.6          21.1               198        4400\n 3 Adelie  Torgersen           42.5          20.7               197        4500\n 4 Adelie  Dream               39.8          19.1               184        4650\n 5 Adelie  Dream               44.1          19.7               196        4400\n 6 Adelie  Dream               39.6          18.8               190        4600\n 7 Adelie  Biscoe              41.3          21.1               195        4400\n 8 Adelie  Torgersen           41.8          19.4               198        4450\n 9 Adelie  Torgersen           42.9          17.6               196        4700\n10 Adelie  Dream               39.6          18.1               186        4450\n# ℹ 24 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nCette syntaxe évite de dupliquer le code inutilement, elle évite d’avoir à créer des objets supplémentaires inutiles, et permet donc de limiter les risques d’erreur. Il vous sera donc très utile par la suite de prendre l’habitude de réfléchir aux questions posées en pensant à cet argument .by.\n\n\n5.9.2 Un exemple avec mutate()\nPour bien enfoncer le clou, voici un autre exemple, impliquant la fonction mutate(). Imaginez que nous ayons besoin d’identifier, toujours dans le tableau penguins, les individus de chaque espèce dont la longueur du bec est supérieure à la moyenne de l’espèce. Il nous faut donc calculer, pour chaque espèce, la moyenne des longueur de bec, puis créer une nouvelle colonne de vrais/faux pour chaque individu de chaque espèce : ceux dont le bec est supérieur à la moyenne auront TRUE dans cette nouvelle colonne, les autres auront FALSE. Voilà comment nous pourrions procéder grâce à l’argument .by :\n\nbecs &lt;- penguins |&gt; \n  mutate(long_bill = bill_length_mm &gt;= mean(bill_length_mm, na.rm = TRUE),\n         .by = species)\n\nbecs\n\n# A tibble: 344 × 9\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 3 more variables: sex &lt;fct&gt;, year &lt;int&gt;, long_bill &lt;lgl&gt;\n\n\nVous voyez qu’on peut obtenir le résultat souhaité en une seule commande. Pour mieux voir la nouvelle colonne :\n\nbecs |&gt; \n  relocate(long_bill)\n\n# A tibble: 344 × 9\n   long_bill species island    bill_length_mm bill_depth_mm flipper_length_mm\n   &lt;lgl&gt;     &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;\n 1 TRUE      Adelie  Torgersen           39.1          18.7               181\n 2 TRUE      Adelie  Torgersen           39.5          17.4               186\n 3 TRUE      Adelie  Torgersen           40.3          18                 195\n 4 NA        Adelie  Torgersen           NA            NA                  NA\n 5 FALSE     Adelie  Torgersen           36.7          19.3               193\n 6 TRUE      Adelie  Torgersen           39.3          20.6               190\n 7 TRUE      Adelie  Torgersen           38.9          17.8               181\n 8 TRUE      Adelie  Torgersen           39.2          19.6               195\n 9 FALSE     Adelie  Torgersen           34.1          18.1               193\n10 TRUE      Adelie  Torgersen           42            20.2               190\n# ℹ 334 more rows\n# ℹ 3 more variables: body_mass_g &lt;int&gt;, sex &lt;fct&gt;, year &lt;int&gt;\n\n\nAvec ce nouveau tableau, nous pourrions faire des statistiques ou des graphiques intéressants :\n\nbecs |&gt; \n  filter(!is.na(sex)) |&gt; \n  ggplot(aes(x = species, y = body_mass_g, color = long_bill)) +\n  geom_jitter(height = 0, width = 0.2) +\n  facet_wrap(~sex, ncol = 2) +\n  labs(x = \"\", y = \"Masse corporelle (g)\", color = \"Taille du bec\") +\n  theme_bw() +\n  scale_color_manual(values = c(\"purple\", \"orange\"), labels = c(\"Court\", \"Long\"))\n\n\n\n\nFigure 5.14: Masse en gramme des mâles et des femelles des 3 espèces de manchots. Les femelles ont majoritairement un bec plus court (en violet) que la moyenne de l’espèce et les mâles plus longs (en orange) que la moyenne de l’espèce.\n\n\n\n\nMais attention, ça n’est pas du tout la même chose que de taper ceci :\n\npenguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  ggplot(aes(x = species, y = body_mass_g, color = bill_length_mm &gt;= mean(bill_length_mm, na.rm = TRUE))) +\n  geom_jitter(height = 0, width = 0.2) +\n  facet_wrap(~sex, ncol = 2) +\n  labs(x = \"\", y = \"Masse corporelle (g)\", color = \"Taille du bec\") +\n  theme_bw() +\n  scale_color_manual(values = c(\"purple\", \"orange\"), labels = c(\"Court\", \"Long\"))\n\n\n\n\nFigure 5.15: Masse en gramme des mâles et des femelles des 3 espèces de manchots. Les manchots Adélie ont majoritairement un bec plus court (en violet) que la moyenne toutes espèces confondues.\n\n\n\n\nDans ce code, deux choses ont changé\n\nje suis parti du tableau penguins pour faire le graphique, et non du tableau becs créé plus haut\net j’ai associé la couleur des points à cette expression : bill_length_mm &gt;= mean(bill_length_mm, na.rm = TRUE).\n\nAutrement dit, les points seront d’une couleur si l’individu possède un bec plus long que la moyenne, et d’une autre couleur sinon. Mais attention, ici, il est impossible de travailler à l’échelle de chaque espèce. La moyenne des longueurs de becs qui est calculée est la moyenne toutes espèces confondues. Par conséquent, les manchots Adélie, qui ont les becs plus courts que les 2 autres espèces, apparaissent majoritairement en violet et les deux autres espèces en orange. Si ce graphique peut être utile également, il ne raconte donc pas du tout la même histoire que le précédent. Ici, nous voyons des différences inter-spécifiques, auparavant, nous mettions en évidence un dimorphisme sexuel."
  },
  {
    "objectID": "05-DataWrangling.html#associer-plusieurs-tableaux-avec-left_join-et-inner_join",
    "href": "05-DataWrangling.html#associer-plusieurs-tableaux-avec-left_join-et-inner_join",
    "title": "5  Manipuler des tableaux avec dplyr",
    "section": "5.10 Associer plusieurs tableaux avec left_join() et inner_join()",
    "text": "5.10 Associer plusieurs tableaux avec left_join() et inner_join()\n\n5.10.1 Principe\nUne autre règle que nous n’avons pas encore évoquée au sujet des “tidy data” ou “données rangées” est la suivante :\n\nChaque tableau contient des données appartenant à une unité d’observation cohérente et unique.\n\nAinsi, le package nycflights13 contient 5 tableaux distincts :\n\nhelp(package = \"nycflights13\")\n\n\nflights contient des informations concernant les vols intérieurs ayant décollé des 3 aéroports de New York en 2013 (par exemple heure prévue de décollage et d’arrivée, heure effective de décollage et d’arrivée, numéro du vol et compagnie aérienne, date et heure du vol, code des aéroports d’origine et de destination, etc.)\nairlines contient des informations au sujet des compagnies aériennes (code et nom complet de chaque compagnie aérienne)\nairports contient des informations au sujet des aéroports de New York et de tous les aéroports desservis par les vols au décollage de New York (code et nom complet de chaque aéroport, latitude, longitude et altitude de chaque aéroport, etc.)\nplanes contient des informations au sujet de chacun des avions ayant desservi Ney York en 2013 (numéro d’immatriculation, année de fabrication, type d’avion, modèle et fabricant, type de moteur, nombre de places, vitesse de croisière, etc.)\n\nÇa n’aurait pas de sens de faire figurer toutes ces informations dans le même tableau. Pourtant, lorsque l’on traite des données, on constate souvent qu’un même tableau contient des variables qui concernent des unités d’observations différentes qu’il conviendrait de scinder en plusieurs tableaux. Et à l’inverse, lorsque nous disposons de plusieurs tableaux, il est parfois nécessaire de récupérer des informations dans plusieurs d’entre eux afin, notamment de produire des tableaux de synthèse ou de rechercher des tendances inattendues.\nPour illustrer ce besoin, nous allons nous poser 2 questions en relation avec les données du package nycflights13 :\n\nQuelles sont les destinations les plus fréquemment desservies par les vols ayant décollé de New York en 2013 ?\nPeut-on dire que les retards constatés à l’arrivée des vols sont liés à l’année de fabrication des avions (et donc, dans une certaine mesure, à leur vétusté) ?\n\nRépondre à la première question est assez simple : il suffit en apparence de compter, parmi les vols du tableau flights, le nombre de vols pour chaque destinations (variable dest), puis de trier le résultat par ordre décroissant :\n\nflights |&gt;\n  count(dest) |&gt;\n  arrange(desc(n))\n\n# A tibble: 105 × 2\n   dest      n\n   &lt;chr&gt; &lt;int&gt;\n 1 ORD   17283\n 2 ATL   17215\n 3 LAX   16174\n 4 BOS   15508\n 5 MCO   14082\n 6 CLT   14064\n 7 SFO   13331\n 8 FLL   12055\n 9 MIA   11728\n10 DCA    9705\n# ℹ 95 more rows\n\n\nLe problème est ici que les aéroports de destination sont renseignés sous la forme d’un code à 3 lettres. À quel aéroport correspondent les codes ORD et ATL ? S’agit-il d’Orlando et Atlanta ? Pour le savoir, il faut aller chercher l’information qui se trouve dans le tableau airports : il contient, parmi d’autres variables, les codes et les noms de 1458 aéroports aux État-Unis. Il va donc nous falloir trouver un moyen de fusionner les informations du tableau que nous venons de créer (et auquel nous allons donner le nom popular_dest), avec les informations contenues dans le tableau airports.\n\npopular_dest &lt;- flights |&gt;\n  count(dest) |&gt;\n  arrange(desc(n))\n\nPour répondre à la deuxième question, on commence là aussi par chercher des informations dans le tableau flights au sujet des retards à l’arrivée. Pour limiter la taille des tableaux que l’on va manipuler, on va se concentrer sur les vols ayant eu plus de 15 minutes de retard à l’arrivée :\n\nlate_flights &lt;- flights |&gt; \n  filter(arr_delay &gt; 15)\n\nlate_flights\n\n# A tibble: 77,630 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      533            529         4      850            830\n 2  2013     1     1      542            540         2      923            850\n 3  2013     1     1      555            600        -5      913            854\n 4  2013     1     1      559            600        -1      941            910\n 5  2013     1     1      602            605        -3      821            805\n 6  2013     1     1      608            600         8      807            735\n 7  2013     1     1      624            630        -6      909            840\n 8  2013     1     1      628            630        -2     1016            947\n 9  2013     1     1      635            635         0     1028            940\n10  2013     1     1      656            705        -9     1007            940\n# ℹ 77,620 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nNous obtenons un tableau de 77630, mais nous n’avons aucune information sur la date de construction (et donc sur l’âge) de ces avions. Nous avons toutefois l’information de l’immatriculation de chaque avion dans la colonne tailnum. Or, le tableau planes, qui contient des informations sur les caractéristiques des avions, indique, pour chacun d’entre eux, à la fois l’immatriculation et l’année de construction. Là encore, il va donc nous falloir fusionner les informations de deux tableaux : late_flights que nous venons de créer, et planes.\n\nplanes\n\n# A tibble: 3,322 × 9\n   tailnum  year type              manufacturer model engines seats speed engine\n   &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;             &lt;chr&gt;        &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; \n 1 N10156   2004 Fixed wing multi… EMBRAER      EMB-…       2    55    NA Turbo…\n 2 N102UW   1998 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n 3 N103US   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n 4 N104UW   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n 5 N10575   2002 Fixed wing multi… EMBRAER      EMB-…       2    55    NA Turbo…\n 6 N105UW   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n 7 N107US   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n 8 N108UW   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n 9 N109UW   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n10 N110UW   1999 Fixed wing multi… AIRBUS INDU… A320…       2   182    NA Turbo…\n# ℹ 3,312 more rows\n\n\nLe package dplyr fournit toute une gamme de fonctions permettant d’effectuer des associations de tableaux en fonction de critères spécifiés par l’utilisateur, et nous allons en utiliser deux.\n\n\n5.10.2 inner_join()\n\n\n\n\n\nFigure 5.16: Schéma de la fonction inner_join() tiré de la ‘cheatsheet’ de dplyr et tidyr.\n\n\n\n\nLa fonction inner_join() permet de relier deux tableaux en ne conservant que les lignes qui sont présentes à la fois dans l’un et dans l’autre. Il faut identifier, dans chacun des tableaux, une colonne contenant des données en commun, qui servira de guide pour mettre les lignes correctes les unes en face des autres. Ici, pour notre première questions, nous partons de notre tableau popular_dest, qui contient les codes des aéroports dans sa colonne dest, et nous faisons une “jointure interne” avec le tableau airports qui contient lui aussi une colonne contenant les codes des aéroports : la variable faa.\n\ninner_popular &lt;- popular_dest |&gt;\n  inner_join(airports, by = join_by(dest == faa))\ninner_popular\n\n# A tibble: 101 × 9\n   dest      n name                           lat    lon   alt    tz dst   tzone\n   &lt;chr&gt; &lt;int&gt; &lt;chr&gt;                        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 ORD   17283 Chicago Ohare Intl            42.0  -87.9   668    -6 A     Amer…\n 2 ATL   17215 Hartsfield Jackson Atlanta …  33.6  -84.4  1026    -5 A     Amer…\n 3 LAX   16174 Los Angeles Intl              33.9 -118.    126    -8 A     Amer…\n 4 BOS   15508 General Edward Lawrence Log…  42.4  -71.0    19    -5 A     Amer…\n 5 MCO   14082 Orlando Intl                  28.4  -81.3    96    -5 A     Amer…\n 6 CLT   14064 Charlotte Douglas Intl        35.2  -80.9   748    -5 A     Amer…\n 7 SFO   13331 San Francisco Intl            37.6 -122.     13    -8 A     Amer…\n 8 FLL   12055 Fort Lauderdale Hollywood I…  26.1  -80.2     9    -5 A     Amer…\n 9 MIA   11728 Miami Intl                    25.8  -80.3     8    -5 A     Amer…\n10 DCA    9705 Ronald Reagan Washington Na…  38.9  -77.0    15    -5 A     Amer…\n# ℹ 91 more rows\n\n\nLe nouvel objet inner_popular contient donc les données du tableau popular_dest auxquelles ont été ajoutées les colonnes correspondantes du tableau airports. C’est l’argument by = join_by() de la fonction inner_join() qui nous garantit que les bonnes lignes des deux tableaux sont mises face à face et que nous ne nous retrouvons pas avec des données totalement mélangées : pour chaque élément de la colonne dest du tableau popular_dest, l’élément correspondant de la colonne faa du tableau airports est identifié, et les variables de ces 2 lignes sont mises bout à bout dans un nouveau tableau. L’opération se répète pour tous les éléments de dest et de faa, et seules les lignes communes qui sont présente à la fois dans popular_dest et dans airports sont conservées.\nSi tout ce qui nous intéresse, c’est de connaître le nom complet des aéroports les plus populaires, on peut utiliser select() pour ne garder que les variables intéressantes :\n\ninner_popular &lt;- popular_dest |&gt;\n  inner_join(airports, by = join_by(dest == faa)) |&gt;\n  select(dest, name, n)\ninner_popular\n\n# A tibble: 101 × 3\n   dest  name                                   n\n   &lt;chr&gt; &lt;chr&gt;                              &lt;int&gt;\n 1 ORD   Chicago Ohare Intl                 17283\n 2 ATL   Hartsfield Jackson Atlanta Intl    17215\n 3 LAX   Los Angeles Intl                   16174\n 4 BOS   General Edward Lawrence Logan Intl 15508\n 5 MCO   Orlando Intl                       14082\n 6 CLT   Charlotte Douglas Intl             14064\n 7 SFO   San Francisco Intl                 13331\n 8 FLL   Fort Lauderdale Hollywood Intl     12055\n 9 MIA   Miami Intl                         11728\n10 DCA   Ronald Reagan Washington Natl       9705\n# ℹ 91 more rows\n\n\nOn peut noter plusieurs choses dans ce nouveau tableau :\n\nORD n’est pas l’aéroport d’Orlando mais l’aéroport international de Chicago Ohare. C’est donc la destination la plus fréquente au départ de New York.\nATL est bien l’aéroport d’Atlanta.\ninner_popular contient 101 lignes alors que notre tableau de départ en contenait 105.\n\n\nnrow(popular_dest)\n\n[1] 105\n\nnrow(inner_popular)\n\n[1] 101\n\n\nCertaines lignes ont donc été supprimées car le code aéroport dans popular_dest (notre tableau de départ) n’a pas été retrouvé dans la colonne faa du tableau airports. C’est le principe même de la jointure interne (voir Figure 5.16) : seules les lignes communes trouvées dans les 2 tableaux sont conservées. Pour connaitre quelles lignes ont éte éliminées, on peut utiliser anti_join() :\n\npopular_dest |&gt;\n  anti_join(airports, by = join_by(dest == faa))\n\n# A tibble: 4 × 2\n  dest      n\n  &lt;chr&gt; &lt;int&gt;\n1 SJU    5819\n2 BQN     896\n3 STT     522\n4 PSE     365\n\n\nSi l’on souhaite absolument conserver toutes les lignes du tableau de départ, il faut faire une jointure gauche, ou “left join” (voir Section 5.10.3 ci-dessous).\nPour notre deuxième question, on procède exactement de la même façon : on réalise une jointure de tableaux entre late-flights et planes :\n\ninner_late &lt;- late_flights |&gt; \n  inner_join(planes, by = join_by(tailnum))\n\ninner_late\n\n# A tibble: 66,434 × 27\n   year.x month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1   2013     1     1      533            529         4      850            830\n 2   2013     1     1      542            540         2      923            850\n 3   2013     1     1      555            600        -5      913            854\n 4   2013     1     1      624            630        -6      909            840\n 5   2013     1     1      628            630        -2     1016            947\n 6   2013     1     1      702            700         2     1058           1014\n 7   2013     1     1      709            700         9      852            832\n 8   2013     1     1      715            713         2      911            850\n 9   2013     1     1      732            645        47     1011            941\n10   2013     1     1      739            739         0     1104           1038\n# ℹ 66,424 more rows\n# ℹ 19 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, year.y &lt;int&gt;, type &lt;chr&gt;,\n#   manufacturer &lt;chr&gt;, model &lt;chr&gt;, engines &lt;int&gt;, seats &lt;int&gt;, speed &lt;int&gt;,\n#   engine &lt;chr&gt;\n\n\nIci, la variable qui nous permet d’associer les bonnes informations du tableau planes avec les bonnes lignes du tableau late_flights porte le même nom dans les deux tableaux : tailnum. C’est la raison pour laquelle nous spécifions simplement by = join_by(tailnum) (et non pas by = join_by(tailnum == tailnum)) dans la fonction inner_join(). Là encore, nous avons perdu des lignes en cours de route : de 77630 lignes dans le tableau late_flights, nous passons à 66434 dans le tableau inner_late après la jointure.\nPour simplifier la suite des analyses, nous pouvons sélectionner les seules variables qui nous intéressent, et calculer l’âge des avions :\n\ninner_late &lt;- late_flights |&gt; \n  inner_join(planes, by = join_by(tailnum)) |&gt; \n  select(tailnum, arr_delay, year.y) |&gt; \n  mutate(age = 2013 - year.y)\n\ninner_late\n\n# A tibble: 66,434 × 4\n   tailnum arr_delay year.y   age\n   &lt;chr&gt;       &lt;dbl&gt;  &lt;int&gt; &lt;dbl&gt;\n 1 N24211         20   1998    15\n 2 N619AA         33   1990    23\n 3 N516JB         19   2000    13\n 4 N11107         29   2002    11\n 5 N33289         29   2004     9\n 6 N779JB         44   2009     4\n 7 N26226         20   1998    15\n 8 N841UA         21   2001    12\n 9 N37456         30   2012     1\n10 N37408         26   2001    12\n# ℹ 66,424 more rows\n\n\nNotez bien que nos deux tableaux contenaient une variable nommée year, mais que l’information de ces 2 variables était différente :\n\ndans le tableau late_flights, year correspond à l’année où chaque vol a décollé de New York (donc 2013 pour tous les vols)\ndans le tableau planes, year correspond à l’année de fabrication de chaque avion.\n\nLors de la jointure, un suffixe est donc ajouté automatiquement aux deux variables year pour que ces 2 colonnes ne soient pas fusionnées et éviter les confusions : year.x (pour la colonne du premier tableau) et year.y (pour la colonne du deuxième tableau, celle qui nous intéresse.\nPour répondre à la question posée au départ, il ne reste plus qu’à visualiser les résultats sur un graphique par exemple :\n\ninner_late |&gt; \n  ggplot(aes(x = age, y = arr_delay)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth() +\n  labs(x = \"Âge des avions (en années)\", y = \"Retard à l'arrivée des vols (en minutes)\") +\n  theme_bw()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 1244 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1244 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nManifestement, l’âge des avions n’a pas grand chose à voir avec l’importance des retards constatés à l’arrivée des vols. En tous cas, s’il existe une relation, elle n’est certainement pas aussi simple qu’on aurait pu le penser.\n\n\n5.10.3 left_join()\n\n\n\n\n\nFigure 5.17: Schéma de la fonction left_join() tiré de la ‘cheatsheet’ de dplyr et tidyr.\n\n\n\n\nComme indiqué par la Figure 5.17 ci-dessus, une jointure gauche fonctionne comme inner_join(), mais elle permet de conserver toutes les lignes du tableau de gauche, et de leur faire correspondre les lignes du second tableau. Si aucune correspondance n’est trouvée dans le second tableau, des données manquantes sont ajoutées sous forme de NAs. Voyons ce que cela donne avec les mêmes exemples que précédemment :\n\nleft_popular &lt;- popular_dest |&gt;\n  left_join(airports, by = join_by(dest == faa)) |&gt;\n  select(dest, name, n)\nleft_popular\n\n# A tibble: 105 × 3\n   dest  name                                   n\n   &lt;chr&gt; &lt;chr&gt;                              &lt;int&gt;\n 1 ORD   Chicago Ohare Intl                 17283\n 2 ATL   Hartsfield Jackson Atlanta Intl    17215\n 3 LAX   Los Angeles Intl                   16174\n 4 BOS   General Edward Lawrence Logan Intl 15508\n 5 MCO   Orlando Intl                       14082\n 6 CLT   Charlotte Douglas Intl             14064\n 7 SFO   San Francisco Intl                 13331\n 8 FLL   Fort Lauderdale Hollywood Intl     12055\n 9 MIA   Miami Intl                         11728\n10 DCA   Ronald Reagan Washington Natl       9705\n# ℹ 95 more rows\n\n\nEn apparence, le tableau left_popular, créé avec left_join() semble identique au tableau inner_popular créé avec inner_join(). Pourtant, ce n’est pas le cas :\n\nidentical(inner_popular, left_popular)\n\n[1] FALSE\n\n\nEn l’occurrence, nous avons vu que inner_popular ne contenait pas autant de ligne que le tableau de départ popular_dest. Avec une jointure gauche, les lignes du tableau de départ sont toutes conservées. popular_dest et left_popular ont donc le même nombre de lignes.\n\nnrow(inner_popular)\n\n[1] 101\n\nnrow(left_popular)\n\n[1] 105\n\nnrow(popular_dest)\n\n[1] 105\n\n\nPour savoir quelles lignes de popular_dest manquent dans inner_dest (il devrait y en avoir 4), il suffit de filtrer les lignes de left_dest qui contiennent des données manquantes dans la colonne name :\n\nleft_popular |&gt;\n  filter(is.na(name))\n\n# A tibble: 4 × 3\n  dest  name      n\n  &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n1 SJU   &lt;NA&gt;   5819\n2 BQN   &lt;NA&gt;    896\n3 STT   &lt;NA&gt;    522\n4 PSE   &lt;NA&gt;    365\n\n\nOn trouve évidemment les mêmes résultats qu’avec la fonction anti_join() évoquée plus haut. Une rapide recherche sur internet vous apprendra que ces aéroports ne sont pas situés sur le sol américain. Trois d’entre eux sont situés à Puerto Rico (SJU, BQN et PSE) et le dernier (STT) est situé aux Îles Vierges.\nDe la même façon, répondre à la deuxième question avec une jointure gauche est presque identique à ce que nous avons vu avec inner_join() :\n\nlate_flights |&gt; \n  inner_join(planes, by = join_by(tailnum)) |&gt; \n  select(tailnum, arr_delay, year.y) |&gt; \n  mutate(age = 2013 - year.y) |&gt; \n  ggplot(aes(x = age, y = arr_delay)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth() +\n  labs(x = \"Âge des avions (en années)\", y = \"Retard à l'arrivée des vols (en minutes)\") +\n  theme_bw()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 1244 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1244 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nAttention toutefois : le fait que left_join() et inner_join() fournisse le même résultat n’est pas systématique. Ici, c’est bien le choix de la question d’intérêt qui fait que nous pouvons choisir l’une ou l’autre de ces fonctions de façon interchangeable. Mais ça ne sera pas toujours le cas. Quand on a besoin de conserver toutes les lignes du tableau de départ (ce qui est souvent lecas), il faudra utiliser left_join().\nIl y aurait bien plus à dire sur les jointures :\n\nQuelles sont les autres possibilités de jointures (right_join(), outer_join(), full_join(), semi_join(), cross_join(), nest_join(), etc…) ?\nQue se passe-t-il si les colonnes communes des 2 tableaux contiennent des éléments dupliqués ?\nEst-il possible de joindre des tableaux en associant plus d’une colonne de chaque tableau d’origine (la réponse est oui !) ?\n\nPour avoir la réponse à toutes ces questions, je vous conseille de lire ce chapitre de cet ouvrage très complet sur “la science des données” avec R et le tidyverse : R for Data Science. Les deux fonctions inner_join() et left_join() décrites ici devraient néanmoins vous permettre de couvrir l’essentiel de vos besoins. Et il je vous encourage vivement à explorer les fichiers d’aide des fonctions de jointures car il s’agit de fonctions tr`´s puissantes et dont les possibilités sont très larges.\n\n\n5.10.4 Accoler deux tableaux\nOutre l’association de tableaux en utilisant des jointures, il est parfois utile d’accoler 2 tableaux :\n\nSoit l’un au-dessous de l’autre, quand ils ont les mêmes nombres de colonnes, et si possible, les mêmes variables aux mêmes endroits. La fonction bind_rows() permet de faire cela.\nSoit l’un à côté de l’autre quand ils ont le même nombre de lignes, et si possible les mêmes observations en lignes. La fonction bind_cols() permet de faire cela.\n\nPrenons un exemple. Imaginons que nous ayons 2 tableaux contenant les mêmes variables. Le premier, nommé colorado, contient les informations des vols ayant décollé de New York en 2013 et ayant atterri à l’aéroport de Yempa Valley au Colorado (aéroport HDN).\n\ncolorado &lt;- flights |&gt;\n  filter(dest == \"HDN\")\n\ncolorado\n\n# A tibble: 15 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     5      829            830        -1     1047           1111\n 2  2013     1    12      827            830        -3     1112           1111\n 3  2013     1    19      843            830        13     1123           1111\n 4  2013     1    26      828            830        -2     1114           1111\n 5  2013    12    21      916            830        46     1149           1117\n 6  2013    12    28      913            829        44     1128           1116\n 7  2013     2     2      858            830        28     1124           1111\n 8  2013     2     9       NA            830        NA       NA           1111\n 9  2013     2    16      834            830         4     1114           1111\n10  2013     2    23      826            830        -4     1050           1111\n11  2013     3     2      854            830        24     1104           1111\n12  2013     3     9      838            830         8     1107           1111\n13  2013     3    16      845            830        15     1154           1111\n14  2013     3    23      835            830         5     1104           1111\n15  2013     3    30      825            830        -5     1045           1111\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nLe second est nommé indiana. Il contient les informations des vols ayant décollé de New York en 2013 et ayant atterri à l’aéroport de South Bend en Indiana (aéroport SBN).\n\nindiana &lt;- flights |&gt;\n  filter(dest == \"SBN\")\n\nindiana\n\n# A tibble: 10 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013    10    18     1820           1745        35     2030           2011\n 2  2013    11     1     2012           1905        67     2221           2131\n 3  2013    11    22     2013           1905        68     2224           2131\n 4  2013    12     1     1241           1215        26     1431           1431\n 5  2013     8    30     1909           1910        -1     2117           2136\n 6  2013     9     1      833            840        -7     1030           1040\n 7  2013     9     8      847            840         7     1043           1040\n 8  2013     9    20     1948           1950        -2     2207           2216\n 9  2013     9    22      837            840        -3     1025           1040\n10  2013     9    27     2011           1950        21     2209           2216\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nPuisque les variables de ces 2 tableaux sont les mêmes, nous pouvons “empiler” ces 2 tableaux pour n’en former qu’un seul :\n\nbind_rows(colorado, indiana)\n\n# A tibble: 25 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     5      829            830        -1     1047           1111\n 2  2013     1    12      827            830        -3     1112           1111\n 3  2013     1    19      843            830        13     1123           1111\n 4  2013     1    26      828            830        -2     1114           1111\n 5  2013    12    21      916            830        46     1149           1117\n 6  2013    12    28      913            829        44     1128           1116\n 7  2013     2     2      858            830        28     1124           1111\n 8  2013     2     9       NA            830        NA       NA           1111\n 9  2013     2    16      834            830         4     1114           1111\n10  2013     2    23      826            830        -4     1050           1111\n# ℹ 15 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nVous noterez que le nombre de lignes du nouveau tableau est la somme des nombres de lignes des 2 tableaux de départ. Bien sûr, cette opération n’est utile que si les tableaux nous sont fournis séparément. Ici, il aurait été bien plus rapide d’obtenir le même résultat en tapant :\n\nflights |&gt;\n  filter(dest %in% c(\"HDN\", \"SBN\"))\n\n# A tibble: 25 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     5      829            830        -1     1047           1111\n 2  2013     1    12      827            830        -3     1112           1111\n 3  2013     1    19      843            830        13     1123           1111\n 4  2013     1    26      828            830        -2     1114           1111\n 5  2013    10    18     1820           1745        35     2030           2011\n 6  2013    11     1     2012           1905        67     2221           2131\n 7  2013    11    22     2013           1905        68     2224           2131\n 8  2013    12     1     1241           1215        26     1431           1431\n 9  2013    12    21      916            830        46     1149           1117\n10  2013    12    28      913            829        44     1128           1116\n# ℹ 15 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nLe fonctionnement de bind_cols() est le même :\n\na &lt;- tibble(x = 1:3,\n            y = c(2, 4, 6),\n            z = c(TRUE, FALSE, FALSE))\n\nb &lt;- tibble(r = 10:8,\n            s = rnorm(3))\n\na\n\n# A tibble: 3 × 3\n      x     y z    \n  &lt;int&gt; &lt;dbl&gt; &lt;lgl&gt;\n1     1     2 TRUE \n2     2     4 FALSE\n3     3     6 FALSE\n\nb\n\n# A tibble: 3 × 2\n      r      s\n  &lt;int&gt;  &lt;dbl&gt;\n1    10  0.716\n2     9 -1.95 \n3     8  1.33 \n\nbind_cols(a,b)\n\n# A tibble: 3 × 5\n      x     y z         r      s\n  &lt;int&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;int&gt;  &lt;dbl&gt;\n1     1     2 TRUE     10  0.716\n2     2     4 FALSE     9 -1.95 \n3     3     6 FALSE     8  1.33 \n\n\nIci, puisque a et b ont le même nombre de lignes, il est possible de les accoler. Cela n’a de sens que si les lignes des 2 tableaux correspondent aux mêmes observations."
  },
  {
    "objectID": "05-DataWrangling.html#sec-exo-13",
    "href": "05-DataWrangling.html#sec-exo-13",
    "title": "5  Manipuler des tableaux avec dplyr",
    "section": "5.11 Exercices",
    "text": "5.11 Exercices\n\nCréez un tableau delayed indiquant, pour chaque compagnie aérienne et chaque mois de l’année, le nombre de vols ayant eu un retard supérieur à 30 minutes à l’arrivée à destination. Ce tableau devrait contenir uniquement 3 colonnes :\n\n\ncarrier : la compagnie aérienne.\nmonth : le mois de l’année 2013.\nn_delayed : le nombre de vols ayant plus de 30 minutes de retard.\n\n\nCréez un tableau total indiquant le nombre total de vols affrétés (et non annulés) par chaque compagnie aérienne et chaque mois de l’année. Ce tableau devrait contenir seulement 3 colonnes :\n\n\ncarrier : la compagnie aérienne.\nmonth : le mois de l’année 2013.\nn_total : le nombre total de vols arrivés à destination.\n\n\nFusionnez ces 2 tableaux en réalisant la jointure appropriée. Le tableau final, que vous nommerez carrier_stats devrait contenir 185 lignes. Si certaines colonnes contiennent des données manquantes, remplacez-les par des 0 à l’aide des fonctions mutate() et replace_na().\nAjoutez à votre tableau carrier_stats une variable rate qui contient la proportion de vols arrivés à destination avec plus de 30 minutes de retard, pour chaque compagnie aérienne et chaque mois de l’année.\nAjoutez à votre tableau carrier_stats le nom complet des compagnies aériennes en réalisant la jointure appropriée avec le tableau airlines.\nFaites un graphique synthétique présentant ces résultats de la façon la plus claire possible\nQuelle compagnie aérienne semble se comporter très différemment des autres ? À quoi pouvez-vous attribuer ce comportement atypique ?\nPour les compagnies affrétant un grand nombre de vols chaque année (e.g. UA, B6 et EV), quelles sont les périodes où les plus fortes proportions de vols en retard sont observées ? Et les plus faibles ? Quelle(s) hypothèse(s) pouvez-vous formuler pour expliquer ces observations ?\nFaites un tableau synthétique présentant ces résultats de la façon la plus compacte et claire que possible, afin par exemple de les intégrer à un rapport."
  },
  {
    "objectID": "06-EDA.html#pré-requis",
    "href": "06-EDA.html#pré-requis",
    "title": "6  Exploration statistique des données",
    "section": "6.1 Pré-requis",
    "text": "6.1 Pré-requis\nLa première étape de toute analyse de données est l’exploration. Avant de se lancer dans des tests statistiques et des procédures complexes, et à supposer que les données dont vous disposez sont déjà dans un format approprié, il est toujours très utile :\n\nd’explorer visuellement les données dont on dispose en faisant des graphiques nombreux et variés, afin de comprendre, notamment quelle est la distribution des variables numériques, quelles sont les catégories les plus représentées pour les variables qualitatives (facteurs), quelles sont les relations les plus marquantes entre variables numériques et/ou catégorielles, etc. Vous avez appris au Chapitre 3 comment produire toutes sortes de graphiques avec le package ggplot2. Il va maintenant falloir vous poser la question du choix des graphiques à produire du point de vue de l’exploration statistique de données inconnues.\nd’explorer les données en calculant des indices de statistiques descriptives. Ces indices relèvent en général de 2 catégories : les indices de position (e.g. moyennes, médianes, quartiles…) et les indices de dispersion (e.g. variance, écart-type, intervalle inter-quartiles…). Nous avons déjà vu comment utiliser la fonction summarise() et son argument .by pour calculer des moyennes ou des effectifs pour plusieurs sous-groupes de nos jeux de données (Section 5.8). Dans ce chapitre, nous irons plus loin, et nous découvrirons d’une part comment calculer d’autres indices statistiques pertinents, et comment utiliser d’autres fonctions encore plus utiles que summarise().\n\nNous verrons ensuite dans le Chapitre 7 comment calculer des indices d’incertitude (Section 7.4 et Section 7.5). Attention, il ne faudra pas confondre indices de dispersion et indices d’incertitude. Et enfin, avant de passer aux tests statistiques, nous verrons comment visualiser dispersion et incertitude au Chapitre 8.\nAfin d’explorer ces questions, nous aurons besoin des packages suivants :\n\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(palmerpenguins)\nlibrary(nycflights13)\n\nComme vous le savez maintenant, les packages du tidyverse (Wickham 2023) permettent de manipuler facilement des tableaux de données et de réaliser des graphiques. Charger le tidyverse permet d’accéder, entre autres, aux packages readr (Wickham, Hester, et Bryan 2023), pour importer facilement des fichiers .csv au format tibble, tidyr (Wickham, Vaughan, et Girlich 2023) et dplyr (Wickham et al. 2023) pour manipuler des tableaux de données ou encore ggplot2 (Wickham et al. 2024) pour produire des graphiques. Le package skimr (Waring et al. 2022) permet de calculer des résumés de données très informatifs. Les packages palmerpenguins (Horst, Hill, et Gorman 2022) et nycflights13 (Wickham 2021) fournissent des jeux de données qui seront faciles à manipuler pour illustrer ce chapitre (et les suivants).\n\n\n\n\n\n\nImportant\n\n\n\nSi vous avez installé le tidyverse ou dplyr avant le printemps 2023, ré-installez dplyr avec install.packages(\"dplyr\"). Ce package a en effet été mis à jour courant 2023, et nous aurons besoin de sa version v1.1.0 ou d’une version plus récente pour utiliser certaines fonctions. Chargez-le ensuite en mémoire avec library(dplyr).\n\n\n\n\n\n\n\n\nAttention\n\n\n\nPensez à installer tous les packages listés ci-dessus avant de les charger en mémoire si vous ne l’avez pas déjà fait. Si vous ne savez plus comment faire, consultez d’urgence la Section 1.4\n\n\nPour travailler dans de bonnes conditions, et puisque nous abordons maintenant les statistiques à proprement parler, je vous conseille de créez un nouveau script dans le même dossier que votre Rproject. Là encore, si vous ne savez plus comment faire consultez la Section 1.3."
  },
  {
    "objectID": "06-EDA.html#sec-reframe",
    "href": "06-EDA.html#sec-reframe",
    "title": "6  Exploration statistique des données",
    "section": "6.2 Créer des résumés avec la fonction reframe()",
    "text": "6.2 Créer des résumés avec la fonction reframe()\nDans la Section 5.8, nous avons vu comment utiliser la fonction summarise() et éventuellement son argument .by pour calculer des statistiques descriptives variées. N’hésitez pas à relire cette section si vous n’êtes pas sûr d’avoir tout compris ou tout retenu. Les calculs que nous pouvons faire grâce à la fonction summarise() impliquent des fonctions statistiques qui ne renvoient qu’une valeur à la fois lorsqu’on leur fournit une série de valeurs. Par exemple, si on dispose d’un vecteur numérique (les entiers compris entre 1 et 100 pour l’exemple) :\n\n1:100\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100\n\n\nla fonction mean() ne renvoie qu’une valeur, la moyenne des 100 nombres contenus dans le vecteur :\n\nmean(1:100)\n\n[1] 50.5\n\n\nDe même pour les fonctions sd(), ou median(), ou toutes les autres fonctions listées à la fin de la Section 5.8.4 :\n\nsd(1:100)\n\n[1] 29.01149\n\nmedian(1:100)\n\n[1] 50.5\n\n\nIl existe toutefois des fonctions qui renvoient plus d’une valeur à la fois. Par exemple, la fonction quantile() (que nous avons utilisée dans un autre contexte à la Section 5.9.1), renvoie par défaut 5 éléments :\n\nla valeur minimale contenue dans le vecteur (ou quantile 0%) : c’est la valeur la plus faible contenue dans la série de données\nle premier quartile du vecteur (Q1 ou quantile 25%) est la valeur coupant l’échantillon en deux : 25% des observations du vecteur y sont inférieures et 75% y sont supérieures\nla médiane du vecteur (Q2 ou quantile 50%) est la valeur coupant l’échantillon en deux : 50% des observations du vecteur sont inférieures à cette valeur et 50% y sont supérieures\nle troisième quartile du vecteur (Q3 ou quantile 75%) est la valeur coupant l’échantillon en deux : 75% des observations du vecteur y sont inférieures et 25% y sont supérieures\nla valeur maximale contenue dans le vecteur (ou quantile 100%) : c’est la valeur la plus élevée contenue dans la série de données.\n\nPar exemple, toujours avec le vecteur des entiers contenus entre 1 et 100 :\n\nquantile(1:100)\n\n    0%    25%    50%    75%   100% \n  1.00  25.75  50.50  75.25 100.00 \n\n\nL’objet obtenu est un vecteur dont chaque élément porte un nom. Pour transformer cet objet en tibble, on utilise la fonction enframe() :\n\nenframe(quantile(1:100))\n\n# A tibble: 5 × 2\n  name  value\n  &lt;chr&gt; &lt;dbl&gt;\n1 0%      1  \n2 25%    25.8\n3 50%    50.5\n4 75%    75.2\n5 100%  100  \n\n\nIl peut être très utile de calculer ces différentes valeurs pour plusieurs variables à la fois, ou pour plusieurs sous-groupes d’un jeu de données. Le problème est que nous ne pouvons pas utiliser summarise() car la fonction quantile() ne renvoie pas qu’une unique valeur. Par exemple, pour calculer les quantiles des longueurs de becs pour chaque espèce de manchots, on pourrait être tenté de taper ceci :\n\npenguins |&gt; \n  summarise(Indices = quantile(bill_length_mm, na.rm = TRUE), \n            .by = species)\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n# A tibble: 15 × 2\n   species   Indices\n   &lt;fct&gt;       &lt;dbl&gt;\n 1 Adelie       32.1\n 2 Adelie       36.8\n 3 Adelie       38.8\n 4 Adelie       40.8\n 5 Adelie       46  \n 6 Gentoo       40.9\n 7 Gentoo       45.3\n 8 Gentoo       47.3\n 9 Gentoo       49.6\n10 Gentoo       59.6\n11 Chinstrap    40.9\n12 Chinstrap    46.3\n13 Chinstrap    49.6\n14 Chinstrap    51.1\n15 Chinstrap    58  \n\n\nC’est dans ces situations que la fonction reframe() est utile. Elle joue le même rôle que summarise(), mais dans les situation où les fonctions statistiques renvoient plus d’une valeur à la fois :\n\npenguins |&gt; \n  reframe(Indices = quantile(bill_length_mm, na.rm = TRUE), \n          .by = species)\n\n# A tibble: 15 × 2\n   species   Indices\n   &lt;fct&gt;       &lt;dbl&gt;\n 1 Adelie       32.1\n 2 Adelie       36.8\n 3 Adelie       38.8\n 4 Adelie       40.8\n 5 Adelie       46  \n 6 Gentoo       40.9\n 7 Gentoo       45.3\n 8 Gentoo       47.3\n 9 Gentoo       49.6\n10 Gentoo       59.6\n11 Chinstrap    40.9\n12 Chinstrap    46.3\n13 Chinstrap    49.6\n14 Chinstrap    51.1\n15 Chinstrap    58  \n\n\nAu contraire de summarise(), reframe() ne renvoie pas de message d’avertissement dans cette situation. Dans cet exemple, on ne sait malheureusement pas à quoi correspondent les chiffres renvoyés puisque l’information des quartiles a disparu (quelles valeurs correspondent aux médianes ou aux premiers quartiles par exemple). Pour y remédier, on doit transformer le vecteur renvoyé par quantile() en tibble. Nous avons déjà vu comment le faire grâce à la fonction enframe(). Par ailleurs, puisque la fonction va maintenant renvoyer un tableau, on n’a pas besoin de lui fournir de nom de colonnes (je retire donc Indices = de mon code) :\n\npenguins |&gt; \n  reframe(enframe(quantile(bill_length_mm, na.rm = TRUE)), \n          .by = species)\n\n# A tibble: 15 × 3\n   species   name  value\n   &lt;fct&gt;     &lt;chr&gt; &lt;dbl&gt;\n 1 Adelie    0%     32.1\n 2 Adelie    25%    36.8\n 3 Adelie    50%    38.8\n 4 Adelie    75%    40.8\n 5 Adelie    100%   46  \n 6 Gentoo    0%     40.9\n 7 Gentoo    25%    45.3\n 8 Gentoo    50%    47.3\n 9 Gentoo    75%    49.6\n10 Gentoo    100%   59.6\n11 Chinstrap 0%     40.9\n12 Chinstrap 25%    46.3\n13 Chinstrap 50%    49.6\n14 Chinstrap 75%    51.1\n15 Chinstrap 100%   58  \n\n\nEnfin, comme décrit à la Section 4.3.2, il est possible de modifier la forme de ce tableau avec pivot_wider(), pour le lire plus facilement et éventuellement l’intégrer dans un rapport ou compte-rendu :\n\npenguins |&gt;\n  reframe(enframe(quantile(bill_length_mm, na.rm = TRUE)), \n          .by = species) |&gt; \n  pivot_wider(names_from = species,\n              values_from = value)\n\n# A tibble: 5 × 4\n  name  Adelie Gentoo Chinstrap\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 0%      32.1   40.9      40.9\n2 25%     36.8   45.3      46.3\n3 50%     38.8   47.3      49.6\n4 75%     40.8   49.6      51.1\n5 100%    46     59.6      58  \n\n\nCes statistiques nous permettent de constater que les manchots de l’espèce Adélie semblent avoir des becs plus courts que les 2 autres espèces (les 5 quantiles le confirment). Les manchots Gentoo et Chinstrap ont en revanche des becs de longueur à peu près similaires, bien que ceux des Chinstrap soient peut-être très légèrement plus longs (Q1, médiane et Q3 supérieurs à ceux des Gentoo). On peut vérifier tout ça graphiquement avec des boîtes à moustaches, puisque les 5 valeurs de quantiles sont justement celles qui sont utilisées pour tracer les boîtes à moustaches :\n\npenguins |&gt; \n  ggplot(aes(x = species, y = bill_length_mm)) +\n  geom_boxplot() +\n  labs(x = \"Espèce\", y = \"Longueur du bec (mm)\") +\n  theme_bw()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nOu avec un graphique de densité :\n\npenguins |&gt; \n  ggplot(aes(x = bill_length_mm, fill = species)) +\n  geom_density(alpha = 0.5, show.legend = FALSE) +\n  geom_rug() +\n  labs(x = \"Longueur du bec (mm)\", y = \"Densité\") +\n  facet_wrap(~species, ncol = 1) +\n  scale_fill_brewer(palette = \"Accent\") +\n  theme_bw()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\nÀ ce stade, vous devriez être capables de créer (et d’interpréter !) ce type de graphiques. Si ce n’est pas le cas, relisez d’urgence les sections 3.5.1.5 et 3.9.3.\n\n\n\n\n\n\nÀ retenir\n\n\n\n\nla fonction summarise() s’utilise avec des fonctions statistiques qui ne renvoient qu’une valeur (par exemple mean(), median(), sd(), var()…)\nla fonction reframe() s’utilise avec des fonctions statistiques qui renvoient plusieurs valeurs (par exemple quantile(), range()…)\n\n\n\nLes fonctions summarise() et reframe(), avec leur argument .by() (ou la fonction group_by()) permettent donc de calculer n’importe quel indice de statistique descriptive sur un tableau de données entier ou sur des modalités ou combinaisons de modalités de facteurs. Il existe par ailleurs de nombreuses fonctions, disponibles de base dans R ou dans certains packages spécifiques, qui permettent de fournir des résumés plus ou moins automatiques de tout ou partie des variables d’un jeu de données. Nous allons maintenant en décrire 2, mais il en existe beaucoup d’autres : à vous d’explorer les possibilités et d’utiliser les fonctions qui vous paraissent les plus pertinentes, les plus simples à utiliser, les plus visuelles ou les plus complètes."
  },
  {
    "objectID": "06-EDA.html#créer-des-résumés-de-données-avec-la-fonction-summary",
    "href": "06-EDA.html#créer-des-résumés-de-données-avec-la-fonction-summary",
    "title": "6  Exploration statistique des données",
    "section": "6.3 Créer des résumés de données avec la fonction summary()",
    "text": "6.3 Créer des résumés de données avec la fonction summary()\nLa fonction summary() (à ne pas confondre avec summarise()) permet d’obtenir des résumés de données pour tous types d’objets dans R. Selon la classe des objets que l’on transmets à summary(), la nature des résultats obtenus changera. Nous verrons ainsi aux chapitres 12 et 14 que cette fonction peut être utilisée pour examiner les résultats d’analyses de variances ou de modèles de régressions linéaires. Pour l’instant, nous nous intéressons à 3 situations :\n\nce que renvoie la fonction quand on lui fournit un vecteur\nce que renvoie la fonction quand on lui fournit un facteur\nce que renvoie la fonction quand on lui fournit un tableau\n\n\n6.3.1 Variable continue : vecteur numérique\nCommençons par fournir un vecteur numérique à la fonction summary(). Nous allons pour cela extraire les données de masses corporelles des manchots du tableau penguins :\n\npenguins$body_mass_g\n\n  [1] 3750 3800 3250   NA 3450 3650 3625 4675 3475 4250 3300 3700 3200 3800 4400\n [16] 3700 3450 4500 3325 4200 3400 3600 3800 3950 3800 3800 3550 3200 3150 3950\n [31] 3250 3900 3300 3900 3325 4150 3950 3550 3300 4650 3150 3900 3100 4400 3000\n [46] 4600 3425 2975 3450 4150 3500 4300 3450 4050 2900 3700 3550 3800 2850 3750\n [61] 3150 4400 3600 4050 2850 3950 3350 4100 3050 4450 3600 3900 3550 4150 3700\n [76] 4250 3700 3900 3550 4000 3200 4700 3800 4200 3350 3550 3800 3500 3950 3600\n [91] 3550 4300 3400 4450 3300 4300 3700 4350 2900 4100 3725 4725 3075 4250 2925\n[106] 3550 3750 3900 3175 4775 3825 4600 3200 4275 3900 4075 2900 3775 3350 3325\n[121] 3150 3500 3450 3875 3050 4000 3275 4300 3050 4000 3325 3500 3500 4475 3425\n[136] 3900 3175 3975 3400 4250 3400 3475 3050 3725 3000 3650 4250 3475 3450 3750\n[151] 3700 4000 4500 5700 4450 5700 5400 4550 4800 5200 4400 5150 4650 5550 4650\n[166] 5850 4200 5850 4150 6300 4800 5350 5700 5000 4400 5050 5000 5100 4100 5650\n[181] 4600 5550 5250 4700 5050 6050 5150 5400 4950 5250 4350 5350 3950 5700 4300\n[196] 4750 5550 4900 4200 5400 5100 5300 4850 5300 4400 5000 4900 5050 4300 5000\n[211] 4450 5550 4200 5300 4400 5650 4700 5700 4650 5800 4700 5550 4750 5000 5100\n[226] 5200 4700 5800 4600 6000 4750 5950 4625 5450 4725 5350 4750 5600 4600 5300\n[241] 4875 5550 4950 5400 4750 5650 4850 5200 4925 4875 4625 5250 4850 5600 4975\n[256] 5500 4725 5500 4700 5500 4575 5500 5000 5950 4650 5500 4375 5850 4875 6000\n[271] 4925   NA 4850 5750 5200 5400 3500 3900 3650 3525 3725 3950 3250 3750 4150\n[286] 3700 3800 3775 3700 4050 3575 4050 3300 3700 3450 4400 3600 3400 2900 3800\n[301] 3300 4150 3400 3800 3700 4550 3200 4300 3350 4100 3600 3900 3850 4800 2700\n[316] 4500 3950 3650 3550 3500 3675 4450 3400 4300 3250 3675 3325 3950 3600 4050\n[331] 3350 3450 3250 4050 3800 3525 3950 3650 3650 4000 3400 3775 4100 3775\n\n\nNous avons donc 344 valeurs de masses en grammes qui correspondent aux 344 manchots du jeu de données. La fonction summary() renvoie le résumé suivant lorsqu’on lui fournit ces valeurs :\n\nsummary(penguins$body_mass_g)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   2700    3550    4050    4202    4750    6300       2 \n\n\nNous obtenons ici 7 valeurs, qui correspondent aux cinq valeurs renvoyées par la fonction quantile() (voir Section 6.2), ainsi que la moyenne et le nombre de valeurs manquantes. Dans l’ordre, on a donc :\n\nla valeur minimale observée dans le vecteur. Ici, le manchot le plus léger de l’échantillon pèse donc 2700 grammes.\nla valeur du premier quartile du vecteur. Ici, 25% des manchots de l’échantillon (soit 86 individus) ont une masse inférieure à 3550 grammes, et 75% des individus de l’échantillon (soit 258 individus) ont une masse supérieure à 3550 grammes.\nla valeur de médiane du vecteur. La médiane est le deuxième quartile. Ici, 50% des manchots de l’échantillon (soit 172 individus) ont une masse inférieure à 4050 grammes, et 50% des individus de l’échantillon (soit 172 individus) ont une masse supérieure à 4050 grammes.\nla moyenne du vecteur. Ici, les manchots des 3 espèces du jeu de données ont en moyenne une masse 4202 grammes.\nla valeur du troisième quartile du vecteur. Ici, 75% des manchots de l’échantillon (soit 258 individus) ont une masse inférieure à 4700 grammes, et 25% des individus de l’échantillon (soit 86 individus) ont une masse supérieure à 4750 grammes.\nla valeur maximale observée dans le vecteur. Ici, le manchot le plus lourd de l’échantillon pèse donc 6300 grammes.\nle nombre de données manquantes. Ici, 2 manchots n’ont pas été pesés et présentent donc la mention NA (comme Not Available) pour la variable body_mass_g.\n\nUtiliser la fonction quantile() fournit donc presque les mêmes informations :\n\nquantile(penguins$body_mass_g, na.rm = TRUE)\n\n  0%  25%  50%  75% 100% \n2700 3550 4050 4750 6300 \n\n\nAttention, contrairement à ce que nous avons vu plus haut, la fonction summary() ne possède pas d’argument .by() et il n’est pas possible de l’utiliser avec la fonction group_by(). Par conséquent, il n’est pas possible de se servir de cette fonction pour avoir des valeurs pour chaque modalités d’un facteur (pour chaque espèce par exemple).\nLes différents indices statistiques fournis nous renseignent à la fois sur la position de la distribution et sur la dispersion des données.\n\nLa position correspond à la tendance centrale et indique quelles sont les valeurs qui caractérisent le plus grand nombre d’individus. La moyenne et la médiane sont les deux indices de position les plus fréquemment utilisés. Lorsqu’une variable a une distribution parfaitement symétrique, la moyenne et la médiane sont strictement égales. Mais lorsqu’une distribution est asymétrique, la moyenne et la médiane diffèrent. En particulier, la moyenne est beaucoup plus sensible aux valeurs extrêmes que la médiane. Cela signifie que quand une distribution est très asymétrique, la médiane est souvent une meilleure indication des valeurs les plus fréquemment observées.\n\n\n\n\n\n\nFigure 6.1: Distribution des masses corporelles des manchots\n\n\n\n\nL’histogramme de la Figure 6.1 montre la distribution de la taille des manchots (toutes espèces confondues). Cette distribution présente une asymétrie à droite. Cela signifie que la distribution n’est pas symétrique et que la “queue de distribution” est plus longue à droite qu’à gauche. La plupart des individus ont une masse comprise entre 3500 et 3700 grammes, au niveau du pic principal du graphique. La médiane, en orange et qui vaut 4050 grammes est plus proche du pic que la moyenne, en rouge, qui vaut 4202 grammes. Ici, la différence entre moyenne et médiane n’est pas énorme, mais elle peut le devenir si la distribution est vraiment très asymétrique, par exemple, si quelques individus seulement avaient une masse supérieure à 7000 grammes, la moyenne serait tirée vers la droite du graphique alors que la médiane ne serait presque pas affectée. La moyenne représenterait alors encore moins fidèlement la tendance centrale.\nSi l’on revient à la fonction summary(), observer des valeurs proches pour la moyenne et la médiane nous indique donc le degré de symétrie de la distribution.\n\nLa dispersion des données nous renseigne sur la dispersion des points autour des indices de position. Les quartiles et les valeurs minimales et maximales renvoyées par la fonction summary() nous renseignent sur l’étalement des points. Les valeurs situées entre le premier et le troisième quartile correspondent aux 50% des valeurs de l’échantillon les plus centrales. Plus l’étendue entre ces quartiles (notée IQR pour “intervalle interquartile”) sera grande, plus la dispersion sera importante. D’ailleurs, lorsque la dispersion est très importante, les moyennes et médianes ne renseignent que très moyennement sur la tendance centrale. Les indices de position sont surtout pertinents lorsque la dispersion des points autour de cette tendance centrale n’est pas trop large. Par exemple, si la distribution des données ressemblait à ceci (Figure 6.2), la moyenne et la médiane seraient fort peu utiles car très éloignées de la plupart des observations :\n\n\n\n\n\n\nFigure 6.2: Distribution des masses corporelles (données fictives)\n\n\n\n\nOn comprend donc l’importance de considérer les indices de dispersion en plus des indices de position pour caractériser et comprendre une série de données numériques. L’intervalle interquartile est toujours utile pour connaître l’étendue des données qui correspondent aux 50% des observations les plus centrales. Les autres indices de dispersion très fréquemment utilisés, mais qui ne sont pas proposés par défaut par la fonction summary(), sont la variance et l’écart-type. Il est possible de calculer tous les indices renvoyés par la fonction summary() et ceux qui nous manquent grâce à la fonction summarise() :\n\npenguins |&gt; \n  summarise(min = min(body_mass_g, na.rm = TRUE),\n            Q1 = quantile(body_mass_g, 0.25, na.rm = TRUE),\n            med = median(body_mass_g, na.rm = TRUE),\n            moy = mean(body_mass_g, na.rm = TRUE),\n            Q3 = quantile(body_mass_g, 0.75, na.rm = TRUE),\n            max = max(body_mass_g, na.rm = TRUE),\n            var = var(body_mass_g, na.rm = TRUE),\n            et = sd(body_mass_g, na.rm = TRUE))\n\n# A tibble: 1 × 8\n    min    Q1   med   moy    Q3   max     var    et\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1  2700  3550  4050 4202.  4750  6300 643131.  802.\n\n\nVous notez que le code est beaucoup plus long, et qu’utiliser summary() peut donc faire gagner beaucoup de temps, même si cette fonction ne nous fournit ni la variance ni l’écart-type. Mais comme souvent dans R, il est possible de calculer à la main toutes ces valeurs si besoin. N’oubliez pas non plus qu’avec summarise(), on pourrait utiliser l’argument .by ou la fonction group_by() pour calculer très rapidement ces indices pour chaque espèce de manchot, ou pour chaque espèce et chaque sexe. Comme indiqué plus haut, les fonctions suivantes peuvent être utilisées :\n\nmean() permet de calculer la moyenne.\nmedian() permet de calculer la médiane.\nmin() et max() permettent de calculer les valeurs minimales et maximales respectivement.\nquantile() permet de calculer les quartiles. Vous notez que contrairement aux exemples de la partie précédente, on utilise ici la fonction quantile() en précisant une valeur supplémentaire pour n’obtenir qu’une valeur à la fois : 0.25 pour le premier quartile, et 0.75 pour le troisième.\nsd() permet de calculer l’écart-type.\nvar() permet de calculer la variance.\n\nPour toutes ces fonctions l’argument na.rm = TRUE permet d’obtenir les résultats même si certaines valeurs sont manquantes. Enfin, la fonction IQR() permet de calculer l’intervalle inter-quartiles :\n\nIQR(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 1200\n\n\nIci, les 50% des valeurs les plus centrales de l’échantillon sont situées dans un intervalle de 1200 grammes autour de la médiane.\n\n\n6.3.2 Variable quantitative : facteur\nSi l’on fournit une variable catégorielle à summary(), le résultat obtenu sera naturellement différent : calculer des moyennes, médianes ou quartiles n’aurait en effet pas de sens lorsque la variable fournie ne contient que des catégories :\n\nsummary(penguins$species)\n\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\n\nPour les facteurs, summary() compte simplement le nombre d’observations pour chaque modalité. Ici, la variable species est un facteur qui compte 3 modalités. La fonction summary() nous indique donc le nombre d’individus pour chaque modalité : notre jeu de données se compose de 152 individus de l’espèce Adélie, 68 individus de l’espèce Chinstrap, et 124 individus de l’espèce Gentoo.\nComme pour les vecteurs numériques, si le facteur présente des données manquantes, la fonction summary() compte également leur nombre :\n\nsummary(penguins$sex)\n\nfemale   male   NA's \n   165    168     11 \n\n\nPour les facteurs, la fonction summary() est donc tout à fait équivalente à la fonction count() :\n\npenguins |&gt; \n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nL’avantage de la fonction count() est qu’il est possible d’utiliser plusieurs facteurs pour compter le nombre d’observations de toutes les combinaisons de modalités (par exemple, combien d’individus de chaque sexe pour chaque espèce), ce qui n’est pas possible avec la fonction summary().\n\n\n6.3.3 Les tableaux : data.frame ou tibble\nL’avantage de la fonction summary() par rapport à la fonction count() apparaît lorsque l’on souhaite obtenir des informations sur toutes les variables d’un tableau à la fois :\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\nIci, on obtient un résumé pour chaque colonne du tableau. Les colonnes numériques sont traitées comme les vecteurs numériques (on obtient alors les minimas et maximas, les quartiles, les moyennes et médianes) et les colonnes contenant des variables catégorielles sont traitées comme des facteurs (et on obtient alors le nombre d’observations pour chaque modalité).\nOn constate au passage que la variable year est considérée ici comme une variable numérique, alors qu’elle devrait plutôt être considérée comme un facteur, ce qui nous permettrait de savoir combien d’individus ont été échantillonnés chaque année :\n\npenguins |&gt; \n  mutate(year = factor(year)) |&gt; \n  summary()\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex        year    \n Min.   :172.0     Min.   :2700   female:165   2007:110  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   2008:114  \n Median :197.0     Median :4050   NA's  : 11   2009:120  \n Mean   :200.9     Mean   :4202                          \n 3rd Qu.:213.0     3rd Qu.:4750                          \n Max.   :231.0     Max.   :6300                          \n NA's   :2         NA's   :2                             \n\n\nAu final, la fonction summary() est très utile dans certaines situations, notamment pour avoir rapidement accès à des statistiques descriptives simples sur toutes les colonnes d’un tableau. Elle reste cependant limitée car d’une part, elle ne fournit pas les variances ou les écarts-types pour les variables numériques, et il est impossible d’avoir des résumés plus fins, pour chaque modalité d’un facteur par exemple. Ici, il serait en effet intéressant d’avoir des informations synthétiques concernant les mesures biométriques des manchots, espèce par espèce, plutôt que toutes espèces confondues. C’est là que la fonction skim() intervient."
  },
  {
    "objectID": "06-EDA.html#sec-skim",
    "href": "06-EDA.html#sec-skim",
    "title": "6  Exploration statistique des données",
    "section": "6.4 Créer des résumés de données avec la fonction skim()",
    "text": "6.4 Créer des résumés de données avec la fonction skim()\nLa fonction skim() fait partie du package skimr. Avant de pouvoir l’utiliser, pensez donc à l’installer et à le charger en mémoire si ce n’est pas déjà fait. Comme pour la fonction summary(), on peut utiliser la fonction skim() sur plusieurs types d’objets. Nous nous contenterons d’examiner ici le cas le plus fréquent : celui des tableaux, groupés avec group_by() ou non.\n\n6.4.1 Tableau non groupé\nCommençons par examiner le résultat avec le tableau penguins non groupé :\n\nskim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\n\nLes résultats obtenus grâce à cette fonction sont nombreux. La première section nous donne des informations sur le tableau :\n\nson nom, son nombre de lignes et de colonnes\nla nature des variables qu’il contient (ici 3 facteurs et 5 variables numériques)\nla présence de variables utilisées pour faire des regroupements (il n’y en a pas encore à ce stade)\n\nEnsuite, un bloc apporte des informations sur chaque facteur présent dans le tableau :\n\nle nom de la variable catégorielle (skim_variable)\nle nombre de données manquantes (n_missing) et le taux de “données complètes” (complete_rate)\ndes informations sur le nombre de modalités (n_unique)\nle nombre d’observations pour les modalités les plus représentées (top_counts)\n\nEn un coup d’œil, on sait donc que 3 espèces sont présentes (et on connait leurs effectifs), on sait que les manchots ont été échantillonnées sur 3 îles, et on sait que le sexe de 11 individus sur 344 (soit 3%) est inconnu. Pour le reste, il y a presque autant de femelles que de mâles.\nLe dernier bloc renseigne sur les variables numériques. Pour chaque d’elle, on a :\n\nle nom de la variable numérique (skim_variable)\nle nombre de données manquantes (n_missing) et le taux de “données complètes” (complete_rate)\nla moyenne (mean) et l’écart-type (sd), ce qui est une nouveauté par rapport à la fonction summary()\nles valeurs minimales (p0), de premier quartile (p25), de second quartile (p50, c’est la médiane !), de troisième quartile (p75) et la valeur maximale (p100)\nun histogramme très simple (hist) qui donne un premier aperçu grossier de la forme de la distribution\n\nLà encore, en un coup d’œil, on dispose donc de toutes les informations pertinentes pour juger de la distribution, de la position et de la dispersion de chaque variable numérique du jeu de données.\n\n\n6.4.2 Tableau groupé\nLa fonction skim(), déjà très pratique, le devient encore plus lorsque l’on choisit de lui fournir seulement certaines variables, et qu’on fait certains regroupements. Par exemple, on peut sélectionner les variables relatives aux dimensions du bec (bill_length_mm et bill_depth_mm) avec la fonction select() que nous connaissons déjà, et demander un résumé des données pour chaque espèce grâce à la fonction group_by() que nous connaissons également :\n\npenguins |&gt;                     # Avec le tableau penguins...\n  select(species, \n         bill_length_mm,\n         bill_depth_mm) |&gt;      # Je sélectionne les variables d'intérêt...\n  group_by(species) |&gt;          # Je regroupe par espèce...\n  skim()                        # Et je produis un résumé des données\n\n\nData summary\n\n\nName\ngroup_by(…)\n\n\nNumber of rows\n344\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nspecies\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nspecies\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbill_length_mm\nAdelie\n1\n0.99\n38.79\n2.66\n32.1\n36.75\n38.80\n40.75\n46.0\n▁▆▇▆▁\n\n\nbill_length_mm\nChinstrap\n0\n1.00\n48.83\n3.34\n40.9\n46.35\n49.55\n51.08\n58.0\n▂▇▇▅▁\n\n\nbill_length_mm\nGentoo\n1\n0.99\n47.50\n3.08\n40.9\n45.30\n47.30\n49.55\n59.6\n▃▇▆▁▁\n\n\nbill_depth_mm\nAdelie\n1\n0.99\n18.35\n1.22\n15.5\n17.50\n18.40\n19.00\n21.5\n▂▆▇▃▁\n\n\nbill_depth_mm\nChinstrap\n0\n1.00\n18.42\n1.14\n16.4\n17.50\n18.45\n19.40\n20.8\n▅▇▇▆▂\n\n\nbill_depth_mm\nGentoo\n1\n0.99\n14.98\n0.98\n13.1\n14.20\n15.00\n15.70\n17.3\n▅▇▇▆▂\n\n\n\n\n\nOn constate ici que pour chaque variable numérique sélectionnée, des statistiques descriptives détaillées nous sont fournies pour chacune des 3 espèces. Ce premier examen semble montrer que :\n\nL’espèce Adélie est celle qui possède le bec le plus court (ses valeurs de moyennes, médianes et quartiles sont plus faibles que celles des 2 autres espèces).\nL’espèce Gentoo est celle qui possède le bec le plus fin, ou le moins épais (ses valeurs de moyennes, médianes et quartiles sont plus faibles que celles des 2 autres espèces)\nIl ne semble pas y avoir de fortes différences d’écarts-types (donc, une dispersion comparable des points autour de leur moyenne respective) entre les 3 espèces : pour chacune des 2 variables numériques, des valeurs d’écarts-types comparables sont en effet observées pour les 3 espèces\nLa distribution des 2 variables numériques semble approximativement suivre une distribution symétrique pour les 3 espèces, avec une forme de courbe en cloche. Les distributions devraient donc suivre à peu une distribution normale\n\n\n\n\n\n\n\nNote\n\n\n\nVous comprenez j’espère l’importance d’examiner ce genre de résumé des données avant de vous lancer dans des tests statistiques. Ils sont un complément indispensable aux explorations graphiques que vous devez également prendre l’habitude de réaliser pour mieux appréhender et comprendre la nature de vos données. Puisque chaque jeu de données est unique, vous devrez vous adapter à la situation et aux questions scientifiques qui vous seront posées (ou que vous vous poserez !) : les choix qui seront pertinents pour une situation ne le seront pas nécessairement pour une autre. Mais dans tous les cas, pour savoir où vous allez et pour ne pas faire de bêtise au moment des tests statistiques et de leur interprétation, vous devrez toujours explorer vos données, avec des graphiques exploratoires et des statistiques descriptives(."
  },
  {
    "objectID": "06-EDA.html#sec-exo20",
    "href": "06-EDA.html#sec-exo20",
    "title": "6  Exploration statistique des données",
    "section": "6.5 Exercice",
    "text": "6.5 Exercice\nEn utilisant les fonctions de résumé abordées jusqu’ici et le tableau weather, répondez aux questions suivante :\n\nDans quel aéroport de New York les précipitations moyennes ont-elle été les plus fortes en 2013 ?\nDans quel aéroport de New York la vitesse du vent moyenne était-elle la plus forte en 2013 ? Quelle est cette vitesse ?\nDans quel aéroport de New York les rafales de vent étaient-elles les plus variables en 2013 ? Quel indice statistique vous donne cette information et quelle est sa valeur ?\nLes précipitation dans les 3 aéroports de New-York ont-elles une distribution symétrique ?\nQuelle est la température médiane observée en 2013 tous aéroports confondus ?\nTous aéroports confondus, quel est le mois de l’année où la température a été la plus variable en 2013 ? Quelles étaient les températures minimales et maximales observées ce mois-là ?\n\n\n\n\n\nHorst, Allison, Alison Hill, et Kristen Gorman. 2022. palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://CRAN.R-project.org/package=palmerpenguins.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, et Shannon Ellis. 2022. skimr: Compact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr.\n\n\nWickham, Hadley. 2021. nycflights13: Flights that Departed NYC in 2013. https://github.com/hadley/nycflights13.\n\n\n———. 2023. tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, et Teun van den Brand. 2024. ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, et Davis Vaughan. 2023. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, et Jennifer Bryan. 2023. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Davis Vaughan, et Maximilian Girlich. 2023. tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr."
  },
  {
    "objectID": "07-Incertitude.html#pré-requis",
    "href": "07-Incertitude.html#pré-requis",
    "title": "7  Dispersion et incertitude",
    "section": "7.1 Pré-requis",
    "text": "7.1 Pré-requis\nNous avons ici besoin des packages suivants :\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(scales)\n\nPensez à les charger en mémoire si ce n’est pas déjà fait ou si vous venez de démarrer une nouvelle session de travail.\nLe package scales (Wickham, Pedersen, et Seidel 2023) contient de très nombreuses fonctions particulièrement utiles pour améliorer l’aspect des légendes d’axes (par exemple, pour transformer des chiffres compris entre 0 et 1 en pourcentages, ou pour ajouter le symbole d’une devise quand l’axe renseigne sur des montants en euros ou dollars par exemple)."
  },
  {
    "objectID": "07-Incertitude.html#la-notion-de-dispersion",
    "href": "07-Incertitude.html#la-notion-de-dispersion",
    "title": "7  Dispersion et incertitude",
    "section": "7.2 La notion de dispersion",
    "text": "7.2 La notion de dispersion\nComme expliqué à la Section 6.3.1, les indices de dispersion nous renseignent sur la variabilités des données autour de la valeur centrale (moyenne ou médiane) d’une population ou d’un échantillon. L’écart-type, la variance et l’intervalle interquartile sont 3 exemples d’indices de dispersion. Prenons l’exemple de l’écart-type. Un écart-type faible indique que la majorité des observations ont des valeurs proches de la moyenne. À l’inverse, un écart-type important indique que la plupart des points sont éloignés de la moyenne. L’écart-type est une caractéristique de la population que l’on étudie grâce à un échantillon, au même titre que la moyenne. En travaillant sur un échantillon, on espère accéder aux vraies grandeurs de la population. Même si ces vraies grandeurs sont à jamais inaccessibles (on ne connaîtra jamais parfaitement quelle est la vraie valeur de moyenne \\(\\mu\\) ou d’écart-type \\(\\sigma\\) de la population), on espère qu’avec un échantillonnage réalisé correctement, la moyenne de l’échantillon (\\(\\bar{x}\\) ou \\(m\\)) et l’écart-type (\\(s\\)) de l’échantillon reflètent assez fidèlement les valeurs de la population générale. C’est la notion d’estimateur, intimement liée à la notion d’inférence statistique : la moyenne de l’échantillon, que l’on connait avec précision, est un estimateur de la moyenne \\(\\mu\\) de la population qui restera à jamais inconnue. C’est la raison pour laquelle la moyenne de l’échantillon est parfois notée \\(\\hat{\\mu}\\) (en plus de \\(\\bar{x}\\) ou \\(m\\)). De même, l’écart-type \\(s\\) et la variance \\(s^2\\) d’un échantillon sont des estimateurs de l’écart-type \\(\\sigma\\) et de la variance \\(\\sigma^2\\) de la population générale. C’est la raison pour laquelle on les note parfois \\(\\hat{\\sigma}\\) et \\(\\hat{\\sigma}^2\\) respectivement. L’accent circonflexe se prononce “chapeau”. On dit donc que \\(\\hat{\\sigma}\\) (sigma chapeau, l’écart-type de l’échantillon) est un estimateur de l´écart-type de la population générale. Comme nous l’avons vu, les indices de dispersion doivent accompagner les indices de position lorsque l’on décrit des données, car présenter une valeur de moyenne, ou de médiane seule n’a pas de sens : il faut savoir à quel point les données sont proches ou éloignées de la tendance centrale pour savoir si, dans la population générale, les indicateurs de position correspondent ou non, aux valeurs portées par la majorité des individus.\nNous avons vu au Chapitre 6 comment calculer des indices de position et de dispersion. Tout ceci devrait donc être clair pour vous à ce stade."
  },
  {
    "objectID": "07-Incertitude.html#la-notion-dincertitude",
    "href": "07-Incertitude.html#la-notion-dincertitude",
    "title": "7  Dispersion et incertitude",
    "section": "7.3 La notion d’incertitude",
    "text": "7.3 La notion d’incertitude\nPar ailleurs, puisqu’on ne sait jamais avec certitude si nos estimations (de moyennes ou d’écarts-types ou de tout autre paramètre) reflètent fidèlement ou non les vraies valeurs de la population, nous devons quantifier à quel point nos estimations s’écartent de celles de la population générale. C’est tout l’intérêt des statistiques et c’est ce que permettent les indices d’incertitude : on ne connaîtra jamais la vraie valeur de moyenne ou d’écart-type de la population, mais on peut quantifier à quel point nos estimations (basées sur un échantillon) sont précises ou imprécises.\nLes deux indices d’incertitude les plus connus (et les plus utilisés) sont :\n\nl’intervalle de confiance à 95% (de la moyenne ou de tout autre estimateur ; les formules sont nombreuses et il n’est pas utile de les détailler ici : nous verrons comment les calculer plus bas)\net l’erreur standard de la moyenne (\\(se_{\\bar{x}}\\)), dont la formule est la suivante :\n\n\\[se_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\\] avec \\(s\\), l’écart-type de l’échantillon et \\(n\\) la taille de l’échantillon.\nComme pour la moyenne, on peut calculer l’erreur standard d’un écart-type, d’une médiane, d’une proportion, ou de tout autre estimateur calculé sur un échantillon. Cet indice d’incertitude ne nous renseigne pas sur une grandeur de la population générale qu’on chercherait à estimer, mais bien sur l’incertitude associée à une estimation que nous faisons en travaillant sur un échantillon de taille forcément limitée. Tout processus d’échantillonnage est forcément entaché d’incertitude, causée entre autre par le hasard de l’échantillonnage (ou fluctuation d’échantillonnage). Puisque nous travaillons sur des échantillons forcément imparfaits, les indices d’incertitude vont nous permettre de quantifier à quel point nos estimations s’écartent des vraies valeurs de la population. Ces “vraies valeurs”, faute de pouvoir collecter tous les individus de la population, resteront à jamais inconnues.\n\n\n\n\n\n\nAutrement dit…\n\n\n\nQuand on étudie des populations naturelles grâce à des échantillons on se trompe toujours. Les statistiques nous permettent de quantifier à quel point on se trompe grâce aux indices d’incertitude, et c’est déjà pas mal !\n\n\nEn examinant la formule de l’erreur standard de la moyenne présentée ci-dessus, on comprend intuitivement que plus la taille de l’échantillon (\\(n\\)) augmente, plus l’erreur standard (donc l’incertitude) associée à notre estimation de moyenne diminue. Autrement dit, plus les données sont abondantes dans l’échantillon, meilleure sera notre estimation de moyenne, et donc, moins le risque de raconter des bêtises sera grand.\nL’autre indice d’incertitude très fréquemment utilisé est l’intervalle de confiance à 95% (de la moyenne, de la médiane, de la variance, ou de tout autre estimateur calculé dans un échantillon). L’intervalle de confiance nous renseigne sur la gamme des valeurs les plus probables pour un paramètre de la population étudiée. Par exemple, si j’observe, dans un échantillon, une moyenne de 10, avec un intervalle de confiance calculé de [7 ; 15], cela signifie que, dans la population générale, la vraie valeur de moyenne a de bonnes chances de se trouver dans l’intervalle [7 ; 15]. Dans la population générale, toutes les valeurs comprises entre 7 et 15 sont vraisemblables pour la moyenne alors que les valeurs situées en dehors de cet intervalle sont moins probables. Une autre façon de comprendre l’intervalle de confiance est de dire que si je récupère un grand nombre d’échantillons dans la même population, en utilisant exactement le même protocole expérimental, 95% des échantillons que je vais récupérer auront une moyenne située à l’intérieur de l’intervalle de confiance à 95%, et 5% des échantillons auront une moyenne située à l’extérieur de l’intervalle de confiance à 95%. C’est une notion qui n’est pas si évidente que ça à comprendre, donc prenez bien le temps de relire cette section si besoin, et de poser des questions le cas échéant.\nConcrètement, plus l’intervalle de confiance est large, moins notre confiance est grande. Si la moyenne d’un échantillon vaut \\(\\bar{x} = 10\\), et que son intervalle de confiance à 95% vaut [9,5 ; 11], la gamme des valeurs probables pour la moyenne de la population est étroite. Autrement dit, la moyenne de l’échantillon (10), a de bonne chances d’être très proche de la vraie valeur de moyenne de la population générale (vraisemblablement comprise quelque part entre 9,5 et 11). À l’inverse, si l’intervalle de confiance à 95% de la moyenne vaut [4 ; 17], la gamme des valeurs possibles pour la vraie moyenne de la population est grande. La moyenne de l’échantillon aura donc de grandes chances d’être assez éloignée de la vraie valeur de la population.\nLa notion d’intervalle de confiance à 95% est donc très proche de celle d’erreur standard. D’ailleurs, pour de nombreux paramètres, l’intervalle de confiance est calculé à partir de l’erreur standard.\n\n\n\n\n\n\nÀ retenir !\n\n\n\nLes paramètres de position et de dispersion sont des caractéristiques des populations que l’on étudie. On espère pouvoir les estimer correctement grâce aux échantillons dont on dispose.\nLes indices d’incertitude ne sont pas des paramètres ou des caractéristiques des populations que l’on étudie. On ne cherche donc pas à les estimer. Ils nous permettent en revanche de quantifier à quel point on se trompe en cherchant à estimer des paramètres de la population générale à partir d’estimateurs calculés sur un échantillon.\nAutrement dit, calculer des estimateurs de position et de dispersion permet d’apprendre des choses au sujet des populations étudiées. Calculer des indices d’incertitude ne permet pas d’apprendre quoi que ce soit sur les populations étudiées, mais permet d’apprendre des choses sur la qualité de notre travail d’échantillonnage et d’estimation. En général, les indices d’incertitudes nous fournissent une gamme de valeurs au sein desquelles les vraies valeurs des paramètres des populations générales ont de bonnes chances de se trouver. Si on a bien travaillé, et qu’on dispose de beaucoup de données, ces gammes de valeurs seront peu étendues. Si à l’inverse, nous ne disposons que de trop peu de données, ces gammes de valeurs seront très étendues."
  },
  {
    "objectID": "07-Incertitude.html#sec-errstd",
    "href": "07-Incertitude.html#sec-errstd",
    "title": "7  Dispersion et incertitude",
    "section": "7.4 Calcul de l’erreur standard de la moyenne",
    "text": "7.4 Calcul de l’erreur standard de la moyenne\nContrairement aux indices de position et de dispersion, il n’existe pas de fonction intégrée à R qui permette de calculer l’erreur standard de la moyenne. Toutefois, sa formule très simple nous permet de la calculer à la main quand on en a besoin grâce à la fonction summarise().\nPar exemple, reprenons les données de masse corporelle des 3 espèces de manchots dans le tableau penguins. Imaginons que nous souhaitions étudier les différences de masses corporelles des 3 espèces, en tenant compte du sexe des individus. Pour chaque espèce et chaque sexe, nous allons calculer la masse moyenne des individus en grammes (variable body_mass_g). Nous prendrons soin au préalable d’éliminer les lignes pour lesquelles le sexe des individus est inconnu :\n\npenguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  summarise(moyenne = mean(body_mass_g, na.rm = TRUE),\n            .by = c(species, sex))\n\n# A tibble: 6 × 3\n  species   sex    moyenne\n  &lt;fct&gt;     &lt;fct&gt;    &lt;dbl&gt;\n1 Adelie    male     4043.\n2 Adelie    female   3369.\n3 Gentoo    female   4680.\n4 Gentoo    male     5485.\n5 Chinstrap female   3527.\n6 Chinstrap male     3939.\n\n\nPour pouvoir réutiliser ce tableau, je lui donne un nom :\n\nmasses &lt;- penguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  summarise(moyenne = mean(body_mass_g, na.rm = TRUE),\n            .by = c(species, sex))\n\nAu final, on peut faire un graphique avec ces données. Puisqu’on dispose d’une variable numérique et 2 variables catégorielles, je fais le cjoix de produire un diagramme bâtons facettés :\n\nmasses |&gt; \n  ggplot(aes(x = sex, y = moyenne, fill = species)) +\n  geom_col(color = \"grey20\", alpha = 0.5) +\n  facet_wrap(~species, nrow = 1) +\n  labs(x = \"\", y = \"Masse moyenne (g)\", fill = \"Espèce\") +\n  theme_bw() +\n  scale_fill_brewer(palette = \"Accent\") +\n  scale_y_continuous(labels = number_format()) +\n  scale_x_discrete(labels = c(\"Femelles\", \"Mâles\"))\n\n\n\n\nFigure 7.1: Comparison des tailles moyennes observées chez les mâles et les femelles de 3 espèces de manchots\n\n\n\n\nVous remarquerez que :\n\nJ’utilise geom_col() et non geom_bar() car j’ai déjà calculé manuellement la variable que je souhaite associer à l’axe des ordonnées\nJ’associe la couleur de remplissage à l’espèce, bien que ça ne soit pas indispensable puisque je fais un sous-graphique par espèce. Cela rend toutefois le graphique plus agréable et en facilite la lecture.\nJe modifie 3 échelles :\n\n\navec scale_fill_brewer(), je change la palette de couleur utilisée pour le remplissage des barres\navec scale_y_continuous(), je modifie l’échelle (continue) de l’axe des ordonnées. Je fais appel à la fonction number_format() du package scales afin d’ajouter un séparateur des milliers aux chiffres de l’axe.\navec scale_x_discrete(), le modifie les termes qui apparaissent sur l’axe des abscisses (qui est un axe discontinu, ou catégoriel, ou discret). Les catégories female et male de la variable sex du tableau penguins sont transformées en Femelles et Mâles respectivement\n\nPuisque chaque barre de la Figure 7.1 correspond à une valeur de moyenne, il nous faut présenter également l’incertitude associée à ces calculs de moyennes. Pour cela, nous devons calculer l’erreur standard des moyennes :\n\npenguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  summarise(moyenne = mean(body_mass_g, na.rm = TRUE),\n            N_obs = n(),\n            erreur_standard = sd(body_mass_g, na.rm = TRUE) / sqrt(N_obs),\n            .by = c(species, sex))\n\n# A tibble: 6 × 5\n  species   sex    moyenne N_obs erreur_standard\n  &lt;fct&gt;     &lt;fct&gt;    &lt;dbl&gt; &lt;int&gt;           &lt;dbl&gt;\n1 Adelie    male     4043.    73            40.6\n2 Adelie    female   3369.    73            31.5\n3 Gentoo    female   4680.    58            37.0\n4 Gentoo    male     5485.    61            40.1\n5 Chinstrap female   3527.    34            48.9\n6 Chinstrap male     3939.    34            62.1\n\n\nNotre tableau de statistiques descriptives possède maintenant 2 colonnes supplémentaires : le nombre d’observations (que j’ai nommé N_obs), et l’erreur standard associée à chaque moyenne, calculée grâce à la formule vue plus haut \\(se_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\\) (la fonction sqrt() permet de calculer la racine carrée). On constate que l’erreur standard, qui s’exprime dans la même unité que la moyenne, varie du simple au double selon les groupes. Ainsi, pour les mâles de l’espèce Chinstrap, l’incertitude est deux fois plus importante que pour les femelles de l’espèce Adélie. Cela est probablement dû en partie à des différences de tailles d’échantillons importantes entre ces 2 catégories (73 femelles Adélie contre 34 mâles Chinstrap), mais ça n’est certainement pas la seule explications. Sinon, les femelles Chinstrap auraient elles aussi une incertitude plus grande. L’incertitude reflète aussi, de façon indirecte, la variabilité de la variable étudiée.\nUne fois de plus, je donne un nom à ce tableau de données pour pouvoir le réutiliser plus tard :\n\nmasses_se &lt;- penguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  summarise(moyenne = mean(body_mass_g, na.rm = TRUE),\n            N_obs = n(),\n            erreur_standard = sd(body_mass_g, na.rm = TRUE) / sqrt(N_obs),\n            .by = c(species, sex))\n\nNotez que le package ggplot2 contient une fonction permettant de calculer à la fois la moyenne et erreur standard de la moyenne d’un échantillon : mean_se(). Puisque cette fonction renvoie 3 valeurs (\\(\\bar{x}\\), \\(\\bar{x} - se\\) et \\(\\bar{x} + se\\)), on utilise reframe() :\n\npenguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  reframe(mean_se(body_mass_g), \n          .by = c(species, sex))\n\n# A tibble: 6 × 5\n  species   sex        y  ymin  ymax\n  &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie    male   4043. 4003. 4084.\n2 Adelie    female 3369. 3337. 3400.\n3 Gentoo    female 4680. 4643. 4717.\n4 Gentoo    male   5485. 5445. 5525.\n5 Chinstrap female 3527. 3478. 3576.\n6 Chinstrap male   3939. 3877. 4001.\n\n\nLes résultats obtenus ne sont pas exactement au même format :\n\nla colonne y contient les valeurs de moyennes (\\(\\bar{x}\\))\nla colonne ymin contient la valeur de moyenne moins une fois l’erreur standard (\\(\\bar{x} - se\\))\nla colonne ymax contient la valeur de moyenne plus une fois l’erreur standard (\\(\\bar{x} + se\\))\n\nNotez également que contrairement aux fonctions mean() ou sd(), la fonction mean_se() n’a pas besoin qu’on lui précise na.rm = TRUE : par défaut, elle ignore les valeurs manquantes.\nIl ne nous restera plus qu’à ajouter des barres d’erreur sur notre graphique pour visualiser l’incertitude associée à chaque valeur de moyenne. C’est ce que nous verrons au Chapitre 8."
  },
  {
    "objectID": "07-Incertitude.html#sec-confint",
    "href": "07-Incertitude.html#sec-confint",
    "title": "7  Dispersion et incertitude",
    "section": "7.5 Calculs d’intervalles de confiance à 95%",
    "text": "7.5 Calculs d’intervalles de confiance à 95%\nComme pour les erreurs standard, il est possible de calculer des intervalles de confiance de n’importe quel estimateur calculé à partir d’un échantillon, pour déterminer la gamme des valeurs les plus probables pour les paramètres équivalents dans la population générale. Nous nous concentrerons ici sur le calcul des intervalles de confiance à 95% de la moyenne, mais nous serons amenés à examiner également l’intervalle de confiance de la médiane, puis au Chapitre 11, l’intervalle de confiance à 95% d’une différence de moyennes.\nContrairement à l’erreur standard, il n’y a pas qu’une bonne façon de calculer l’intervalle de confiance à 95% d’une moyenne. Plusieurs formules existent et le choix de la formule dépend en partie de la distribution des données (la distribution suit-elle une loi Normale ou non) et de la taille de l’échantillon dont nous disposons (\\(n\\) est-il supérieur à 30 ou non ?). Dans la situation idéale d’une variable qui suit la distribution Normale, les bornes inférieures et supérieures de l’intervalle de confiance à 95% sont obtenues grâce à cette formule\n\\[\\bar{x} - 1.96 \\cdot \\frac{s}{\\sqrt{n}} &lt; \\mu &lt; \\bar{x} + 1.96 \\cdot \\frac{s}{\\sqrt{n}}\\] Autrement dit, la vraie moyenne \\(\\mu\\) d’une population a de bonnes chances de se trouver dans un intervalle de plus ou moins 1.96 fois l’erreur standard de la moyenne. En première approximation, l’intervalle de confiance est donc la moyenne de l’échantillon \\(\\bar{x}\\) plus ou moins 2 fois l’erreur standard (que nous avons appris à calculer à la main un peu plus tôt). On peut donc calculer à la main les bornes inférieures et supérieures de l’intervalle de confiance ainsi :\n\npenguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  reframe(mean_se(body_mass_g, mult = 1.96), \n          .by = c(species, sex))\n\n# A tibble: 6 × 5\n  species   sex        y  ymin  ymax\n  &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie    male   4043. 3964. 4123.\n2 Adelie    female 3369. 3307. 3431.\n3 Gentoo    female 4680. 4607. 4752.\n4 Gentoo    male   5485. 5406. 5563.\n5 Chinstrap female 3527. 3431. 3623.\n6 Chinstrap male   3939. 3817. 4061.\n\n\nIci, grâce à l’argument mult = 1.96 de la fonction mean_se() :\n\nla colonne ymin contient maintenant les valeurs de moyennes moins 1.96 fois l’erreur standard\nla colonne ymax contient maintenant les valeurs de moyennes plus 1.96 fois l’erreur standard\n\nDans la pratique, puisque cette méthode reste approximative et dépend de la nature des données dont on dispose, on utilisera plutôt des fonctions spécifiques qui calculeront pour nous les intervalles de confiance à 95% de nos estimateurs. C’est ce que permet en particulier la fonction mean_cl_normal() du package ggplot2. Il est toutefois important de bien comprendre qu’il y a un lien étroit entre l’erreur standard (l’incertitude associées à l’estimation d’un paramètre d’une population à partir des données d’un échantillon), et l’intervalle de confiance à 95% de ce paramètre.\n\npenguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  reframe(mean_cl_normal(body_mass_g), \n          .by = c(species, sex))\n\n# A tibble: 6 × 5\n  species   sex        y  ymin  ymax\n  &lt;fct&gt;     &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie    male   4043. 3963. 4124.\n2 Adelie    female 3369. 3306. 3432.\n3 Gentoo    female 4680. 4606. 4754.\n4 Gentoo    male   5485. 5405. 5565.\n5 Chinstrap female 3527. 3428. 3627.\n6 Chinstrap male   3939. 3813. 4065.\n\n\nComme dans les tableaux précédents, 3 nouvelles colonnes ont été crées :\n\ny contient toujours la moyenne des températures mensuelles pour chaque aéroport\nymin contient maintenant les bornes inférieures de l’intervalle à 95% des moyennes\nymax contient maintenant les bornes supérieures de l’intervalle à 95% des moyennes\n\nPour que la suite soit plus claire, nous allons afficher et donner des noms à ces différents tableaux en prenant soin de renommer les colonnes pour plus de clarté.\nTout d’abord, nous disposons du tableau masses_se, qui contient, les masses moyennes des mâles et des femelles des 3 espèces, et les erreurs standard de ces moyennes :\n\nmasses_se\n\n# A tibble: 6 × 5\n  species   sex    moyenne N_obs erreur_standard\n  &lt;fct&gt;     &lt;fct&gt;    &lt;dbl&gt; &lt;int&gt;           &lt;dbl&gt;\n1 Adelie    male     4043.    73            40.6\n2 Adelie    female   3369.    73            31.5\n3 Gentoo    female   4680.    58            37.0\n4 Gentoo    male     5485.    61            40.1\n5 Chinstrap female   3527.    34            48.9\n6 Chinstrap male     3939.    34            62.1\n\n\nEnsuite, nous avons produit un tableau presque équivalent que nous allons nommer masses_se_bornes et pour lequel nous allons modifier le nom des colonnes y, ymin et ymax :\n\nmasses_se_bornes &lt;- penguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  reframe(mean_se(body_mass_g),\n            .by = c(species, sex)) |&gt; \n  rename(moyenne = y,\n         moyenne_moins_se = ymin,\n         moyenne_plus_se = ymax)\n\nmasses_se_bornes\n\n# A tibble: 6 × 5\n  species   sex    moyenne moyenne_moins_se moyenne_plus_se\n  &lt;fct&gt;     &lt;fct&gt;    &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;\n1 Adelie    male     4043.            4003.           4084.\n2 Adelie    female   3369.            3337.           3400.\n3 Gentoo    female   4680.            4643.           4717.\n4 Gentoo    male     5485.            5445.           5525.\n5 Chinstrap female   3527.            3478.           3576.\n6 Chinstrap male     3939.            3877.           4001.\n\n\nNous avons ensuite calculé manuellement des intervalles de confiance approximatifs, avec la fonction mean_se() et son argument mult = 1.96. Là encore, nous allons stocker cet objet dans un tableau nommé masses_ci_approx, et nous allons modifier le nom des colonnes y, ymin, et ymax :\n\nmasses_ci_approx &lt;- penguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  reframe(mean_se(body_mass_g, mult = 1.96), \n          .by = c(species, sex)) |&gt; \n  rename(moyenne = y,\n         ci_borne_inf = ymin,\n         ci_borne_sup = ymax)\n\nmasses_ci_approx\n\n# A tibble: 6 × 5\n  species   sex    moyenne ci_borne_inf ci_borne_sup\n  &lt;fct&gt;     &lt;fct&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Adelie    male     4043.        3964.        4123.\n2 Adelie    female   3369.        3307.        3431.\n3 Gentoo    female   4680.        4607.        4752.\n4 Gentoo    male     5485.        5406.        5563.\n5 Chinstrap female   3527.        3431.        3623.\n6 Chinstrap male     3939.        3817.        4061.\n\n\nEnfin, nous avons calculé les intervalles de confiance avec une fonction spécialement dédiée à cette tâche : la fonction mean_cl_normal(). Nous allons stocker cet objet dans un tableau nommé masses_ci, et nous allons modifier le nom des colonnes y, ymin, et ymax :\n\nmasses_ci &lt;- penguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  reframe(mean_cl_normal(body_mass_g), \n          .by = c(species, sex)) |&gt; \n  rename(moyenne = y,\n         ci_borne_inf = ymin,\n         ci_borne_sup = ymax)\n\nmasses_ci\n\n# A tibble: 6 × 5\n  species   sex    moyenne ci_borne_inf ci_borne_sup\n  &lt;fct&gt;     &lt;fct&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Adelie    male     4043.        3963.        4124.\n2 Adelie    female   3369.        3306.        3432.\n3 Gentoo    female   4680.        4606.        4754.\n4 Gentoo    male     5485.        5405.        5565.\n5 Chinstrap female   3527.        3428.        3627.\n6 Chinstrap male     3939.        3813.        4065.\n\n\nMaintenant, si l’on compare les 2 tableaux contenant les calculs d’intervalles de confiance de la moyenne, on constate que les résultats sont très proches :\n\nmasses_ci_approx\nmasses_ci\n\n\n\n# A tibble: 6 × 5\n  species   sex    moyenne ci_borne_inf ci_borne_sup\n  &lt;fct&gt;     &lt;fct&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Adelie    male     4043.        3964.        4123.\n2 Adelie    female   3369.        3307.        3431.\n3 Gentoo    female   4680.        4607.        4752.\n4 Gentoo    male     5485.        5406.        5563.\n5 Chinstrap female   3527.        3431.        3623.\n6 Chinstrap male     3939.        3817.        4061.\n\n\n# A tibble: 6 × 5\n  species   sex    moyenne ci_borne_inf ci_borne_sup\n  &lt;fct&gt;     &lt;fct&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Adelie    male     4043.        3963.        4124.\n2 Adelie    female   3369.        3306.        3432.\n3 Gentoo    female   4680.        4606.        4754.\n4 Gentoo    male     5485.        5405.        5565.\n5 Chinstrap female   3527.        3428.        3627.\n6 Chinstrap male     3939.        3813.        4065.\n\n\n\n\nLes bornes inférieures et supérieures des intervalles de confiance à 95% des moyennes ne sont pas égales quand on les calcule manuellement de façon approchée et quand on les calcule de façon exacte, mais les différences sont minimes.\n\n\n\n\nWickham, Hadley, Thomas Lin Pedersen, et Dana Seidel. 2023. scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales."
  },
  {
    "objectID": "08-ErrorBars.html#pré-requis",
    "href": "08-ErrorBars.html#pré-requis",
    "title": "8  Visualiser l’incertitude et la dispersion",
    "section": "8.1 Pré-requis",
    "text": "8.1 Pré-requis\nNous avons ici besoin des packages suivants :\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(nycflights13)\nlibrary(scales)\nlibrary(colorspace)\n\nLe package colorspace (Ihaka et al. 2023) permet d’utiliser de très nombreuses palettes de couleurs. Ces palettes ont de nombreux avantages (dont la propriété de permettre aux daltoniens de distinguer correctement les différentes couleurs des palettes catégorielles). N’hésitez pas à consulter le site dédié à ce package qui est très complet et présente de nombreux exemples.\nNous aurons aussi besoin des tableaux créés au Chapitre 7 (masses_se, masses_se_bornes, masses_ci_approx et masses_ci).\nDonc pensez bien à charger en mémoire les packages et à relancer les commandes de vos scripts qui vous ont permis de créer ces tableaux s’ils ne sont plus en mémoire dans votre environnement de travail. Il existe plusieurs façons de représenter visuellement les positions, les dispersions et les incertitudes. Concernant les positions et les dispersions tout d’abord, nous avons déjà vu plusieurs façons de faire au Chapitre 3, en particulier dans les parties consacrées aux histogrammes, aux stripcharts et aux boxplots. Nous reprenons ici brièvement chacun de ces 3 types de graphique afin de les remettre en contexte avec ce que nous avons appris ici.\nDans un dernier temps, nous verrons comment visualiser l’incertitude associée à des calculs de moyennes ou de variances grâce aux barres d’erreurs ou aux encoches des boîtes à moustaches."
  },
  {
    "objectID": "08-ErrorBars.html#position-et-dispersion-les-histogrammes",
    "href": "08-ErrorBars.html#position-et-dispersion-les-histogrammes",
    "title": "8  Visualiser l’incertitude et la dispersion",
    "section": "8.2 Position et dispersion : les histogrammes",
    "text": "8.2 Position et dispersion : les histogrammes\nJe vous renvoie à la Section 3.5.1 si vous avez besoin de vous rafraîchir la mémoire. Vous pouvez aussi jeter aussi un œil à la partie sur les histogrammes facettés, section 3.9.1.\nLes histogrammes permettent de déterminer à la fois où se trouvent les valeurs les plus fréquemment observées (la position du pic principal correspond à la tendance centrale), et la dispersion (ou variabilité) des valeurs autour de la tendance centrale. Par exemple, la fonction facet_grid() permet de faire des histogrammes des températures pour chaque aéroport de New York et chaque mois de l’année 2013 :\n\npenguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  ggplot(aes(x = body_mass_g, fill = species)) +\n  geom_histogram(bins = 20, color = \"grey20\", alpha = 0.5, show.legend = FALSE) +\n  facet_grid(species ~ sex, scales = \"free_y\") +\n  labs(x = \"Masse corporelle (g)\", y = \"Fréquence\") +\n  scale_fill_brewer(palette = \"Accent\") +\n  scale_x_continuous(labels = number_format())+\n  theme_bw()\n\n\n\n\nFigure 8.1: Distribution des masses corporelles chez les mâles et les femelles de 3 espèces de manchots\n\n\n\n\nIci, 6 histogrammes sont produits. Ils permettent de constater que :\n\nles masses sont à peu près toutes distribuées selon une courbe en cloche\nles masses moyennes sont plus élevées chez les Gentoo que chez les deux autres espèces. C’est bien la position des pics sur l’axe des abscisses qui nous renseigne là-dessus.\npour chaque espèce, les masses moyennes sont globalement plus élevées chez les mâles que chez les femelles. Par exemple, pour l’espèce Adélie, le pic se situe autour de 3500 grammes pour les femelles, et autour de 4000 grammes pour les mâles.\nla variabilité des masses est comparable pour les 3 espèces et les 2 sexes. Cette fois, c’est l’étalement des histogrammes qui nous renseigne sur la dispersion. Ici, l’étalement est toujours d’environ 1500 grammes, sauf peut-être pour les femelles Gentoo dont l’étalement est d’environ 1000 grammes."
  },
  {
    "objectID": "08-ErrorBars.html#position-et-dispersion-les-stripcharts",
    "href": "08-ErrorBars.html#position-et-dispersion-les-stripcharts",
    "title": "8  Visualiser l’incertitude et la dispersion",
    "section": "8.3 Position et dispersion : les stripcharts",
    "text": "8.3 Position et dispersion : les stripcharts\nUne autre façon de visualiser à la fois les tendances centrales et les dispersions consiste à produire un nuage de points “stripchart”. Là encore, je vous renvoie à la partie sur les stripcharts, section 3.9.2 si vous avez besoin de vous rafraîchir la mémoire.\n\npenguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  ggplot(aes(x = sex, y = body_mass_g, fill = sex)) +\n  geom_jitter(shape = 21, color = \"grey20\", show.legend = FALSE,\n              width = 0.15, height = 0,\n              alpha = 0.5) +\n  facet_wrap(~ species, ncol = 1, scales = \"free_y\") +\n  labs(x = \"Sexe\", y = \"Masse corporelle (g)\") +\n  scale_fill_brewer(palette = \"Accent\") +\n  scale_y_continuous(labels = number_format())+\n  theme_bw()\n\n\n\n\nFigure 8.2: Distribution des masses corporelles chez les mâles et les femelles de 3 espèces de manchots\n\n\n\n\nCette fois, nous visualisons la totalité des données disponibles, et non les données regroupées dans des classes plus ou moins arbitraires. Mais là encore, on peut facilement comparer la position de chaque série de données : pour chaque espèce, les mâles ont des masses corporelles plus importantes que les femelles, et globalement, les Gentoo ont des masses corporelles plus élevées que les autres espèces. La dispersion des données est aussi facile à comparer entre les groupes. C’est ici l’étendue du nuage de points sur l’axe des ordonnées qui nous permet de le faire.\nEnfin, les stripcharts facettés sont particulièrement utiles lorsque le nombre de séries de données est grand. Par exemple, dans le package nycflights13, le tableau weather contient des données météo enregistrées toutes les heures de l’année 2013. Si l’on souhaite comparer l’évolution des températures mensuelles dans chacun des 3 aéroports de New York, voilà ce qu’on peut faire :\n\nweather |&gt;\n  mutate(temp_celsius = (temp - 32) / 1.8) |&gt;\n  ggplot(aes(x = factor(month), y = temp_celsius, fill = temp_celsius)) +\n  geom_jitter(shape = 21, color = \"grey20\", alpha = 0.5,\n              width = 0.15, height = 0, show.legend = FALSE) +\n  facet_wrap(~ origin, ncol = 1) +\n  scale_fill_continuous_diverging(palette = \"Blue-Red 3\") +\n  labs(x = \"Mois\", y = \"Températures (ºC)\") +\n  theme_bw()\n\n\n\n\nFigure 8.3: Distribution des températures mensuelles dans les 3 aéroports de New York en 20123"
  },
  {
    "objectID": "08-ErrorBars.html#position-et-dispersion-les-boxplots",
    "href": "08-ErrorBars.html#position-et-dispersion-les-boxplots",
    "title": "8  Visualiser l’incertitude et la dispersion",
    "section": "8.4 Position et dispersion : les boxplots",
    "text": "8.4 Position et dispersion : les boxplots\nLa dernière façon classique de visualiser à la fois les tendances centrales et les dispersions consiste à produire un graphique boîtes à moustaches, ou “boxplot”. Là encore, je vous renvoie à la partie sur les boxplots, section 3.9.3 si vous avez besoin de vous rafraîchir la mémoire. Les boîtes à moustaches sont également très pratiques pour comparer de nombreux groupes les uns avec les autres. Avec les données de températures, voilà à quoi ça ressemble :\n\nweather |&gt;\n  mutate(temp_celsius = (temp - 32) / 1.8) |&gt;\n  ggplot(aes(x = factor(month), y = temp_celsius, fill = factor(month))) +\n  geom_boxplot(show.legend = FALSE, alpha = 0.5) +\n  facet_wrap(~ origin, ncol = 1) +\n  labs(x = \"Mois\", y = \"Températures (ºC)\") +\n  theme_bw()\n\n\n\n\nFigure 8.4: Distribution des températures mensuelles dans les 3 aéroports de New York en 20123\n\n\n\n\nVous voyez que le code est très proche pour produire un stripchart ou un boxplot. Comme indiqué au Chapitre 6, les différents éléments de chaque boîte nous renseignent sur la position et sur la dispersion des données pour chaque mois et chaque aéroport :\n\nLa limite inférieure de la boîte correspond au premier quartile : 25% des données de l’échantillon sont situées au-dessous de cette valeur.\nLa limite supérieure de la boîte correspond au troisième quartile : 75% des données de l’échantillon sont situées au-dessous de cette valeur.\nLe segment épais à l’intérieur de la boîte correspond au second quartile : c’est la médiane de l’échantillon, qui nous renseigne sur la position de la distribution. 50% des données de l’échantillon sont situées au-dessus de cette valeur, et 50% au-dessous.\nLa hauteur de la boîte correspond à l’étendue (ou intervalle) interquartile ou Inter Quartile Range (IQR) en anglais. On trouve dans cette boîte 50% des observations de l’échantillon. C’est une mesure de la dispersion des 50% des données les plus centrales. Une boîte plus allongée indique donc une plus grande dispersion.\nLes moustaches correspondent à des valeurs qui sont en dessous du premier quartile (pour la moustache du bas) et au-dessus du troisième quartile (pour la moustache du haut). La règle utilisée dans R est que ces moustaches s’étendent jusqu’aux valeurs minimales et maximales de l’échantillon, mais elles ne peuvent en aucun cas s’étendre au-delà de 1,5 fois la hauteur de la boîte (1,5 fois l’IQR) vers le haut et le bas. Si des points apparaissent au-delà des moustaches (vers le haut ou le bas), ces points sont appelés “outliers”. On peut en observer ici pour plusieurs mois et pour les 3 aéroports (par exemple, en avril dans les 3 aéroports). Ce sont des points qui s’éloignent du centre de la distribution de façon importante puisqu’ils sont au-delà de 1,5 fois l’IQR de part et d’autre du premier ou du troisième quartile. Il peut s’agir d’anomalies de mesures, d’anomalies de saisie des données, ou tout simplement, d’enregistrements tout à fait valides mais atypiques ou extrêmes ; il ne s’agit donc pas toujours de point aberrants. J’attire votre attention sur le fait que la définition de ces outliers est relativement arbitraire. Nous pourrions faire le choix d’étendre les moustaches jusqu’à 1,8 fois l’IQR (ou 2, ou 2,5). Nous observerions alors beaucoup moins d’outliers. D’une façons générale, la longueur des moustaches renseigne sur la variabilité des données en dehors de la zone centrale. Plus elles sont longues, plus la variabilité est importante. Très souvent, l’examen attentif des outliers est utile car il nous permet d’en apprendre plus sur le comportement extrême de certaines observations.\n\nLorsque les boîtes ont une forme à peu près symétrique de part et d’autre de la médiane (c’est le cas pour cet exemple dans la plupart des catégories), cela signifie qu’un histogramme des mêmes données serait symétrique également.\nLes stripcharts et les boxplots sont donc un bon moyen de comparer rapidement la position et la dispersion d’un grand nombre de séries de données : ici, en quelques lignes de code, nous en comparons 12 pour chacun des 3 aéroports de New York.\nLes histogrammes sont plus utiles lorsqu’il y a moins de catégories à comparer, comme pour la Figure 8.1. Ils permettent en outre de mieux visualiser les distributions non symétriques, ou qui présentent plusieurs pics (distribution bi- ou poly-modales)."
  },
  {
    "objectID": "08-ErrorBars.html#visualiser-lincertitude-les-barres-derreur",
    "href": "08-ErrorBars.html#visualiser-lincertitude-les-barres-derreur",
    "title": "8  Visualiser l’incertitude et la dispersion",
    "section": "8.5 Visualiser l’incertitude : les barres d’erreur",
    "text": "8.5 Visualiser l’incertitude : les barres d’erreur\nComme évoqué plus haut, il est important de ne pas confondre dispersion et incertitude. Lorsque l’on visualise des moyennes calculées à partir des données d’un échantillon, il est important de faire apparaître des barres d’erreurs, qui correspondent en général :\n\nsoit à l’erreur standard de la moyenne\nsoit à l’intervalle de confiance à 95% de la moyenne\n\nPuisque deux choix sont possibles, il sera important de préciser systématiquement dans la légende du graphique, la nature des barres représentées. Revenons aux données de masses corporelles des manchots, et commençons par visualiser les masses moyennes avec les erreurs standards. Pour cela, je reprends le tableau masses_se créé précédemment :\n\nmasses_se |&gt; \n  ggplot(aes(x = sex, y = moyenne, fill = species)) +\n  geom_col(color = \"grey20\", alpha = 0.5) +\n  geom_errorbar(aes(ymin = moyenne - erreur_standard,\n                    ymax = moyenne + erreur_standard),\n                width = 0.15) +\n  facet_wrap(~species, nrow = 1) +\n  labs(x = \"\", y = \"Masse moyenne (g)\", fill = \"Espèce\") +\n  theme_bw() +\n  scale_fill_brewer(palette = \"Accent\") +\n  scale_y_continuous(labels = number_format()) +\n  scale_x_discrete(labels = c(\"Femelles\", \"Mâles\"))\n\n\n\n\nFigure 8.5: Comparison des masses moyennes observées chez les mâles et les femelles de 3 espèces de manchots. Les barres d’erreur sont les erreurs standard\n\n\n\n\nVous remarquerez que :\n\nla fonction geom_errorbar() contient de nouvelles caractéristiques esthétiques qu’il nous faut obligatoirement renseigner : les extrémités inférieures et supérieures des barres d’erreur. Il nous faut donc associer 2 variables à ces caractéristiques esthétiques. Ici, nous utilisons moyenne - erreur_standard pour la borne inférieure des barres d’erreur, et moyenne + erreur_standard pour la borne supérieure. Les variables moyenne et erreur_standard faisant partie du tableau masses_se, geom_errorbar() les trouve sans difficulté.\nl’argument width de la fonction geom_errorbar() permet d’indiquer la longueur des segments horizontaux qui apparaissent à chaque extrémité des barres d’erreur.\n\nNous pouvons arriver au même résultats en utilisant le tableau masses_se_bornes, qui contient des variables différentes :\n\nmasses_se_bornes |&gt; \n  ggplot(aes(x = sex, y = moyenne, fill = species)) +\n  geom_col(color = \"grey20\", alpha = 0.5) +\n  geom_errorbar(aes(ymin = moyenne_moins_se,\n                    ymax = moyenne_plus_se),\n                width = 0.15) +\n  facet_wrap(~species, nrow = 1) +\n  labs(x = \"\", y = \"Masse moyenne (g)\", fill = \"Espèce\") +\n  theme_bw() +\n  scale_fill_brewer(palette = \"Accent\") +\n  scale_y_continuous(labels = number_format()) +\n  scale_x_discrete(labels = c(\"Femelles\", \"Mâles\"))\n\n\n\n\nFigure 8.6: Comparison des masses moyennes observées chez les mâles et les femelles de 3 espèces de manchots. Les barres d’erreur sont les erreurs standard\n\n\n\n\nSeule la spécification de ymin et ymax dans geom_errorbar() a changé puisque le tableau masses_se_bornes contient des variables différentes de celles du tableau masses_se.\nDe la même façon, nous pouvons parfaitement faire apparaître, au lieu des erreurs standards, les intervalles de confiance à 95% de chaque masse moyenne. Il nous suffit pour cela d’utiliser le tableau masses_ci qui contient les valeurs de moyennes et des bornes supérieures et inférieures de ces intervalles :\n\nmasses_ci |&gt; \n  ggplot(aes(x = sex, y = moyenne, fill = species)) +\n  geom_col(color = \"grey20\", alpha = 0.5) +\n  geom_errorbar(aes(ymin = ci_borne_inf,\n                    ymax = ci_borne_sup),\n                width = 0.15) +\n  facet_wrap(~species, nrow = 1) +\n  labs(x = \"\", y = \"Masse moyenne (g)\", fill = \"Espèce\") +\n  theme_bw() +\n  scale_fill_brewer(palette = \"Accent\") +\n  scale_y_continuous(labels = number_format()) +\n  scale_x_discrete(labels = c(\"Femelles\", \"Mâles\"))\n\n\n\n\nFigure 8.7: Comparison des masses moyennes observées chez les mâles et les femelles de 3 espèces de manchots. Les barres d’erreur sont les intervales de confiance à 95% des masses moyennes.\n\n\n\n\nComme vous voyez, les barres d’erreurs sont maintenant plus longues que sur la Figure 8.5. C’est normal car rappelez-vous que les intervalles de confiance sont à peu près équivalents à 2 fois les erreurs standards. L’intérêt de représenter les intervalles de confiance est qu’ils sont directement liés aux tests statistiques que nous aborderons dans les chapitres qui viennent. Globalement, quand 2 séries de données ont des intervalles de confiance qui se chevauchent largement (comme par exemple pour les mâles Adélie et Chinstrap), alors, un test d’hypothèses conclurait presque toujours à l’absence de différence significative entre les 2 groupes. À l’inverse, quand 2 séries de données ont des intervalles de confiance qui ne se chevauchent pas du tout (comme les mâles et les femelles Adélie par exemple), alors, un test d’hypothèses conclurait presque toujours à l’existence d’une différence significative entre les 2 groupes. Lorsque les intervalles de confiance entre 2 catégories se chevauchent faiblement ou partiellement (comme entre les femelles Adélie et Chinstrap), la situation est moins tranchée, et nous devrons nous en remettre aux résultats du test pour savoir si la différence observée devrait être considérée comme significative ou non."
  },
  {
    "objectID": "08-ErrorBars.html#visualiser-lincertitude-les-boîtes-à-moustaches",
    "href": "08-ErrorBars.html#visualiser-lincertitude-les-boîtes-à-moustaches",
    "title": "8  Visualiser l’incertitude et la dispersion",
    "section": "8.6 Visualiser l’incertitude : les boîtes à moustaches",
    "text": "8.6 Visualiser l’incertitude : les boîtes à moustaches\nOutre les informations de position et de dispersion, les boîtes à moustaches permettent également de visualiser l’incertitude associée aux médianes. Il suffit pour cela d’ajouter l’argument notch = TRUE dans la fonction geom_boxplot() :\n\npenguins |&gt; \n  filter(!is.na(sex)) |&gt; \n  ggplot(aes(x = sex, y = body_mass_g, fill = species)) +\n  geom_boxplot(color = \"grey20\", alpha = 0.5, notch = TRUE) +\n  facet_wrap(~species, nrow = 1) +\n  labs(x = \"Sexe\", y = \"Masse moyenne (g)\", fill = \"Espèce\") +\n  theme_bw() +\n  scale_fill_brewer(palette = \"Accent\") +\n  scale_y_continuous(labels = number_format()) +\n  scale_x_discrete(labels = c(\"Femelles\", \"Mâles\"))\n\n\n\n\nFigure 8.8: Comparison des masses corporelles des mâles et femelles de 3 espèces de manchots.\n\n\n\n\nDes encoches ont été ajoutées autour de la médiane de chaque boîte à moustache. Ces encoches sont des encoches d’incertitudes. Les limites inférieures et supérieures de ces encoches correspondent aux bornes inférieures et supérieures de l’intervalle de confiance à 95% des médianes. Comme pour les moyennes, le chevauchement ou l’absence de chevauchement entre les encoches de 2 séries de données nous renseigne sur l’issue probable des futurs tests statistiques que nous serions amenés à réaliser. Notez que tout ce que nous avons dit plus haut sur le chevauchement des intervalles de confiance des moyennes se retrouve ici pour les intervalles de confiance des médianes (i.e. large chevauchement entre les encoches des mâles Adélie et Chinstrap, absence de chevauchement entre femelles et mâles Adélie, faible chevauchement entre femelles Adélie et Chinstrap). Il sera donc important de bien examiner ces encoches en amont des tests statistiques pour éviter de faire/dire des bêtises…"
  },
  {
    "objectID": "08-ErrorBars.html#sec-ploterrbar",
    "href": "08-ErrorBars.html#sec-ploterrbar",
    "title": "8  Visualiser l’incertitude et la dispersion",
    "section": "8.7 Exercice",
    "text": "8.7 Exercice\n\nAvec le tableau penguins, calculez les grandeurs suivantes pour chaque espèce de manchot et chaque sexe :\n\n\nla moyenne de la longueur des nageoires\nla variance de la longueur des nageoires\nl’écart-type de la longueur des nageoires\nl’erreur standard de la longueur moyenne des nageoires\nla moyenne de l’épaisseur du bec\nla variance de l’épaisseur du bec\nl’écart-type de l’épaisseur du bec\nl’erreur standard de l’épaisseur du bec\n\nAttention : pensez à retirer les individus dont le sexe est inconnu.\n\nVérifiez avec la fonction skim() que les moyennes et écart-types calculés ci-dessus sont corrects.\nAvec ces données synthétiques faites le graphique suivant :\n\n\n\n\n\n\n\n\n\n\nIhaka, Ross, Paul Murrell, Kurt Hornik, Jason C. Fisher, Reto Stauffer, Claus O. Wilke, Claire D. McWhite, et Achim Zeileis. 2023. colorspace: A Toolbox for Manipulating and Assessing Colors and Palettes. https://CRAN.R-project.org/package=colorspace."
  },
  {
    "objectID": "09-OneSampleTests.html#sec-packages1",
    "href": "09-OneSampleTests.html#sec-packages1",
    "title": "9  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "9.1 Pré-requis",
    "text": "9.1 Pré-requis\nDans ce chapitre, nous abordons pour la première fois les tests d’hypothèses. Je vous conseille donc de créer un nouveau script dans votre répertoire de travail (au même endroit que votre RProject) et de réinitialiser votre session de travail en relançant R (ctrl + shift + 0 sous Windows, ou command + shift + 0 sous MacOS) ou RStudio.\nDans ce chapitre, vous aurez besoin d’utiliser des packages spécifiques et d’importer des données depuis des fichiers externes téléchargeables directement depuis ce document. Les packages dont vous aurez besoin ici et que vous devez donc charger en mémoire, sont le tidyverse et skimr déjà décrits en détail dans les chapitres précédents :\n\nlibrary(tidyverse)\nlibrary(skimr)\n\nVous aurez également besoin des jeux de données suivants :\n\nTemperature.csv\nTemperature2.csv\n\nEnfin, je spécifie ici une fois pour toutes le thème que j’utiliserai pour tous les graphiques de ce chapitre. Libre à vous de choisir un thème différent ou de vous contenter du thème proposé par défaut :\n\ntheme_set(theme_bw())\n\nCette commande permet d’éviter d’avoir à ajouter la couche theme_bw() à chaque graphique que je vais créer durant cette session de travail."
  },
  {
    "objectID": "09-OneSampleTests.html#contexte",
    "href": "09-OneSampleTests.html#contexte",
    "title": "9  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "9.2 Contexte",
    "text": "9.2 Contexte\nOn s’intéresse ici à la température corporelle des adultes en bonne santé. On souhaite examiner la croyance populaire qui veut que cette température vaut en moyenne 37ºC. Pour le vérifier, on dispose d’un échantillon de 25 adultes en bonne santé choisis au hasard parmi la population américaine et dont on a mesuré la température. Comme pour toute étude statistique, les étapes que nous allons devoir suivre sont les suivantes (dans l’ordre) :\n\nImporter les données dans RStudio, les examiner et éventuellement les (re)mettre en forme si besoin.\nFaire une première exploration des données, grâce au calcul d’indices de statistiques descriptives d’une part, et de représentations graphiques d’autre part.\nRéaliser un test d’hypothèses en respectant la procédure adéquate (en particulier, la vérification des conditions d’application).\n\nC’est donc ce que nous allons faire dans les sections suivantes.\n\n\n\n\n\n\nÀ retenir !\n\n\n\nAvant de se lancer dans les tests d’hypothèses, il est toujours indispensable d’examiner les données dont on dispose à l’aide, d’une part de statistiques descriptives numériques, et d’autres part, de graphiques exploratoires.\nNous avons vu dans les chapitres 6 et 7 quels indices statistiques il peut être utile de calculer, et dans les chapitres 3 et 8 quelles représentations graphiques il peut être utile de réaliser, afin de pouvoir se lancer dans des tests d’hypothèses sans risquer de grossières erreurs. N’hésitez pas à cliquer sur ces liens pour vous rafraîchir la mémoire !"
  },
  {
    "objectID": "09-OneSampleTests.html#importation-et-mise-en-forme-des-données",
    "href": "09-OneSampleTests.html#importation-et-mise-en-forme-des-données",
    "title": "9  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "9.3 Importation et mise en forme des données",
    "text": "9.3 Importation et mise en forme des données\nNous allons travailler ici sur les données contenues dans le fichier Temperature.csv. Téléchargez ces données dans votre répertoire de travail (attention : ne les ouvrez pas avec Excel, les données pourraient s’en trouver modifiées !), puis importez les données dans RStudio grâce à l’assistant d’importation. Si vous ne savez plus comment faire, consultez la Section 4.4.3.\nVous stockerez les données dans un objet que vous nommerez Temperature. Après l’importation, tapez son nom dans la console de RStudio et vérifiez que vous obtenez bien exactement ce résultat :\n\nTemperature\n\n# A tibble: 25 × 2\n   individual temperature\n        &lt;dbl&gt;       &lt;dbl&gt;\n 1          1        98.4\n 2          2        98.6\n 3          3        97.8\n 4          4        98.8\n 5          5        97.9\n 6          6        99  \n 7          7        98.2\n 8          8        98.8\n 9          9        98.8\n10         10        99  \n# ℹ 15 more rows\n\n\nLa première chose à faire quand on travaille avec des données inconnues, c’est d’examiner les données brutes. Ici, les données sont importées au format tibble, donc seules les premières lignes sont visibles. Pour visualiser l’ensemble du tableau, utilisez la fonction View() (avec un V majuscule) ou, si vous avez mis en mémoire le tidyverse, la fonction view() (sans majuscule) :\n\nView(Temperature)\n\nCette commande ouvre un nouvel onglet présentant les données dans un tableur simplifié, en lecture seule. On constate ici 2 choses que nous allons modifier :\n\nla première colonne, intitulée individual, n’est pas véritablement une variable. Cette colonne ne contient qu’un identifiant sans intérêt pour notre étude et est en fait identique au numéro de ligne. Nous allons donc supprimer cette colonne.\nles températures sont exprimées en degrés Fahrenheit, ce qui rend leur lecture difficile pour nous qui sommes habitués à utiliser le système métrique et les degrés Celsius. Grâce à la fonction mutate() décrite à la Section 5.6, nous allons donc convertir les températures en degrés Celsius grâce à la formule que nous avons déjà vue dans les chapitres précédents :\n\n\\[ºC = \\frac{ºF - 32}{1.8}\\]\n\nTemp_clean &lt;- Temperature |&gt;\n  select(-individual) |&gt;      # Suppression de la colonne `individual`\n  mutate(                     # Transformation des températures en ºCelsius\n    temperature = (temperature - 32) / 1.8\n    )\n\nTemp_clean\n\n# A tibble: 25 × 1\n   temperature\n         &lt;dbl&gt;\n 1        36.9\n 2        37  \n 3        36.6\n 4        37.1\n 5        36.6\n 6        37.2\n 7        36.8\n 8        37.1\n 9        37.1\n10        37.2\n# ℹ 15 more rows\n\n\nIl nous est maintenant possible d’examiner à nouveau les données avec la fonction View(). Avec des valeurs de températures comprises entre 36.3ºC et 37.8ºC, il n’y a visiblement pas de données aberrantes.\nExaminer les données brutes est donc la première chose que vous devriez prendre l’habitude de faire, et ce de façon systématique, car cela permet de repérer :\n\nLa nature des variables présentes .\nLes variables inutiles qui pourront être supprimées ou négligées.\nLes unités des variables utiles, afin de pouvoir les convertir si nécessaire.\nLes valeurs manquantes, atypiques ou aberrantes qui demanderont toujours une attention particulière.\n\nMaintenant que l’examen préliminaire des données est réalisé, on peut passer au calcul des statistiques descriptives."
  },
  {
    "objectID": "09-OneSampleTests.html#sec-eda",
    "href": "09-OneSampleTests.html#sec-eda",
    "title": "9  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "9.4 Exploration statistique des données",
    "text": "9.4 Exploration statistique des données\n\n9.4.1 Position et dispersion\nOn s’intéresse ici au calcul de grandeurs statistiques nous apportant des renseignements sur les paramètres de position et de dispersion de la population. On calcule donc pour cela des estimateurs de ces paramètres grâce à notre échantillon. Les questions auxquelles on tente de répondre à ce stade sont les suivantes :\n\nQuelle est la tendance centrale (moyenne ou médiane) ?\nQuelle est la dispersion des valeurs autour de la tendance centrale (écart-type, variance, intervalle interquartile…) ?\n\nPour répondre à ces questions, on peut faire appel à de multiples fonctions déjà présentées dans le Chapitre 6. Par exemple les fonctions summarise() et reframe(), en conjonction avec les fonctions mean(), median(), sd(), var(), min(), max() ou quantile(), ou les fonctions summary() ou skim() (du package skimr).\nJe prends ici un exemple simple, mais n’hésitez pas à expérimenter avec les méthodes décrites dans le Chapitre 6.\n\nsummary(Temp_clean)\n\n  temperature   \n Min.   :36.33  \n 1st Qu.:36.67  \n Median :37.00  \n Mean   :36.96  \n 3rd Qu.:37.22  \n Max.   :37.78  \n\n\nOn constate ici que la moyenne et la médiane sont très proches. La distribution des températures doit donc être à peut près symétrique, avec à peu près autant de valeurs au-dessus que de valeurs en dessous de la moyenne. Les premier et troisième quartiles sont à peu près aussi éloignés de la médiane l’un que l’autre, ce qui confirme l’apparente symétrie du jeu de données de part et d’autre de la tendance centrale.\nLa moyenne observée dans l’échantillon vaut 36.96ºC, ce qui est très proche de la moyenne théorique de 37ºC.\nUne autre fonction utile est la fonction IQR(), qui renvoie l’étendue de l’intervalle interquartile (c’est-à-dire la valeur du troisième quartile moins la valeur de premier quartile) :\n\nTemp_clean |&gt;\n  summarise(IQ_range = IQR(temperature))\n\n# A tibble: 1 × 1\n  IQ_range\n     &lt;dbl&gt;\n1    0.556\n\n\nOn constate ici que l’intervalle interquartile a une largeur de 0.56ºC. Cela signifie que les 50% des températures les plus centrales de l’échantillon sont situées dans un intervalle d’environ un demi-degré Celsius autour de la médiane.\nEnfin, pour obtenir des informations complémentaires, on peut utiliser la fonction skim() du package skimr :\n\nskim(Temp_clean)\n\n── Data Summary ────────────────────────\n                           Values    \nName                       Temp_clean\nNumber of rows             25        \nNumber of columns          1         \n_______________________              \nColumn type frequency:               \n  numeric                  1         \n________________________             \nGroup variables            None      \n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate mean    sd   p0  p25 p50  p75 p100 hist \n1 temperature           0             1 37.0 0.377 36.3 36.7  37 37.2 37.8 ▇▇▇▇▂\n\n\nTout comme summary(), la fonction skim() renvoie les valeurs minimales et maximales, les premiers et troisièmes quartiles ainsi que la moyenne et la médiane. Elle nous indique en outre la valeur de l’écart-type de l’échantillon, ainsi que le nombre d’observations et le nombre de données manquantes. Enfin, elle fournit un histogramme très simplifié et sans échelle. Cet histogramme nous permet de nous faire une première idée de la distribution des données et est particulièrement utile pour comparer rapidement un grand nombre de distributions quand il y a plusieurs catégories dans les données (ce qui n’est pas le cas ici).\nOutre ces 3 fonctions (summary(), IQR(), et skim()), il est bien sûr possible de calculer toutes ces valeurs manuellement si besoin :\n\nmean() permet de calculer la moyenne.\nmedian() permet de calculer la médiane.\nmin() et max() permettent de calculer les valeurs minimales et maximales respectivement.\nquantile() permet de calculer les quartiles.\nsd() permet de calculer l’écart-type.\nvar() permet de calculer la variance.\nn() permet de compter le nombre d’observations.\n\nToutes ces fonctions prennent seulement un vecteur en guise d’argument. Il faut donc procéder comme avec IQR() pour les utiliser, en les intégrant à l’intérieur de la fonction summarise(). Par exemple, pour calculer la variance, on peut taper :\n\nTemp_clean |&gt; \n  summarise(variance = var(temperature))\n\n# A tibble: 1 × 1\n  variance\n     &lt;dbl&gt;\n1    0.142\n\n\nou :\n\nTemp_clean |&gt;\n  pull(temperature) |&gt;\n  var()\n\n[1] 0.1417901\n\n\nou encore :\n\nvar(Temp_clean$temperature)\n\n[1] 0.1417901\n\n\nÀ vous d’utiliser la syntaxe qui vous semble la plus simple.\n\n\n9.4.2 Incertitude\nOutre les informations de position et de dispersion, nous avons vu au semestre 4 qu’il était également important d’avoir une idée de l’incertitude associée aux estimations de tendance centrale (erreur standard ou intervalle de confiance de la moyenne ou médiane). Ici, nous allons donc calculer l’intervalle de confiance à 95% de la moyenne. Si vous ne savez plus comment faire, ou que vous ne comprenez pas le code ci-dessous, consultez le livre en ligne du semestre 4 :\n\nTemp_clean |&gt; \n  reframe(mean_cl_normal(temperature))\n\n# A tibble: 1 × 3\n      y  ymin  ymax\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  37.0  36.8  37.1\n\n\nOn constate ici que les bornes inférieure (36.8ºC) et supérieure (37.1ºC) de l’intervalle de confiance à 95% de la moyenne sont proches de la valeur de moyenne de l’échantillon. Dans la population générale, la moyenne de la température corporelle chez les adultes en bonne santé a de bonnes chances de se trouver quelque part entre 36.8ºC et 37.1ºC. Autrement dit, si la température corporelle des adultes en bonne santé n’est pas exactement de 37ºC, l’écart à cette valeur théorique ne doit pas être très important."
  },
  {
    "objectID": "09-OneSampleTests.html#sec-edagraph",
    "href": "09-OneSampleTests.html#sec-edagraph",
    "title": "9  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "9.5 Exploration graphique des données",
    "text": "9.5 Exploration graphique des données\nIci, puisque nous ne disposons que d’une unique variable numérique et que nous n’avons donc qu’un unique groupe, les représentations graphiques que nous allons réaliser doivent nous permettre d’examiner la distribution des données. Pour cela, nous pouvons réaliser soit un histogramme, soit un diagramme de densité.\n\n9.5.1 Histogramme\nVoilà comment produire un histogramme de qualité pour les données de températures :\n\nTemp_clean |&gt;\n  ggplot(aes(x = temperature)) +\n  geom_histogram(bins = 10, fill = \"firebrick2\", color = \"grey20\", \n                 alpha = 0.5) +\n  geom_rug() +\n  labs(x = \"Température (ºC)\",\n       y = \"Fréquence\",\n       title = \"Distribution des températures corporelles\",\n       subtitle = \"n = 25 adultes en bonne santé\")\n\n\n\n\nSi vous ne vous rappelez-plus ce qu’est un histogramme ou comment le faire, ou la signification de l’argument bins, relisez la section consacrée aux histogrammes du livre en ligne de Biométrie du semestre 3. Notez que j’ai ajouté une couleur de remplissage et de la transparence pour rendre le graphique plus facile à lire. J’ai également spécifié des titres pour les axes (en précisant l’unité de la variable numérique dont on représente la distribution) ainsi que le titre (et sous-titre) du graphique, qui précise ce qu’on a sous les yeux et la taille de l’échantillon. Il n’est pas toujours nécessaire de spécifier le titre (et le sous-titre) de cette façon : lorsque vous intégrez des graphiques dans un compte-rendu ou un rapport, le titre est en général précisé sous la figure, au début d’une légende qui la décrit. Enfin, j’ai ajouté geom_rug() pour faire apparaître sous le graphique, le long de l’axe des x, la position des données observées. Cela permet de visualiser les données brutes, et peut donc permettre de mieux comprendre pourquoi un histogramme présente telle ou telle forme.\nIci, la forme de ce l’histogramme est assez proche de celle présentée plus tôt par l’histogramme très simplifié produit par la fonction skim(). Cet histogramme nous apprend qu’en dehors d’un “trou” autour de la température 36.75ºC, la distribution des données est proche d’une courbe en cloche. Il y a fort à parier qu’un test de normalité conclurait à la normalité des données de cet échantillon. C’est ce que nous verrons dans la Section 9.6.1.\n\n\n9.5.2 Diagramme de densité\nUne autre façon de visualiser la distribution d’une variable numérique est de produire un graphique de densité. Il a l’avantage d’éviter à l’utilisateur d’avoir à choisir une valeur pour l’argument bin de la fonction geom_histogram(), mais il a l’inconvénient de présenter une échelle plus difficile à comprendre pour l’axe des ordonnées :\n\nTemp_clean |&gt;\n  ggplot(aes(x = temperature)) +\n  geom_density(fill = \"firebrick2\", alpha = 0.5) +\n  geom_rug() +\n  labs(x = \"Température (ºC)\",\n       y = \"Densité\",\n       title = \"Distribution des températures corporelles\",\n       subtitle = \"n = 25 adultes en bonne santé\")\n\n\n\n\nLes informations apportées par ce graphique sont cohérentes avec celle de l’histogramme :\n\nles températures les plus fréquemment observées dans notre échantillon de 25 adultes en bonne santé se situent légèrement au dessus de 37ºC. Il s’agit d’une information concernant la position des données (c’est-à-dire où se trouve le pic de la distribution sur l’axe des x)\nles températures observées ont une distribution qui ressemble à peu près à une courbe en cloche, avec des valeurs comprises entre 36.4ºC et 37.8ºC environ. La symétrie de part et d’autre du pic n’est pas parfaite, mais elle reste bonne. Il s’agit d’informations concernant la forme de la distribution et la dispersion des données.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBilan des analyses préliminaires\n\n\n\nSuite à l’exploration statistique et graphique des données de températures, voilà ce qu’on retient :\n\nIl n’y a visiblement pas de données aberrantes.\nLa distribution des données semble suivre à peu près la loi Normale.\nLa médiane et la moyenne sont très proches de 37ºC. Un test devrait donc arriver à la conclusion que la température corporelle des adultes n’est pas significativement différente de 37ºC.\nLa largeur de l’intervalle de confiance à 95% semble faible, ce qui indique une incertitude relativement faible. Si la température réelle des adultes en bonne santé n’est pas exactement de 37ºC, elle ne devrait pas en être très éloignée (quelques dixièmes de degrés Celsuis au plus)."
  },
  {
    "objectID": "09-OneSampleTests.html#le-test-paramétrique",
    "href": "09-OneSampleTests.html#le-test-paramétrique",
    "title": "9  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "9.6 Le test paramétrique",
    "text": "9.6 Le test paramétrique\nLe test permettant de comparer la moyenne \\(\\mu\\) d’une population à une valeur théorique, fixée par l’utilisateur, est le test de Student à un échantillon. Il permet de répondre à la question suivante :\n\nLes données observés dans l’échantillon dont je dispose sont-elles compatibles avec l’hypothèse que la moyenne \\(\\mu\\) de la population dont est issu mon échantillon vaut XXX ?\n\navec XXX, une valeur d’intérêt spécifiée par l’utilisateur. Il s’agit d’un test paramétrique très puissant. Comme tous les tests paramétriques, certaines conditions d’application doivent être vérifiées avant de pouvoir l’appliquer.\n\n\n\n\n\n\nImportant\n\n\n\nComme pour tous les tests statistiques que nous allons réaliser lors de ces séances de TP et TEA, nous devrons commencer par spécifier les hypothèses nulles et alternatives de chaque test, ainsi que la valeur du seuil \\(\\alpha\\) que nous allons utiliser. À moins d’avoir une bonne raison de faire autrement, on utilise presque toujours le seuil \\(\\alpha = 0.05\\) dans le domaine des sciences du vivant. C’est donc ce seuil que nous utiliserons dans ce livre en ligne.\n\n\n\n9.6.1 Conditions d’application\nLes conditions d’application du test de Student à un échantillon sont les suivantes :\n\nLes données de l’échantillon sont issues d’un échantillonnage aléatoire au sein de la population générale. Cette condition est partagée par toutes les méthodes que nous verrons dans ces TP. En l’absence d’informations sur la façon dont l’échantillonnage a été réalisé, on considère que cette condition est remplie. Il n’y a pas de moyen statistique de le vérifier, cela fait uniquement référence à la stratégie d’échantillonnage déployée et à la rigueur de la procédure mise en œuvre lors de l’acquisition des données.\nLa variable étudiée doit suivre une distribution Normale dans la population générale. Nous allons vérifier cette condition d’application avec un test de normalité de Shapiro-Wilk.\n\nPour un test de normalité, les hypothèses seront toujours les suivantes :\n\nH\\(_0\\) : la variable étudiée suit une distribution Normale dans la population générale.\nH\\(_1\\) : la variable étudiée ne suit pas une distribution Normale dans la population générale.\n\nLe test de Shapiro-Wilk se réalise de la façon suivante :\n\nshapiro.test(Temp_clean$temperature)\n\nou\n\nTemp_clean |&gt;\n  pull(temperature) |&gt;\n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  pull(Temp_clean, temperature)\nW = 0.97216, p-value = 0.7001\n\n\nla fonction pull() permet d’extraire une colonne (ici temperature) d’un tibble (ici Temp_clean) et de la transformer en vecteur.\nW est la statistique du test. Elle permet à RStudio de calculer la p-value. Ici, \\(p &gt; \\alpha\\). On ne peut donc pas rejeter l’hypothèse nulle de normalité : on ne peut pas exclure que dans la population générale, la température suive bel et bien une distribution Normale. Les conditions d’application du test de Student sont bien vérifiées.\n\n\n\n\n\n\nTests et décision : rappel de cours\n\n\n\nÀ l’issue d’un tests statistique, la décision finale est toujours prise par rapport à l’hypothèse nulle (\\(H_0\\)) :\n\nSi la \\(p-\\)value du test est supérieure ou égale à \\(\\alpha\\), on dit qu’on ne peut pas rejeter l’hypothèse nulle \\(H_0\\). Attention, on ne dit jamais que “\\(H_0\\) est vraie”, car il est impossible de le vérifier avec une certitude absolue. Toutefois, les données observées (celles de notre échantillon), sont compatibles avec l’hypothèse nulle que nous avons formulée, jusqu’à preuve du contraire.\nSi la \\(p-\\)value du test est inférieure à \\(\\alpha\\), on dit qu’on rejette l’hypothèse nulle au seuil \\(\\alpha\\). Autrement dit, les données observées ne sont pas compatibles avec l’hypothèse nulle. On accepte alors l’hypothèse alternative (\\(H_A\\)).\n\nL’hypothèse nulle est toujours l’hypothèse la moins “intéressante”, celle pour laquelle “il ne se passe rien de notable” (par exemple : “les données suivent la distribution Normale”, ou “les moyennes sont égales”).\n\n\n\n\n9.6.2 Signification de la \\(p-\\)value\nLa \\(p-\\)value est une grandeur centrale en statistiques et elle est souvent mal comprise et donc mal interprétée. Je prends donc le temps ici d’expliquer ce qu’est la \\(p-\\)value et comment il faut la comprendre.\n\n\n\n\n\n\nDéfinition : la \\(p-\\)value\n\n\n\nLa \\(p-\\)value d’un test statistique, c’est la probabilité, si \\(H_0\\) est vraie, d’obtenir un effet au moins aussi extrême que celui qu’on a observé dans l’échantillon, sous le seul effet du hasard.\n\n\nIci, la \\(p-\\)value de notre test de Normalité de Shapiro-Wilk vaut 0.7101. Cela signifie que si les données suivent réellement la loi Normale dans la population générale (donc si \\(H_0\\) est vraie), l’écart à la Normalité que nous avons observé (ou un écart encore plus important), peut être observé dans 70.1% des cas. Autrement dit, si on prélève un grand nombre d’échantillons de 25 adultes dans la population générale et qu’on regarde à quoi ressemble la distribution des températures dans chacun de ces échantillons, pour 70.1% d’entre eux, la distribution obtenue sera au moins aussi éloignée de la distribution Normale que celle que nous avons observée ici.\nDans notre cas, l’écart entre la loi Normale et les données de notre échantillon peut être visualisé de la façon suivante :\n\n\n\n\n\nLa courbe de densité des données observées est en rouge, et la distribution Normale théorique correspond à la courbe en bleu. Il y a donc un écart entre la courbe en cloche parfaite de la loi Normale et les données observées. La \\(p-\\)value du test de Shapiro-Wilk nous dit que si la température des adultes en bonne santé suit réellement la loi Normale dans la population générale, alors, l’écart que nous avons observé, ou un écart encore plus important, peut être observé simplement par hasard dans 70.1% des cas. Autrement dit, c’est très probable, et on peut donc considérer que l’écart à la loi Normale que nous avons observé est le fruit du hasard et que notre variable suit donc bien la Loi Normale.\nPour bien comprendre cette notion importante, je simule ci-dessous 36 échantillons de 25 adultes dont les températures suivent parfaitement la loi Normale dans la population générale. Je me place donc dans la situation ou je sais que \\(H_0\\) est vraie, pour illustrer la notion de fluctuation d’échantillonnage. En raison du seul hasard de l’échantillonnage, et alors même que les échantillons que je génère sont issus d’une population qui suit parfaitement la Normale, la distribution dans chaque échantillon s’écarte parfois fortement de la courbe en cloche théorique :\n\n\n\n\n\nOn voit bien ici que certains échantillons s’écartent fortement de la distribution théorique alors même que tous les échantillons sont issus d’une population Normale. Et plus l’échantillon sera de taille réduite, plus les écarts à la courbe en cloche parfaite seront grands. La preuve ci-dessous avec des échantillons de n = 15 adultes au lieu de 25 :\n\n\n\n\n\nAu final, la \\(p-\\)value de 0.701 de notre test de Shapiro-Wilk nous indique que l’hypothèse de la Normalité n’est pas incompatible avec les données que nous avons observées.\nImaginons qu’à l’inverse, nous ayons obtenu une \\(p-\\)value très faible, égale à 0.01 par exemple (donc inférieure à notre seuil \\(\\alpha\\) de 0.05). Nous aurions alors rejeté l’hypothèse nulle. En effet, obtenir une \\(p-\\)value de 0.01, signifie que si \\(H_0\\) est vraie, obtenir un écart à la courbe en cloche théorique aussi important que celui que nous observons est très peu probable (une chance sur 100). Puisqu’il est très improbable d’observer un tel écart si \\(H_0\\) est vraie, on en conclu que \\(H_0\\) n’est pas vraie : les données sont incompatibles avec l’hypothèse nulle et on la rejette donc logiquement.\nCette logique sera valable pour tous les autres tests statistiques que nous aborderons dans cet ouvrage. Pour un test de Normalité, on regarde l’écart entre la distribution Normale et les données observées. Pour un test de comparaison de moyennes, on regarde l’écart entre la moyenne théorique et la moyenne observée, ou entre les 2 moyennes qu’on essaie de comparer. Mais la philosophie reste la même.\n\n\n9.6.3 Réalisation du test de Student et interprétation\nPuisque les conditions d’application du test de Student à un échantillon sont vérifiées, nous avons le droit de faire ce test, et nous devons donc maintenant spécifier les hypothèses nulles et alternatives que nous allons utiliser pour le réaliser :\n\nH\\(_0\\) : dans la population générale, la température corporelle moyenne des adultes en bonne santé vaut 37ºC (\\(\\mu = 37\\)).\nH\\(_1\\) : dans la population générale, la température corporelle moyenne des adultes en bonne santé est différente de 37ºC (\\(\\mu \\neq 37\\)).\n\n\n\n\n\n\n\nHypothèses et paramètres\n\n\n\nNotez que les hypothèses des tests statistiques concernent toujours la valeur d’un paramètre de la population générale, et non la valeur des estimateurs calculés dans un échantillon.\n\n\nOn réalise ensuite le test de la façon suivante :\n\nt.test(Temp_clean$temperature, mu = 37)\n\nou\n\nt.test(temperature ~ 1, mu = 37, data = Temp_clean)\n\nou encore,\n\nTemp_clean |&gt;\n  pull(temperature) |&gt;\n  t.test(mu = 37)\n\n\n    One Sample t-test\n\ndata:  pull(Temp_clean, temperature)\nt = -0.56065, df = 24, p-value = 0.5802\nalternative hypothesis: true mean is not equal to 37\n95 percent confidence interval:\n 36.80235 37.11321\nsample estimates:\nmean of x \n 36.95778 \n\n\nLes résultats fournis ont une forme particulière qui est utilisée par de nombreuses fonctions de tests statistiques dans R. Ils méritent donc qu’on s’y attarde un peu.\nSur la première ligne, R nous confirme que nous avons bien réalisé un test de Student à un échantillon. La première ligne de résultats fournit la valeur du \\(t\\) calculé (ici, -0.56), le nombre de degrés de libertés (ici, df = 24), et la \\(p-\\)value (ici, 0.58, soit une valeur supérieure à \\(\\alpha\\)). Cette première ligne contient donc tous les résultats du test qu’il conviendrait de rappeler dans un rapport. On devrait ainsi dire :\n\nAu seuil \\(\\alpha\\) de 5%, le test de Student ne permet pas rejeter l’hypothèse nulle \\(\\mu = 37\\) (\\(t = -0.56\\), ddl = 24, \\(p = 0.58\\)). Les données observées sont donc compatibles avec l’hypothèse selon laquelle la température corporelle moyenne des adultes en bonne santé vaut 37ºC.\n\nC’est de cette manière que vous devriez rapporter les résultats de ce test dans un compte-rendu ou un rapport à partir de maintenant.\nDans les résultats du test, la ligne suivante (alternative hypothesis: ...) ne donne pas la conclusion du test. Il s’agit simplement d’un rappel concernant l’hypothèse alternative qui a été utilisée pour réaliser le test. Ici, l’hypothèse alternative utilisée est une hypothèse bilatérale (\\(\\mu \\neq 37\\)). Nous verrons plus tard comment spécifier des hypothèses alternatives uni-latérales, même si la plupart du temps, mieux vaut s’abstenir de réaliser de tels tests (à moins bien sûr d’avoir une bonne raison de le faire).\nLes résultats fournis ensuite concernent, non plus le test statistique à proprement parler, mais l’estimation. Ici, la moyenne de l’échantillon est fournie. Il s’agit de la meilleure estimation possible de la moyenne de la population : \\(\\bar{x} = \\hat{\\mu} = 36.96\\). Comme pour toutes les estimations, cette valeur est entachée d’incertitude liée à la fluctuation d’échantillonnage. L’intervalle de confiance à 95% de cette estimation de moyenne est donc également fourni : \\([36.80 ; 37.11]\\). Vous notez qu’il s’agit des mêmes valeurs que celles que nous avions calculées dans la Section 9.4.2. Autrement dit, cet intervalle contient les valeurs les plus vraisemblables pour la véritable valeur de moyenne dans la population générale. Cela confirme bien que nous n’avons pas prouvé au sens strict que la moyenne de la population vaut 37ºC. Nous avons en réalité montré que nous ne pouvions pas exclure que la moyenne de la population générale soit de 37ºC. Puisque cette valeur est comprise dans l’intervalle de confiance, on ne peut donc pas l’exclure : nos données sont compatibles avec cette hypothèse. Mais beaucoup d’autres valeurs figurent aussi dans cet intervalle. Il est donc tout à fait possible que la moyenne soit en réalité différente de 37ºC (par exemple, 36.9ºC). Pour en être sûr, il faudrait probablement un échantillon de plus grande taille afin de limiter l’incertitude, d’augmenter la puissance statistique de notre test, et ainsi d’être en mesure de détecter des différences subtiles."
  },
  {
    "objectID": "09-OneSampleTests.html#lalternative-non-paramétrique",
    "href": "09-OneSampleTests.html#lalternative-non-paramétrique",
    "title": "9  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "9.7 L’alternative non paramétrique",
    "text": "9.7 L’alternative non paramétrique\nSi jamais les conditions d’application du test de Student à un échantillon n’étaient pas remplies, il faudrait alors réaliser son équivalent non paramétrique : le test de Wilcoxon des rangs signés. Ce test est moins puissant que son homologue paramétrique. On ne l’effectue donc que lorsque l’on n’a pas le choix :\n\nwilcox.test(Temp_clean$temperature, mu = 37, conf.int = TRUE)\n\nou\n\nwilcox.test(temperature ~ 1, mu = 37, conf.int = TRUE, data = Temp_clean)\n\nou encore\n\nTemp_clean |&gt;\n  pull(temperature) |&gt;\n  wilcox.test(mu = 37, conf.int = TRUE)\n\nWarning in wilcox.test.default(pull(Temp_clean, temperature), mu = 37, conf.int\n= TRUE): impossible de calculer la p-value exacte avec des ex-aequos\n\n\nWarning in wilcox.test.default(pull(Temp_clean, temperature), mu = 37, conf.int\n= TRUE): impossible de calculer un intervalle de confiance exact avec des\nex-aequos\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  pull(Temp_clean, temperature)\nV = 143, p-value = 0.6077\nalternative hypothesis: true location is not equal to 37\n95 percent confidence interval:\n 36.77780 37.11114\nsample estimates:\n(pseudo)median \n      36.94446 \n\n\nLa syntaxe est identique à celle du test de Student à un échantillon à une exception près : l’ajout de l’argument conf.int = TRUE qui permet d’afficher la (pseudo)médiane de l’échantillon et son intervalle de confiance à 95%.\nLes hypothèses nulles et alternatives de ce test sont les mêmes que celles du test de Student à un échantillon. En toute rigueur, on compare la médiane à une valeur théorique, et non la moyenne. Mais dans la pratique, la grande majorité des utilisateurs de ce test font l’amalgame entre moyenne et médiane. Ici, la conclusion correcte devrait donc être :\n\nAu seuil \\(\\alpha\\) de 5%, on ne peut pas rejeter l’hypothèse nulle (test de Wilcoxon des rangs signés, \\(V\\) = 143, \\(p\\) = 0.6077). La médiane de la population (\\(\\widehat{med}\\) = 36.94) n’est pas significativement différente de 37ºC (IC 95% : \\([36.78 ; 37.11]\\)).\n\nSi les données ne suivent pas la loi Normale, la médiane est bien la métrique la plus intéressante puisque c’est elle qui nous renseigne sur la tendance centrale des données.\nEnfin, les tests de Wilcoxon renvoient souvent des messages d’avertissement. Il ne s’agit que de ça : des avertissements. Tant que la \\(p\\)-value d’un test est éloignée de la valeur seuil \\(\\alpha\\), cela n’a pas d’importance. Quand en revanche la \\(p\\)-value est très proche de \\(\\alpha\\), les messages d’avertissement doivent vous alerter : il faut être très prudent face aux conclusions du test qui peuvent alors être assez “fragiles”."
  },
  {
    "objectID": "09-OneSampleTests.html#sec-puiss",
    "href": "09-OneSampleTests.html#sec-puiss",
    "title": "9  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "9.8 Les notions d’erreur et de puissance statistique",
    "text": "9.8 Les notions d’erreur et de puissance statistique\nPour avoir le droit de réaliser un test paramétrique, il faut au préalable vérifier qu’un certain nombre de conditions sont vérifiées. Si ce n’est pas le cas, on réalise un équivalent non paramétrique. On peut alors se demander pourquoi ne pas se contenter de faire des tests non paramétrique systématiquement, sans s’embêter à faire des tests supplémentaires ou des tests paramétriques.\nLa raison est simple et elle est liée aux notions d’erreur et de puissance statistique.\n\n\n\n\n\n\nDéfinitions\n\n\n\n\nErreur de type I : notée \\(\\alpha\\), c’est la probabilité de rejeter à tort l’hypothèse nulle. C’est donc la probabilité de rejeter \\(H_0\\) alors qu’elle est vraie.\nErreur de type II : notée \\(\\beta\\), c’est la probabilité d’accepter à tort l’hypothèse nulle. C’est donc la probabilité d’accepter \\(H_0\\) alors qu’elle est fausse.\nPuissance statistique : notée 1 - \\(\\beta\\)), c’est la probabilité de rejeter l’hypothèse nulle à raison. C’est donc la probabilité de rejeter \\(H_0\\) quand elle est réellement fausse.\n\n\n\nÀ chaque fois que l’on réalise un test statistique, on commet nécessairement les 2 types d’erreurs \\(\\alpha\\) et \\(\\beta\\). On souhaite évidemment minimiser les erreurs, mais on ne peut malheureusement pas faire baisser les 2 en même temps. Faire baisser \\(\\alpha\\) (pour diminuer les faux positifs) conduit toujours à augmenter \\(\\beta\\) (les faux négatifs). Faire baisser \\(\\alpha\\) revient en effet à accepter plus souvent l’hypothèse nulle quand elle est vraie. Cela conduit inévitablement accepter aussi plus souvent l’hypothèse nulle quand elle est fausse (et donc, à augmenter les faux négatifs).\nPour bien comprendre l’enjeu associé à ces erreurs, prenons l’exemple de notre système judiciaire. Lorsqu’un accusé est jugé, il est présumé innocent jusqu’à preuve du contraire. Le procès est l’équivalent d’un test statistique, avec :\n\n\\(H_0\\) : l’accusé est innocent\n\\(H_1\\) : l’accusé est coupable\n\nCommettre une erreur de type I revient à condamner à tort l’accusé (on rejette à tort \\(H_0\\)), donc on condamne un innocent. À l’inverse, commettre une erreur de type II revient à libérer un coupable (accepter à tort \\(H_0\\)). Un système de justice plus strict condamnera un plus grand nombre d’accusés, qu’ils soient coupables ou non. Un système plus strict fera donc augmenter l’erreur de type I et baisser l’erreur de type II. À vous de voir ce que vous préférez : libérer plus de coupables, ou condamner plus d’innocents ?\nEn statistiques, la question est tranchée puisqu’on préfère maintenir l’erreur de type I à un niveau assez faible (à 5% ou moins), quitte à laisser augmenter l’erreur de type II (qui est considérée comme acceptable jusqu’à 20% environ). Toutefois, seule l’erreur de type I est sous notre contrôle. En effet, c’est nous qui la choisissons lorsque l’on fixe le seuil \\(\\alpha\\) de nos tests statistiques.\n\n\n\n\n\n\nÀ retenir\n\n\n\nC’est vous qui fixez l’erreur de type I lorsque vous faites un test statistique. L’erreur de type I est le seuil \\(\\alpha\\) du test, que l’on fixe en général à 0,05 (soit 5%) dans le domaine des sciences du vivant.\n\n\nUne fois que le seuil \\(\\alpha\\) est fixé, l’erreur \\(\\beta\\) l’est aussi dans une certaine mesure. Mais on ne peut la connaitre avec précision car elle dépend de beaucoup de choses, notamment la taille des échantillons dont on dispose, la variabilité des données, le type de test réalisé, etc. En général, plus la taille de l’échantillon sera grande, plus l’erreur \\(\\beta\\) sera faible, et donc plus la puissance sera élevée. De même, par rapport aux tests non paramétriques, les tests paramétriques permettent de minimiser l’erreur \\(\\beta\\) et donc d’augmenter la puissance.\nPuisque la puissance statistique vaut \\(1 - \\beta\\), cela revient à dire que les tests paramétriques sont plus puissants que les tests non paramétriques (parfois, beaucoup plus). Au contraire des erreurs de type I et II, la puissance est une grandeur que l’on souhaite maximiser. On aimerait en effet être capables de systématiquement rejeter \\(H_0\\) quand elle est fausse. Nous avons vu plus haut que c’est hélas impossible. Mais choisir le bon test et la bonne procédure statistique permettent néanmoins d’augmenter la puissance, jusqu’à un certain point. C’est la raison pour laquelle on réalisera toujours un test paramétrique si les données dont on dispose le permettent (donc si les conditions d’application des tests paramétriques sont respectées). Et ce n’est qu’en dernier recours qu’on se tournera vers les tests non paramétriques, toujours moins puissants.\n\n\n\n\n\n\nImportant\n\n\n\nUn test paramétrique est toujours plus puissant que ses homologues non paramétriques. Avec un test paramétrique, il est donc plus probable de rejeter \\(H_0\\) à raison qu’avec un test non paramétrique."
  },
  {
    "objectID": "09-OneSampleTests.html#bilan",
    "href": "09-OneSampleTests.html#bilan",
    "title": "9  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "9.9 Bilan",
    "text": "9.9 Bilan\nNous avons vu dans ce chapitre quelle est la procédure à suivre pour réaliser un test de comparaison de la moyenne d’une population à une valeur théorique :\n\nexamen préliminaire des données\ncalcul de statistiques descriptives\ncréation de graphiques exploratoires\nvérification des conditions d’application du test paramétrique\nréalisation du test paramétrique ou non paramétrique selon l’issue de l’étape 4\n\nMais nous avons aussi abordé des notions statistiques essentielles pour la suite :\n\nLes ingrédients indispensables pour réaliser un test statistique (les hypothèses nulle et alternative, la statistique du test et le seuil \\(\\alpha\\)).\nLa \\(p-\\)value et la décision du test.\nLes erreurs de type I (\\(\\alpha\\)) et II (\\(\\beta\\)).\nLa puissance statistique (1 - \\(\\beta\\)) qui n’a rien à voir avec la notion de précision.\nLa notion de test paramétrique ou non paramétrique.\n\nAssurez-vous d’avoir les idées claires sur toutes ces notion car elles sont absolument centrales pour ne pas faire/dire de bêtises lorsque l’on analyse des données."
  },
  {
    "objectID": "09-OneSampleTests.html#exercice-dapplication",
    "href": "09-OneSampleTests.html#exercice-dapplication",
    "title": "9  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "9.10 Exercice d’application",
    "text": "9.10 Exercice d’application\nLe fichier Temperature2.csv contient les données brutes d’une seconde étude similaire, réalisée à plus grande échelle. Importez ces données et analysez-les afin de vérifier si la température corporelle moyenne des adultes en bonne santé vaut bien 37ºC. Comme toujours, avant de vous lancer dans la réalisation des tests statistiques, prenez le temps d’examiner vos données comme nous l’avons décrit dans la Section 9.4 et la Section 9.5, afin de savoir où vous allez, et de repérer les éventuelles données manquantes ou aberrantes. Enfin, interprétez les résultats à la lumière des notions que nous avons abordées ici (en particulier la notion de puissance statistique)."
  },
  {
    "objectID": "10-TwoSamplePairedTests.html#sec-packages2",
    "href": "10-TwoSamplePairedTests.html#sec-packages2",
    "title": "10  Comparaison de moyennes : deux échantillons appariés",
    "section": "10.1 Pré-requis",
    "text": "10.1 Pré-requis\nPour ce nouveau chapitre, je vous conseille de travailler dans un nouveau script que vous placerez dans votre répertoire de travail, et dans une nouvelle session de travail (Menu Session &gt; Restart R). Inutile en revanche de créer un nouveau Rproject : vos pouvez tout à fait avoir plusieurs script dans le même répertoire de travail et pour un même Rproject. Comme toujours, consultez le livre en ligne du semestre 3 si vous ne savez plus comment faire.\nSi vous êtes dans une nouvelle session de travail (ou que vous avez quitté puis relancé RStudio), vous devrez penser à recharger en mémoire les packages utiles. Dans ce chapitre, vous aurez besoin d’utiliser les mêmes packages que précédemment :\n\nle tidyverse (Wickham 2023), qui comprend notamment le package readr (Wickham, Hester, et Bryan 2023), pour importer facilement des fichiers .csv au format tibble, le package dplyr (Wickham et al. 2023), pour manipuler des tableaux, et le package ggplot2 (Wickham et al. 2024) pour les représentations graphiques. \nskimr (Waring et al. 2022), qui permet de calculer des résumés de données très informatifs.  \n\n\nlibrary(tidyverse)\nlibrary(skimr)\n\nVous aurez également besoin des jeux de données suivants que vous pouvez dès maintenant télécharger dans votre répertoire de travail :\n\nAutruches.csv\nTestosterone.csv  \n\nEnfin, je spécifie ici une fois pour toutes le thème que j’utiliserai pour tous les graphiques de ce chapitre. Libre à vous de choisir un thème différent ou de vous contenter du thème proposé par défaut :\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "10-TwoSamplePairedTests.html#contexte",
    "href": "10-TwoSamplePairedTests.html#contexte",
    "title": "10  Comparaison de moyennes : deux échantillons appariés",
    "section": "10.2 Contexte",
    "text": "10.2 Contexte\nOn s’intéresse ici à la comparaison de 2 séries de données dont les observations sont liées 2 à 2. C’est par exemple le cas lorsque l’on fait subir un traitement à différents sujets et que l’on souhaite comparer les mesures obtenues avant et après le traitement.\nAutrement dit, dans les plans d’expériences appariés, les deux traitements ou modalités sont appliqués à chaque unité d’échantillonnage : chaque sujet ou unité d’échantillonnage fournit plusieurs valeurs. Ça n’était pas le cas du chapitre précédent (@#sec-moy1) où chaque adulte n’avait fourni qu’une unique valeur de température.\nVoici quelques exemples de situations qui devraient être traitées avec des tests sur données appariées :\n\nComparaison de la masse de patients avant et après une hospitalisation.\nComparaison de la diversité de peuplements de poissons dans des lacs avant et après contamination par des métaux lourds.\nTest des effets d’une crème solaire appliquée sur un bras de chaque volontaire alors que l’autre bras ne reçoit qu’un placébo.\nTest des effets du tabagisme dans un échantillon de fumeurs, dont chaque membre est comparé à un non fumeur choisi pour qu’il lui ressemble le plus possible en terme d’âge, de masse, d’origine ethnique et sociale, etc.\nTest des effets que les conditions socio-économiques ont sur les préférences alimentaires en comparant des vrais jumeaux élevés dans des familles adoptives séparées qui diffèrent en termes de conditions socio-économiques.\n\nLes 2 derniers exemples montrent que même des individus séparés peuvent constituer une “paire statistique” s’ils partagent un certain nombre de caractéristiques (physiques, environnementales, génétiques, comportementales, etc.) pertinentes pour l’étude.\nIci, nous allons nous intéresser au lien qui pourrait exister entre la production de testostérone et l’immunité chez une espèce d’oiseau vivant en Amérique du Nord, le carouge à épaulettes.\n\n\n\nLe carouge à épaulettes\n\n\nChez de nombreuses espèces, les mâles ont plus de chances d’attirer des femelles s’ils produisent des niveaux de testostérone élevés. Est-ce que la forte production de testostérone de certains mâles a un coût, notamment en terme d’immunocompétence ? Autrement dit, est-ce que produire beaucoup de testostérone au moment de la reproduction (ce qui fournit un avantage sélectif) se traduit par une immunité plus faible par la suite, et donc une plus forte susceptibilité de contracter des maladies (ce qui constitue donc un désavantage sélectif) ? Ce type de question est central pour comprendre comment l’allocation des ressources affecte à la fois la survie et la fécondité des individus.\nPour étudier cette question, une équipe de chercheurs (Hasselquist et al. 1999) a mis en place le dispositif expérimental suivant. Les niveaux de testostérone de 13 carouges à épaulettes mâles ont été artificiellement augmentés par l’implantation chirurgicale d’un microtube perméable contenant de la testostérone. L’immunocompétence a été mesurée pour chaque oiseau avant et après l’opération chirurgicale. La variable mesurée est la production d’anticorps suite à l’exposition des oiseaux avec un antigène non pathogène mais censé déclencher une réponse immunitaire. Les taux de production d’anticorps sont exprimés en logarithmes de densité optique par minute \\(\\left(\\ln\\frac{mOD}{min}\\right)\\). Si la production de testostérone influence l’immunocompétence, on s’attend à observer des différence de production d’anticorps avant et après l’intervention chirurgicale."
  },
  {
    "objectID": "10-TwoSamplePairedTests.html#importation-et-mise-en-forme-des-données",
    "href": "10-TwoSamplePairedTests.html#importation-et-mise-en-forme-des-données",
    "title": "10  Comparaison de moyennes : deux échantillons appariés",
    "section": "10.3 Importation et mise en forme des données",
    "text": "10.3 Importation et mise en forme des données\nLes données se trouvent dans le fichier Testosterone.csv. Importez ces données dans un objet nommé Testo et affichez son contenu.\n\nTesto\n\n# A tibble: 13 × 5\n   blackbird beforeImplant afterImplant logBeforeImplant logAfterImplant\n       &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;\n 1         1           105           85             4.65            4.44\n 2         2            50           74             3.91            4.3 \n 3         3           136          145             4.91            4.98\n 4         4            90           86             4.5             4.45\n 5         5           122          148             4.8             5   \n 6         6           132          148             4.88            5   \n 7         7           131          150             4.88            5.01\n 8         8           119          142             4.78            4.96\n 9         9           145          151             4.98            5.02\n10        10           130          113             4.87            4.73\n11        11           116          118             4.75            4.77\n12        12           110           99             4.7             4.6 \n13        13           138          150             4.93            5.01\n\n\nVisiblement, il n’y a pas de données manquantes mais certaines variables sont inutiles. En effet, nous aurons besoin des variables transformées en logarithmes, mais pas des 2 colonnes beforeImplant et afterImplant. Nous allons donc les retirer avec la fonction select(). Par ailleurs, la variable blackbird est importante puisque chaque individu a fourni 2 valeurs de production d’anticorps : 1 avant et 1 après l’opération chirurgicale. Il sera donc important de conserver cet identifiant individuel. Toutefois, il apparaît ici sous la forme d’une variable numérique alors qu’il s’agit d’un identifiant, d’un code. Il faut donc le transformer en facteur car cela n’aurait pas de sens calculer une moyenne des identifiants par exemple. Pour cela, nous utiliserons la fonction factor() à l’intérieur de mutate(). Enfin, nous renommerons les colonnes avec rename() pour avoir des noms plus courts et plus faciles à utiliser. Si vous ne vous rappelez plus comment utilisez ces fonctions, consulter ces chapitres du livre en ligne de biométrie du semestre 3 : select() et rename(), mutate() et factor(). Enfin, nous donnerons le nom Testo_large au tableau modifié :\n\nTesto_large &lt;- Testo %&gt;% \n  select(-beforeImplant, -afterImplant) %&gt;%  # Suppression des colonnes inutiles\n  mutate(blackbird = factor(blackbird)) %&gt;%  # Transformation en facteur \n  rename(ID = blackbird,                     # Changement des noms de variables\n         Before = logBeforeImplant,\n         After = logAfterImplant)\n\nTesto_large\n\n# A tibble: 13 × 3\n   ID    Before After\n   &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 1       4.65  4.44\n 2 2       3.91  4.3 \n 3 3       4.91  4.98\n 4 4       4.5   4.45\n 5 5       4.8   5   \n 6 6       4.88  5   \n 7 7       4.88  5.01\n 8 8       4.78  4.96\n 9 9       4.98  5.02\n10 10      4.87  4.73\n11 11      4.75  4.77\n12 12      4.7   4.6 \n13 13      4.93  5.01\n\n\nLe tableau Testo_large dont nous disposons maintenant n’est pas dans un format qui nous permettra de réaliser toutes les opérations dont nous aurons besoin. En réalité, il ne s’agit pas d’un “tableau rangé” au sens du tidyverse. Un tableau rangé est un tableau dans lequel chaque ligne correspond à une unique observation et chaque colonne correspond à une unique variable. Ici, nous devrions avoir les 3 variables suivantes :\n\nL’identifiant des individus. La colonne ID correspond à cette variable.\nLe moment auquel chaque mesure a été effectuée, avant ou après l’opération chirurgicale. Cette information est pour l’instant stockée dans l’en-tête des colonnes 2 et 3 du tableau Testo_large\nLa mesure de réponse immunitaire (en logarithme de la densité optique par minute). Cette information est pour l’instant stockée sous forme de valeurs numériques dans les colonnes 2 et 3 du tableau Testo_large\n\nPour obtenir un tableau rangé, il nous faut donc réorganiser les colonnes 2 et 3 du tableau Testo_large :\n\nl’entête de ces 2 colonnes devrait constituer une nouvelle variable que nous nommerons Moment\nle contenu de ces 2 colonnes (les valeurs numériques) devrait constituer une nouvelle variable que nous nommerons DO (pour densité optique).\n\nPour effectuer cette transformation, nous utiliserons la fonction pivot_longer() du package tidyr (il est déjà chargé en mémoire si vous avez chargé le tidyverse). Comme son nom l’indique, cette fonction produira un tableau plus “long” (qui aura plus de lignes) que le tableau de départ. Nous l’appellerons donc Testo_long :\n\nTesto_long &lt;- Testo_large %&gt;%\n  pivot_longer(cols = c(Before, After),   # Les colonnes qu'on veut réorganiser\n               names_to = \"Moment\",   # Quel nom donner à la variable qui contiendra les noms des anciennes colonnes\n               values_to = \"DO\") %&gt;%      # Quel nom donner à la variable qui contiendra le contenu des anciennes colonnes\n  mutate(Moment = factor(Moment, levels = c(\"Before\", \"After\")))\n\nTesto_long\n\n# A tibble: 26 × 3\n   ID    Moment    DO\n   &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n 1 1     Before  4.65\n 2 1     After   4.44\n 3 2     Before  3.91\n 4 2     After   4.3 \n 5 3     Before  4.91\n 6 3     After   4.98\n 7 4     Before  4.5 \n 8 4     After   4.45\n 9 5     Before  4.8 \n10 5     After   5   \n# ℹ 16 more rows\n\n\nCe nouvel objet contient les mêmes données que précédemment, mais sous un format différent (il contient maintenant 26 lignes et non plus 13) : il s’agit d’un tableau rangé.\nLa plupart du temps, on a besoin de ces 2 formats de tableaux quand nous traitons des données. Le tableau au format long est à privilégier pour les représentations graphiques et les tests statistiques, et le format court sert souvent à présenter des résultats sous une forme synthétique. Mais parfois (et c’est justement le cas quand on dispose de données appariées comme pour notre exemple de lien entre testostérone et immunocompétence), le tableau au format large permettra de faire certains graphiques, certains tests ou certaines manipulations plus facilement que le tableau rangé au format long.\nSi on ne dispose que d’un tableau au format large, on peut passer au format long, comme nous venons de le faire, grâce à la fonction pivot_longer(). Et si on ne dispose que d’un tableau au format long, on peut passer au format large grâce à la fonction pivot_wider(). Nous avons déjà d’ailleurs évoqué cette fonction dans le livre en ligne de biométrie du semestre 4 pour mettre en forme des résultats obtenus avec summarise() (par exemple ici) ou reframe() (ou là), et je vous encourage à y jeter un œil à nouveau pour vous remémorer la syntaxe. Car il est important que vous maîtrisiez ces 2 fonctions dont vous aurez très souvent besoin.\nMaintenant que nous disposons de ces 2 tableaux, Testo_large et Testo_long, nous pouvons commencer à décrire nos données."
  },
  {
    "objectID": "10-TwoSamplePairedTests.html#exploration-statistique-des-données",
    "href": "10-TwoSamplePairedTests.html#exploration-statistique-des-données",
    "title": "10  Comparaison de moyennes : deux échantillons appariés",
    "section": "10.4 Exploration statistique des données",
    "text": "10.4 Exploration statistique des données\nPour décrire simplement les données, nous nous en tiendront ici à l’utilisation des fonctions summary() et skim().\nPour la fonction summary(), le plus simple est toujours d’utiliser le tableau au format large :\n\nsummary(Testo_large)\n\n       ID        Before          After     \n 1      :1   Min.   :3.910   Min.   :4.30  \n 2      :1   1st Qu.:4.700   1st Qu.:4.60  \n 3      :1   Median :4.800   Median :4.96  \n 4      :1   Mean   :4.734   Mean   :4.79  \n 5      :1   3rd Qu.:4.880   3rd Qu.:5.00  \n 6      :1   Max.   :4.980   Max.   :5.02  \n (Other):7                                 \n\n\nOn constate ici que pour les 2 traitements, les valeurs des différents indices sont très proches entre les 2 séries de données, avec des valeurs de densité optiques (DO) légèrement supérieures après l’opération chirurgicale (sauf pour le premier quartile).\nPour la fonction skim() le plus simple est là aussi d’utiliser le tableau large :\n\nskim(Testo_large)\n\n── Data Summary ────────────────────────\n                           Values     \nName                       Testo_large\nNumber of rows             13         \nNumber of columns          3          \n_______________________               \nColumn type frequency:                \n  factor                   1          \n  numeric                  2          \n________________________              \nGroup variables            None       \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts            \n1 ID                    0             1 FALSE         13 1: 1, 2: 1, 3: 1, 4: 1\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate mean    sd   p0 p25  p50  p75 p100 hist \n1 Before                0             1 4.73 0.280 3.91 4.7 4.8  4.88 4.98 ▁▁▁▃▇\n2 After                 0             1 4.79 0.262 4.3  4.6 4.96 5    5.02 ▂▁▂▁▇\n\n\nOn arrive toutefois aux mêmes résultats avec le tableau long, à condition de grouper les données par traitement (variable Traitement) avec group_by() :\n\nTesto_long %&gt;%\n  group_by(Moment) %&gt;%\n  skim(DO)\n\n── Data Summary ────────────────────────\n                           Values    \nName                       Piped data\nNumber of rows             26        \nNumber of columns          3         \n_______________________              \nColumn type frequency:               \n  numeric                  1         \n________________________             \nGroup variables            Moment    \n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable Moment n_missing complete_rate mean    sd   p0 p25  p50  p75\n1 DO            Before         0             1 4.73 0.280 3.91 4.7 4.8  4.88\n2 DO            After          0             1 4.79 0.262 4.3  4.6 4.96 5   \n  p100 hist \n1 4.98 ▁▁▁▃▇\n2 5.02 ▂▁▂▁▇\n\n\nCela revient à demander à la fonction skim() de produire un résumé des données de densité optique (variable DO), pour chaque catégorie de la variable Moment, soit un résumé pour la catégorie Before (avant l’intervention chirurgicale), et un résumé pour la catégorie After (après l’intervention chirurgicale).\nPar rapport aux résultats fournis par la fonction summary(), la fonction skim() nous permet de confirmer que les valeurs de DO sont très légèrement supérieures après l’opération (sauf pour le premier quartile). Elle nous permet également de constater que l’écart-type est du même ordre de grandeur pour les 2 catégories, bien qu’il soit légèrement plus faible après l’opération. Enfin, les petits histogrammes laissent entrevoir une distribution très asymétrique des données dans chacun des 2 groupes de mesures."
  },
  {
    "objectID": "10-TwoSamplePairedTests.html#exploration-graphique-des-données",
    "href": "10-TwoSamplePairedTests.html#exploration-graphique-des-données",
    "title": "10  Comparaison de moyennes : deux échantillons appariés",
    "section": "10.5 Exploration graphique des données",
    "text": "10.5 Exploration graphique des données\nIci, c’est le tableau rangé au format long qui sera le plus adapté. Lorsque nous avions une unique série de données, nous avons utilisé 2 types de représentations graphiques très similaires pour visualiser les données (les histogrammes et les graphiques de densités). Ici, nous allons utiliser ces mêmes types de graphiques mais “facettés”. Les graphiques facettés ont été abordés dans le livre en ligne de biométrie du semestre 3. Ils permettent de faire des sous-graphiques pour chaque catégorie d’un facteur. Ici, le facteur Moment contient 2 catégories. Les facets nous permettrons donc de comparer les 2 distributions de densités optiques.\nOutre ces graphiques, nous utiliserons aussi les stripcharts et les boites à moustaches pour comparer les 2 catégories. Ces 2 types de graphiques sont particulièrement adaptés pour ce genre de tâche, et seront aussi très utiles pour l’ANOVA lorsque nous aurons plus de 2 catégories à comparer.\nD’une façon générale, nous disposons :\n\nd’une variable numérique, DO : la mesure de densité optique qui rend compte de l’immunocompétence des carouges à épaulettes\nd’une variable catégorielle, le facteur Moment : indique si les valeurs d’immunocompétences ont été mesurées avant ou après l’opération chirurgicale d’implantation de la capsule de testostérone.\n\nTous les graphiques présentés dans le chapitre consacré à cette situation précise dans le livre en ligne de biométrie du semestre 3, peuvent être réalisés. N’hésitez pas à le relire, en particulier la section expliquant comment faire apparaître et interpréter les encoches d’incertitudes sur des boiites à moustaches.\n\n10.5.1 Avec un stripchart\n\nTesto_long %&gt;%\n  ggplot(aes(x = Moment, y = DO)) +\n  geom_jitter(height = 0, width = 0.25) +\n  labs(y = \"immunocompétence (log DO / minute)\",\n       title = \"immunocompétence\\navant et après l'opération\",\n       subtitle = \"n = 13 carouges à épaulettes\")\n\n\n\n\n\n\n\n\n\n\n10.5.2 Avec des histogrammes facettés\nNous allons faire un histogramme pour chaque série de données en utilisant des facettes :\n\nTesto_long %&gt;%\n  ggplot(aes(x = DO)) +\n  geom_histogram(bins = 10, fill = \"firebrick2\", color = \"grey20\", alpha = 0.5)+\n  geom_rug() +\n  facet_wrap(~Moment, ncol = 1) +\n  labs(x = \"immunocompétence (log DO / minute)\",\n       y = \"Fréquence\",\n       title = \"Comparaison de l'immunocompétence avant et après l'opération chirurgicale\",\n       subtitle = \"n = 13 carouges à épaulettes\")\n\n\n\n\n\n\n10.5.3 Avec des diagrammes de densité facettés\n\nTesto_long %&gt;%\n  ggplot(aes(x = DO)) +\n  geom_density(fill = \"firebrick2\", alpha = 0.5) +\n  geom_rug() +\n  facet_wrap(~Moment, ncol = 1) +\n  labs(x = \"immunocompétence (log DO / minute)\",\n       y = \"Densité\",\n       title = \"Comparaison de l'immunocompétence avant et après l'opération chirurgicale\",\n       subtitle = \"n = 13 carouges à épaulettes\")\n\n\n\n\n\n\n10.5.4 Avec des boîtes à moustaches\n\nTesto_long %&gt;%\n  ggplot(aes(x = Moment, y = DO)) +\n  geom_boxplot(notch = TRUE) +\n  expand_limits(y = 5.2) +\n  labs(y = \"immunocompétence (log DO / minute)\",\n       title = \"Comparaison de l'immunocompétence avant et après opération chirurgicale\",\n       subtitle = \"n = 13 carouges à épaulettes\")\n\nNotch went outside hinges\nℹ Do you want `notch = FALSE`?\n\n\n\n\n\n\n\n\n\nDu point de vue de la position des données, ces différents graphiques montrent tous que la seconde série de données (catégorie After : après l’opération chirurgicale) présente en moyenne des valeurs très légèrement plus élevées que la première (catégorie Before avant l’opération). En terme de dispersion, si l’on met de côté la valeur minimale de la série Before qui semble atypique (un individu outlier qui présente une immunocompétence très faible avant l’opération), la dispersion des données autour de la tendance centrale semble globalement plus importante pour la série After. Enfin, pour ce qui concerne l’incertitude, les intervalles de confiance à 95% des médianes (qui apparaissent sous la forme d’encoches sur les boîtes à moustaches) se chevauche assez largement, ce qui nous permet d’anticiper les résultats des tests que nous ferons ensuite : puisque les encoches se chevauchent, il y a fort à parier que le test de comparaison de moyenne ne montrera aucune différence significative. On note également que l’encoche de la série After est particulièrement large : la limite supérieure de l’intervalle de confiance à 95% de la médiane est supérieure à la valeur maximale observée dans l’échantillon. Cela traduit le fait que compte de la grande variabilité des données dans cette série, un échantillon de taille n = 13 n’est probablement pas suffisant pour avoir une estimation précise de la médiane.\n\n\n10.5.5 Avec un nuage de points appariés\nToutes ces représentations graphiques sont certes utiles, mais elles masquent un élément crucial : ce sont les mêmes individus qui sont étudiés avant et après l’opération. Il s’agit de données appariées ! Les graphiques que nous avons faits jusque là ne permettent pas de visualiser ce lien entre les deux séries de données. Pour avoir une bonne vision de ce qui se passe, il nous faut faire apparaître ce lien entre les 2 séries de données :\n\nTesto_long %&gt;%\n  ggplot(aes(x = Moment, y = DO, group = ID, color = ID)) +\n  geom_line() +\n  geom_point() +\n  labs(y = \"immunocompétence (log DO / minute)\",\n       title = \"Comparaison de l'immunocompétence avant et après opération chirurgicale\",\n       subtitle = \"n = 13 carouges à épaulettes\", \n       color = \"Individu\")\n\n\n\n\n\n\n\n\nCe graphique nous donne une image très différente de la réalité des données. On constate ici que l’immunocompétence de certains individus augmente après l’opération (parfois fortement), alors que pour d’autres, elle diminue.\nUne façon d’estimer si les changements d’immunocompétence sont majoritairement orientés dans un sens ou non est de calculer l’intervalle de confiance à 95% de la différence d’immunocompétence entre avant et après l’opération. Pour cela, on peut calculer, grâce au tableau large Testo_large, les différences d’immunocompétences (DO après opération moins DO avant opération), pour chacun des 13 individus. puis, grâce à la fonction mean_cl_normal() déjà utilisée à plusieurs reprises, on calcul l’intervalle de confiance à 95% de la moyenne de cette différence :\n\n# Calcul de la différence de DO (After - Before)\nTesto_large &lt;- Testo_large %&gt;% \n  mutate(Difference = After - Before)\n\n# Affichage du tableau\nTesto_large\n\n# A tibble: 13 × 4\n   ID    Before After Difference\n   &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1 1       4.65  4.44    -0.21  \n 2 2       3.91  4.3      0.390 \n 3 3       4.91  4.98     0.0700\n 4 4       4.5   4.45    -0.0500\n 5 5       4.8   5        0.200 \n 6 6       4.88  5        0.120 \n 7 7       4.88  5.01     0.130 \n 8 8       4.78  4.96     0.180 \n 9 9       4.98  5.02     0.0400\n10 10      4.87  4.73    -0.140 \n11 11      4.75  4.77     0.0200\n12 12      4.7   4.6     -0.100 \n13 13      4.93  5.01     0.0800\n\n# Calcul de la moyenne des différences et de son IC95%\nTesto_large %&gt;% \n  reframe(mean_cl_normal(Difference))\n\n# A tibble: 1 × 3\n       y    ymin  ymax\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 0.0562 -0.0401 0.152\n\n\nOn constate ici que la moyenne des différences de densité optique vaut 0.06, soit une valeur positive, qui montre que l’immunocompétence augmente après l’opération (ce qui semble aller à l’opposé de l’hypothèse des chercheurs). Cette moyenne reste néanmoins très proche de 0. D’ailleurs, l’intervalle de confiance de cette moyenne comprend les valeurs situées entre -0.04 et +0.15. La valeur 0 est donc comprise dans cet intervalle. Le zéro fait donc partie des valeurs les plus probables pour la moyenne de ces différences dans la populations générale. C’est là encore un résultat qui nous permet d’anticiper sur les résultats du tests statistique que nous ferons ensuite."
  },
  {
    "objectID": "10-TwoSamplePairedTests.html#le-test-paramétrique",
    "href": "10-TwoSamplePairedTests.html#le-test-paramétrique",
    "title": "10  Comparaison de moyennes : deux échantillons appariés",
    "section": "10.6 Le test paramétrique",
    "text": "10.6 Le test paramétrique\n\n10.6.1 Procédure\nLe test paramétrique permettant de comparer la moyenne sur des séries appariées est là encore un test de Student : le test de Student sur données appariées (étonnant non ?…). En réalité, ce test de Student n’est pas un test de comparaison de moyennes entre 2 séries de données. La procédure est la suivante :\n\nPour chaque individu, calculer la différence d’immunocompétence entre les deux temps de l’expérience (DO après - DO avant opération). C’est ce que nous avons fait plus haut en ajoutant la colonne Difference au tableau Testo_large.\nPuisque nous avons 13 individus, nous aurons 13 valeurs de différences. La moyenne de cette différence sera comparée à la valeur théorique 0. Autrement dit, si cette moyenne vaut 0, l’immunocompétence sera la même avant et après l’opération. Si la moyenne des différence n’est pas égale 0, alors nous aurons prouvé qu’il existe une différence d’immunocompétence entre les 2 groupes, nous aurons prouvé que la procédure chirurgicale d’implantation de la capsule de testostérone a un impact sur l’immunocompétence des carouges à épaulettes\n\n\n\n\n\n\n\nAttention\n\n\n\nDans un test sur données appariées, on s’intéresse à la moyenne des différences entre les données des 2 séries. Cette moyenne est alors comparée à la valeur théorique \\(\\mu\\) = 0. Ce test est donc équivalent au test vu dans le Chapitre 9 sur la comparaison de la moyenne d’une population à une valeur théorique.\nNotez également que la moyenne des différences n’est pas équivalente à la différence des moyennes. La différence des moyennes est une grandeur qui nous sera utile dans le chapitre suivant (Chapitre 11) sur la comparaison de la moyenne de deux populations lorsque les données sont indépendantes.\n\n\n\n\n10.6.2 Conditions d’application\nLes conditions d’application de ce test paramétrique sont presque les mêmes que pour le test de Student à un échantillon :\n\nLes individus sur lesquels portent la comparaison doivent être issus d’un échantillonnage aléatoire. Comme toujours, en l’absence d’indication contraire, on considère que cette condition est vérifiée.\nLes différences par paires entre les 2 modalités du traitement doivent suivre une distribution Normale. Attention, ce n’est donc pas les données brutes de chaque série qui doivent suivre une loi Normale, mais bien la différence “après” - “avant” calculée pour chaque individu. Nous avons déjà calculé ces différences plus haut :\n\n\n# On s'intéresse aux 13 différences calculées sur les 13 individus\nTesto_large \n\n# A tibble: 13 × 4\n   ID    Before After Difference\n   &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1 1       4.65  4.44    -0.21  \n 2 2       3.91  4.3      0.390 \n 3 3       4.91  4.98     0.0700\n 4 4       4.5   4.45    -0.0500\n 5 5       4.8   5        0.200 \n 6 6       4.88  5        0.120 \n 7 7       4.88  5.01     0.130 \n 8 8       4.78  4.96     0.180 \n 9 9       4.98  5.02     0.0400\n10 10      4.87  4.73    -0.140 \n11 11      4.75  4.77     0.0200\n12 12      4.7   4.6     -0.100 \n13 13      4.93  5.01     0.0800\n\n\nIl nous faut donc tester la Normalité de la nouvelle variable Difference. Commençons par en faire un graphique :\n\nTesto_large %&gt;%\n  ggplot(aes(x = Difference)) +\n  geom_density(fill = \"firebrick2\", alpha = 0.5) +\n  geom_rug() +\n  labs(x = \"Différence d'immunocompétence 'Après - Avant' l'opération (log DO / minute)\",\n       y = \"Densité\",\n       title = \"Distribution de la différence d'immunocompétence entre après et avant l'opération chirurgicale\",\n       subtitle = \"n = 13 carouges à épaulettes\")\n\n\n\n\nCompte tenu du faible nombre d’individus (n = 13), la forme de cette courbe de densité n’est pas si éloignée que ça d’une courbe en cloche (notez que ce n’était pas du tout le cas pour les données brutes de chaque série de départ qui ont toutes les deux des distributions très éloignées de la distribution Normale). On le vérifie avec un test de normalité de Shapiro-Wilk :\n\nH\\(_0\\) : la différence d’immunocompétence des individus suit une distribution Normale.\nH\\(_1\\) : la différence d’immunocompétence des individus ne suit pas une distribution Normale.\n\n\nTesto_large %&gt;%\n  pull(Difference) %&gt;%\n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.97949, p-value = 0.977\n\n\n\nAu seuil \\(\\alpha = 0.05\\), on ne peut pas rejeter l’hypothèse nulle de normalité pour la différence d’immunocompétence entre après et avant l’intervention chirurgicale (test de Shapiro-Wilk, \\(W = 0.98\\), \\(p = 0.977\\)).\n\nLes conditions d’application du test paramétrique sont donc réunies.\n\n\n\n\n\n\nAttention !\n\n\n\nPour ce test, la Normalité doit bien être verifiée sur la différence entre les 2 groupes de valeurs, et non sur chaque groupe de valeur pris séparément. C’est une source d’erreur fréquente. Ici, les données de départ (DO avant et DO après) ne suivaient pas du tout une distribution Normale. Pourtant, la différence de DO suit bel et bien la distribution Normale, nous permettant de faire le test paramétrique.\n\n\n\n\n10.6.3 Réalisation du test et interprétation\nLe test de Student sur données appariées peut se faire de 3 façons distinctes. Les 3 méthodes fournissent exactement les mêmes résultats, seule la syntaxe utilisée change. Quelle que soit la méthode utilisée, les hypothèses nulles et alternatives sont toujours les mêmes :\n\nH\\(_0\\) : le changement moyen de production d’anticorps après la pose chirurgicale de l’implant de testostérone est nul (\\(\\mu_{Diff} = 0\\)). La procédure chirurgicale n’a pas d’effet sur l’immunocompétence. Les variations observées ne sont que le fruit du hasard de l’échantillonnage.\nH\\(_1\\) : le changement moyen de production d’anticorps après la pose chirurgicale de l’implant de testostérone n’est pas nul (\\(\\mu_{Diff} \\neq 0\\)). La procédure chirurgicale a effet significatif sur l’immunocompétence. Les variations observées ne sont pas uniquement dues à la fluctuation d’échantillonnage.\n\n\n10.6.3.1 Première syntaxe\n\n# Méthode nº1 : avec une formule et le tableau au format long\nt.test(DO ~ Moment, data = Testo_long, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  DO by Moment\nt = -1.2714, df = 12, p-value = 0.2277\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.15238464  0.04007695\nsample estimates:\nmean difference \n    -0.05615385 \n\n\nPlusieurs remarques concernant cette première syntaxe :\n\nOn utilise le symbole “~” pour indiquer une formule. On cherche à regarder l’effet du Moment sur la DO qui traduit l’immunocompétence. Le “~” se lit : “en fonction de”.\nAvec la syntaxe utilisant les formules, on doit spécifier l’argument data = Testo_long pour indiquer à RStudio que les variables DO et Moment sont des colonnes de ce tableau.\nEnfin, il est important d’indiquer paired = TRUE puisque nous réalisons un test de Student sur données appariées. Si on ne mets pas cet argument, on réalise un test de Student sur échantillons indépendants, ce qui peut grandement fausser les résultats.\n\nIci, voilà la conclusion de ce test :\n\nLe test de Student sur données appariées ne permet pas de montrer de changement d’immunocompétence suite à l’intégration de l’implant chirurgical de testostérone. On ne peut pas rejeter l’hypothèse nulle au seuil \\(\\alpha = 0.05\\) (\\(t = -1.27\\), \\(ddl = 12\\), \\(p = 0.223\\)). La moyenne des différences de densités optiques observées entre avant et après l’intervention chirurgicale vaut -0.056 (intervalle de confiance à 95% de cette différence : [-0.152 ; 0.040])\n\nDonc visiblement, une forte production de testostérone n’est pas significativement associée à une baisse de l’immunocompétence.\n\n\n\n\n\n\nPoint de vigilance\n\n\n\nAvec cette première syntaxe, la différence qui est calculée n’est pas After - Before comme nous l’avons fait manuellement dans le tableau Testo_large, mais Before - After. En effet, dans le facteur Moment du tableau Testo-Long, la première modalité est Before, et la seconde modalité est After. Par défaut, le test de Student calcule toujours la différence dans le même sens : première modalité moins seconde modalité.\nCela explique pourquoi la différence calculée ici vaut -0.056, alors qu’elle valait +0.056 quand nous l’avions calculée manuellement plus haut. C’est important d’en prendre conscience pour ne pas interpréter à l’envers les résultats du test statistique.\nPour la même raison, les signes et l’ordre des bornes de l’intervalle de confiance à 95% de la moyenne des différences est également inversé. Manuellement, nous avions calculé un intervalle de confiance de [-0.04 ; +0.15], ici, il vaut [-0.15 ; +0.04].\n\n\n\n\n10.6.3.2 Deuxième syntaxe\n\n# Méthode nº2 : avec les 2 séries de données et le tableau au format large\nt.test(Testo_large$Before, Testo_large$After, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  Testo_large$Before and Testo_large$After\nt = -1.2714, df = 12, p-value = 0.2277\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.15238464  0.04007695\nsample estimates:\nmean difference \n    -0.05615385 \n\n\nCette deuxième syntaxe est différente de la première puisque nous n’utilisons plus le format formule. Ici, on indique le nom des 2 colonnes du tableau Testo_large qui contiennent les 2 séries de données. Puisque nous n’utilisons plus de formule, l’argument “data = ...” n’existe plus. C’est pourquoi il nous faut taper spécifiquement “Testo_large$Before” et “Testo_large$After”, et non pas simplement le nom des colonnes. En revanche, comme pour le test précédent, il est indispensable d’indiquer “paired = TRUE” pour faire un test de Student sur données appariées.\nLes résultats fournis et leur interprétation sont identiques à ceux de la syntaxe précédente. Vous notez aussi que les résultats et leurs interprétation dépendent de l’ordre dans lequel les 2 séries de données sont indiquées dans la fonction t.test(). Pour vous en convaincre, regardez ce que donne cette commande :\n\nt.test(Testo_large$After, Testo_large$Before, paired = TRUE)\n\nQu’est-ce qui change ? Et qu’est-ce qui reste inchangé ?\n\n\n10.6.3.3 Troisième syntaxe\n\n# Méthode nº3 : avec la variable Diff, mu = 0, et le tableau au format large\nt.test(Testo_large$Difference, mu = 0)\n\n\n    One Sample t-test\n\ndata:  Testo_large$Difference\nt = 1.2714, df = 12, p-value = 0.2277\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.04007695  0.15238464\nsample estimates:\n mean of x \n0.05615385 \n\n\nEnfin, comme expliqué plus haut, le test de Student sur données appariées est strictement équivalent à un test de Student à un échantillon pour lequel on compare la moyenne des différences individuelles à 0. Là encore, les résultats produits et leur interprétation sont identiques aux deux tests précédents. La seule différence concerne les signes puisque les deux premiers tests regardaient la différence “Before - After” alors que ce troisième test regarde la différence “After - Before” (que nous avons calculée manuellement).\nÀ vous donc de choisir la syntaxe qui vous paraît la plus parlante ou celle que vous avez le plus de facilité à retenir et à interpréter."
  },
  {
    "objectID": "10-TwoSamplePairedTests.html#lalternative-non-paramétrique",
    "href": "10-TwoSamplePairedTests.html#lalternative-non-paramétrique",
    "title": "10  Comparaison de moyennes : deux échantillons appariés",
    "section": "10.7 L’alternative non paramétrique",
    "text": "10.7 L’alternative non paramétrique\nComme pour le test de Student à un échantillon, lorsque les conditions d’application du test de Student sur données appariées ne sont pas vérifiées (c’est à dire lorsque la différence entre les données appariées des deux séries ne suit pas une loi Normale), il faut utiliser un test non paramétrique équivalent.\nIl s’agit là encore du test de Wilcoxon des rangs signés qui s’intéresse aux médianes. Les hypothèses nulles et alternatives sont les suivantes :\n\nH\\(_0\\) : le changement médian de production d’anticorps après la pose chirurgicale de l’implant de testostérone est nul (\\(med_{Diff} = 0\\)).\nH\\(_1\\) : le changement médian de production d’anticorps après la pose chirurgicale de l’implant de testostérone n’est pas nul (\\(med_{Diff} \\neq 0\\)).\n\nComme pour le test de Student, 3 syntaxes sont possibles et strictement équivalentes. Il est important de ne pas oublier l’argument paired = TRUE pour les 2 premières syntaxes afin de s’assurer que l’on réalise bien un test sur données appariées. Enfin, l’argument conf.int = TRUE doit être ajouté pour les 3 syntaxes afin que la (pseudo-) médiane et son intervalle de confiance à 95% soient calculés et affichés.\n\nwilcox.test(DO ~ Moment, data = Testo_long, paired = TRUE, conf.int = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  DO by Moment\nV = 30, p-value = 0.3054\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -0.145  0.040\nsample estimates:\n(pseudo)median \n        -0.055 \n\nwilcox.test(Testo_large$Before, Testo_large$After, paired = TRUE,\n            conf.int = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  Testo_large$Before and Testo_large$After\nV = 30, p-value = 0.3054\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -0.145  0.040\nsample estimates:\n(pseudo)median \n        -0.055 \n\nwilcox.test(Testo_large$Difference, mu = 0, conf.int = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  Testo_large$Difference\nV = 61, p-value = 0.3054\nalternative hypothesis: true location is not equal to 0\n95 percent confidence interval:\n -0.040  0.145\nsample estimates:\n(pseudo)median \n         0.055 \n\n\nIci, la conclusion de ce test est :\n\nLe test de Wilcoxon des rangs signés n’a pas permis de montrer de changement d’immunocompétence suite à l’intégration de l’implant chirurgical de testostérone. On ne peut pas rejeter l’hypothèse nulle au seuil \\(\\alpha = 0.05\\) (\\(V = 61\\), \\(p = 0.305\\)). La médiane des différences de densités optiques observées entre après et avant l’intervention chirurgicale vaut 0.055 (intervalle de confiance à 95% de cette différence : [-0.040 ; 0.145])."
  },
  {
    "objectID": "10-TwoSamplePairedTests.html#exercice-dapplication",
    "href": "10-TwoSamplePairedTests.html#exercice-dapplication",
    "title": "10  Comparaison de moyennes : deux échantillons appariés",
    "section": "10.8 Exercice d’application",
    "text": "10.8 Exercice d’application\nLes autruches vivent dans des environnements chauds et elles sont donc fréquemment exposées au soleil durant de longues périodes. Dans des environnements similaires, les mammifères ont des mécanismes physiologiques leur permettant de réduire la température de leur cerveau par rapport à celle de leur corps. Une équipe de chercheurs (Fuller et al. 2003) a testé si les autruches pouvaient faire de même. La température du corps et du cerveau de 37 autruches a été enregistrée par une journée chaude typique. Les résultats, exprimés en degrés Celsius, figurent dans le fichier Autruches.csv.\nImportez ces données et faites-en l’analyse pour savoir s’il existe une différence de température moyenne entre le corps et le cerveau des autruches. Vos observations chez les autruches sont-elles conformes à ce qui est observé chez les mammifères dans un environnement similaire ? Comme toujours, vous commencerez par faire une analyse descriptive des données, sous forme numérique et graphique, avant de vous lancer dans les tests d’hypothèses.\n\n\n\n\nFuller, Andrea, Peter R. Kamerman, Shane K. Maloney, Graham Mitchell, et Duncan Mitchell. 2003. « Variability in brain and arterial blood temperatures in free-ranging ostriches in their natural habitat ». Journal of Experimental Biology 206 (7): 1171‑81. https://doi.org/10.1242/jeb.00230.\n\n\nHasselquist, Dennis, James A. Marsh, Paul W. Sherman, et John C. Wingfield. 1999. « Is avian humoral immunocompetence suppressed by testosterone? » Behavioral Ecology and Sociobiology 45 (3): 167‑75. https://doi.org/10.1007/s002650050550.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, et Shannon Ellis. 2022. skimr: Compact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr.\n\n\nWickham, Hadley. 2023. tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, et Teun van den Brand. 2024. ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, et Davis Vaughan. 2023. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, et Jennifer Bryan. 2023. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr."
  },
  {
    "objectID": "11-TwoSampleTests.html#sec-packages3",
    "href": "11-TwoSampleTests.html#sec-packages3",
    "title": "11  Comparaison de moyennes : deux échantillons indépendants",
    "section": "11.1 Pré-requis",
    "text": "11.1 Pré-requis\nComme pour chaque nouveau chapitre, je vous conseille de travailler dans un nouveau script que vous placerez dans votre répertoire de travail, et dans une nouvelle session de travail (Menu Session &gt; Restart R). Inutile en revanche de créer un nouveau Rproject : vos pouvez tout à fait avoir plusieurs script dans le même répertoire de travail et pour un même Rproject. Comme toujours, consultez le livre en ligne du semestre 3 si vous ne savez plus comment faire.\nSi vous êtes dans une nouvelle session de travail (ou que vous avez quitté puis relancé RStudio), vous devrez penser à recharger en mémoire les packages utiles. Dans ce chapitre, vous aurez besoin d’utiliser :\n\nle tidyverse (Wickham 2023), qui comprend notamment le package readr (Wickham, Hester, et Bryan 2023), pour importer facilement des fichiers .csv au format tibble, le package dplyr (Wickham et al. 2023), pour manipuler des tableaux, et le package ggplot2 (Wickham et al. 2024) pour les représentations graphiques.\nreadxl (Wickham et Bryan 2023), pour importer facilement des fichiers Excel au format tibble.\nskimr (Waring et al. 2022), qui permet de calculer des résumés de données très informatifs.\ncar (Fox, Weisberg, et Price 2023), qui permet d’effectuer le test de comparaison des variances de Levene.\nle package palmerpenguins (Horst, Hill, et Gorman 2022) pour accéder au jeu de données penguins que nous utiliserons pour les exercices d’application.\n\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(skimr)\nlibrary(car)\nlibrary(palmerpenguins)\n\nVous aurez également besoin des jeux de données suivants que vous pouvez dès maintenant télécharger dans votre répertoire de travail :\n\nHommesFemmes.xls\nHornedLizards.csv\n\nEnfin, je spécifie ici une fois pour toutes le thème que j’utiliserai pour tous les graphiques de ce chapitre. Libre à vous de choisir un thème différent ou de vous contenter du thème proposé par défaut :\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "11-TwoSampleTests.html#contexte",
    "href": "11-TwoSampleTests.html#contexte",
    "title": "11  Comparaison de moyennes : deux échantillons indépendants",
    "section": "11.2 Contexte",
    "text": "11.2 Contexte\nOn s’intéresse maintenant aux méthodes permettant de comparer la moyenne de deux groupes ou de deux traitements dans la cas d’échantillons indépendants. Au contraire de la situation décrite dans le Chapitre 10, dans ce type de design expérimentaux, les deux traitements sont appliqués à des échantillons indépendants issus de 2 groupes ou populations distincts. Chaque individu collecté, ou chaque unité expérimentale observée, ne fournit qu’une seule valeur, indépendante de toutes les autres.\nCette situation est extrêmement classique dans le domaine de l’écologie au sens large. Ainsi, par exemple, lorsque l’on souhaite comparer 2 sites, on réalise des prélèvements dans chacun des 2 sites. Chaque prélèvement ne fournit qu’une valeur pour l’un des 2 sites.\nIci, nous allons examiner une espèce intéressante, le lézard cornu Phrynosoma mcallii, qui possède une frange de piquants autour de la tête. Une équipe d’herpétologues (Young, Brodie, et Brodie 2004) a étudié la question suivante : des piquants plus longs autour de la tête protègent-ils le lézard cornu de son prédateur naturel, la pie grièche migratrice Lanius ludovicianus ? Ce prédateur a en effet une particularité : il accroche ses proies mortes à des barbelés ou des branches pour les consommer plus tard. Les chercheurs ont donc mesuré la longueur des cornes de 30 lézards retrouvés morts et accrochés dans des arbres par la pie grièche migratrice. Et en parallèle, ils ont mesuré les cornes de 154 individus vivants et en bonne santé choisis au hasard dans la population.\n\n\n\n\n\n\n\n(a) Lézard cornu vivant\n\n\n\n\n\n\n\n(b) Lézard cornu mort\n\n\n\n\n\n\n\n(c) Pie grièche\n\n\n\n\nFigure 11.1: Le lézard cornu et son prédateur\n\n\nNous disposons donc de 2 groupes indépendants : chaque lézard n’a fourni qu’une valeur de longueur de cornes, et chaque lézard n’appartient qu’à un groupe, vivant ou mort. Nous sommes donc dans la situation typique de la comparaison de moyennes de 2 populations avec des données indépendantes. Avant de procéder aux tests, et comme toujours, nous allons commencer par importer et mettre en forme les données (si besoin), puis nous devrons explorer les données, à l’aide d’une part d’indices statistiques de position, de dispersion et d’incertitude et d’autre part de représentations graphiques pertinentes. Enfin, nous vérifierons les conditions d’application du test paramétrique de Student avant de réaliser ce test si les conditions d’application sont remplies, ou son équivalent non paramétrique si elles ne le sont pas."
  },
  {
    "objectID": "11-TwoSampleTests.html#importation-et-mise-en-forme-des-données",
    "href": "11-TwoSampleTests.html#importation-et-mise-en-forme-des-données",
    "title": "11  Comparaison de moyennes : deux échantillons indépendants",
    "section": "11.3 Importation et mise en forme des données",
    "text": "11.3 Importation et mise en forme des données\nLes données de cette étude sont stockées dans le fichier HornedLizards.csv. Importez ces données dans un objet nommé Lizard_raw et examinez le tableau obtenu.\n\nLizard_raw\n\n# A tibble: 185 × 2\n   squamosalHornLength Survival\n                 &lt;dbl&gt; &lt;chr&gt;   \n 1                25.2 living  \n 2                26.9 living  \n 3                26.6 living  \n 4                25.6 living  \n 5                25.7 living  \n 6                25.9 living  \n 7                27.3 living  \n 8                25.1 living  \n 9                30.3 living  \n10                25.6 living  \n# ℹ 175 more rows\n\n\n\nView(Lizard_raw)\n\nOn constate ici 3 choses :\n\nla variable Survival devrait être un facteur.\nle nom de la première colonne (squamosalHornLength), qui contient les mesures des longueurs de cornes, est bien trop long.\npour l’un des lézards vivants, la mesure de longueur des cornes est manquante. Nous allons donc retirer cet individu pour éviter les messages d’erreurs par la suite.\n\nNous pouvons facilement réaliser les 3 modifications d’un coup, et stokcer le résultat dans un nouveau tableau Lizard :\n\nLizard &lt;- Lizard_raw %&gt;%\n  mutate(Survival = factor(Survival)) %&gt;%\n  rename(Horn_len = squamosalHornLength) %&gt;%\n  filter(!is.na(Horn_len))\n\nLizard\n\n# A tibble: 184 × 2\n   Horn_len Survival\n      &lt;dbl&gt; &lt;fct&gt;   \n 1     25.2 living  \n 2     26.9 living  \n 3     26.6 living  \n 4     25.6 living  \n 5     25.7 living  \n 6     25.9 living  \n 7     27.3 living  \n 8     25.1 living  \n 9     30.3 living  \n10     25.6 living  \n# ℹ 174 more rows\n\n\nCe tableau est bien un tableau rangé, au format long : chaque colonne contient une unique variable (Horn_len : longueur des cornes, Survival : groupe de l’individu mesuré, vivant ou mort), et chaque ligne contient les informations d’un unique individu.\nIci, il ne serait pas correct de présenter les données au format large. il nous faudrait en effet une colonne pour chaque groupe, lézard vivant et lézard mort, mais puisque les données de ces 2 groupes sont indépendantes, nous aurions 2 problèmes :\n\nsi le nombre d’individu n’est pas le même dans les 2 groupes, les deux colonnes n’auraient pas la même longueur. C’est impossible dans RStudio, et le logiciel remplierait donc la colonne la plus courte de NAs pour y remédier.\nles lignes de cet hypothétique tableau large ne correspondraient plus à des observations uniques. Chaque ligne renseignerait en effet sur les mesures de 2 individus distincts, un vivant et un mort.\n\nLorsque vous disposez de données appartenant à des groupes indépendants, il faut donc toujours travailler avec un tableau rangé, nécessairement au format long."
  },
  {
    "objectID": "11-TwoSampleTests.html#exploration-statistique-des-données",
    "href": "11-TwoSampleTests.html#exploration-statistique-des-données",
    "title": "11  Comparaison de moyennes : deux échantillons indépendants",
    "section": "11.4 Exploration statistique des données",
    "text": "11.4 Exploration statistique des données\nComme dans le Chapitre 10 sur les données appariées, les statistiques descriptives doivent ici être réalisées pour chaque groupe d’individus, et non tous groupes confondus. Ici, le plus simple est d’utiliser la fonction skim() sur les données groupées par niveau du facteur Survival (avec la fonction group_by()) :\n\nLizard %&gt;%\n  group_by(Survival) %&gt;%\n  skim()\n\n── Data Summary ────────────────────────\n                           Values    \nName                       Piped data\nNumber of rows             184       \nNumber of columns          2         \n_______________________              \nColumn type frequency:               \n  numeric                  1         \n________________________             \nGroup variables            Survival  \n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable Survival n_missing complete_rate mean   sd   p0  p25  p50  p75\n1 Horn_len      killed           0             1 22.0 2.71 15.2 21.1 22.2 23.8\n2 Horn_len      living           0             1 24.3 2.63 13.1 23   24.6 26  \n  p100 hist \n1 26.7 ▂▂▇▇▃\n2 30.3 ▁▁▅▇▂\n\n\nOn constate ici qu’il n’y a pas de données manquantes (n_missing = 0 dans les deux groupes). La moyenne des longueurs de cornes est plus grande chez les lézards vivants (\\(\\bar{x}_{living} = 24.3\\) mm) que chez les lézards retrouvés morts (\\(\\bar{x}_{killed} = 22.0\\)), de plus de 2 millimètres. On retrouve cette tendance pour les médianes, ainsi que pour les premiers et troisièmes quartiles. En revanche, les écarts-types des 2 groupes sont proches, et celui du groupe living est très légèrement plus faible (0.08 mm) que celui du groupe killed.\nEnfin, les histogrammes très simplifiés fournis laissent penser que les données de chaque groupe ne s’écartent pas trop fortement d’une courbe en cloche.\nOutre ces informations sur les ordres de grandeurs observés dans chaque groupe de lézards pour les indices de position (moyennes, médianes et quartiles), et de dispersion (écarts-types et histogrammes), la fonction skim() ne fournit pas les effectifs observés dans chaque groupe. On sait qu’il y a en tout 184 individus, mais on ne sait pas comment ils se répartissent dans les 2 groupes de lézards. Pour le déterminer, on peut utiliser une fonction décrite plus tôt, la fonction count() :\n\nLizard %&gt;%\n  count(Survival)\n\n# A tibble: 2 × 2\n  Survival     n\n  &lt;fct&gt;    &lt;int&gt;\n1 killed      30\n2 living     154\n\n\nOn peut obtenir la même information avec la fonction summarise() et son argument .by, et la fonction n() :\n\nLizard %&gt;% \n  summarize(Effectif = n(), .by = Survival)\n\n# A tibble: 2 × 2\n  Survival Effectif\n  &lt;fct&gt;       &lt;int&gt;\n1 living        154\n2 killed         30\n\n\nOn constate ici que les tailles d’échantillons sont très différentes. C’est normal compte tenu de la difficulté de repérer des individus morts dans la nature, et ce n’est pas gênant pour nos analyses puisque la taille des deux échantillons reste élevée.\nEnfin, on peut calculer des indices d’incertitude. C’est d’autant plus important qu’il est difficile de se faire une idée de la signification d’une différence moyenne de longueur de cornes de 2 millimètres. Est-ce important ou négligeable ? Est-ce que ces estimations sont précises ou non ? Comme dans les chapitres précédents, nous allons calculer les intervalles de confiance à 95% de la moyenne de chaque groupe :\n\nLizard %&gt;% \n  reframe(mean_cl_normal(Horn_len), .by = Survival)\n\n# A tibble: 2 × 4\n  Survival     y  ymin  ymax\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 living    24.3  23.9  24.7\n2 killed    22.0  21.0  23.0\n\n\nLa colonne y nous présente à nouveau la moyenne de chaque groupe, la colonne ymin contient les bornes inférieures des intervalles de confiance à 95%, et la colonne ymax les bornes supérieures. On constate ici que les intervalles de confiance à 95% des longueurs de cornes des 2 groupes ne se chevauchent pas du tout : la borne inférieure du groupe living est au-dessus de la borne supérieure du groupe killed. Autrement dit, dans la population générale, la longueur moyenne des cornes chez les lézards vivants a de bonnes chances de se trouver dans l’intervalle [23.9 ; 24.7] millimètres, alors qu’elle a de bonnes chances de se trouver dans l’intervalle [21 ; 23] millimètres chez les lézards morts. La différence de moyennes entre ces 2 groupes vaut donc probablement entre 0.9 millimètres au moins, et 3.7 millimètres au plus. Le test statistique que nous ferons ensuite devrait donc confirmer que ces différences sont significatives, autrement dit, qu’elles ne sont pas liées au simple hasard de l’échantillonnage."
  },
  {
    "objectID": "11-TwoSampleTests.html#exploration-graphique-des-données",
    "href": "11-TwoSampleTests.html#exploration-graphique-des-données",
    "title": "11  Comparaison de moyennes : deux échantillons indépendants",
    "section": "11.5 Exploration graphique des données",
    "text": "11.5 Exploration graphique des données\nComme toujours, nous pouvons réaliser plusieurs types de graphiques pour en apprendre plus sur la distribution des données dans les deux groupes. Si nous faisons un nuage de points, il est évidemment impossible ici de relier les points deux à deux. Non seulement cela n’aurait aucun sens puisque les échantillons sont indépendants, mais en outre, nous ne disposons pas du même nombre d’individus dans les 2 échantillons. Nous nous contenterons donc da faire un stripchart.\n\n11.5.1 Avec un stripchart\n\nLizard %&gt;%\n  ggplot(aes(x = Survival, y = Horn_len)) +\n  geom_jitter(height = 0, width = 0.2, alpha = 0.5) +\n  labs(x = \"Groupe de lézards\",\n       y = \"Longueur des cornes (mm)\",\n       title = \"Visualisation des longueurs\\nde cornes du lézard cornu\")\n\n\n\n\n\n\n\n\nCe premier graphique permet de visualiser très clairement les différences de tailles d’échantillons entre les deux groupes. Il permet également de voir que l’étendue des longueurs de cornes est plus importante dans le groupe des individus vivants que dans celui des individus morts. En outre, le nuage de points des vivants semble être plus haut sur l’axe des y que celui des morts, confirmant les statistiques descriptives qui montraient des tailles de cornes en moyenne plus importantes dans le groupe des vivants.\n\n\n11.5.2 Avec des histogrammes facettés\n\nLizard %&gt;%\n  ggplot(aes(x = Horn_len)) +\n  geom_histogram(bins = 15, fill = \"firebrick2\", color = \"grey20\", alpha = 0.5)+\n  geom_rug() +\n  facet_wrap(~Survival, ncol = 1, scales = \"free_y\") +\n  labs(x = \"Longueur des cornes (mm)\",\n       y = \"Fréquence\",\n       title = \"Distribution de la longueur des cornes dans 2 groupes de lézards cornus\",\n       subtitle = \"nb morts : 30, nb vivants : 154\")\n\n\n\n\nNotez ici l’utilisation de l’argument scales = \"free_y\" dans la fonction facet_wrap(). Cet argument permet de ne pas imposer la même échelle pour l’axe des ordonnées des 2 graphiques. Ce choix est ici pertinent puisque les effectifs des 2 groupes sont très différents. Faîtes un essai sans cet argument pour voir la différence. Il est en revanche important de conserver le même axe des x afin de faciliter la comparaison des 2 groupes.\nCette visualisation nous montre que les données doivent suivre à peu près une distribution Normale dans les 2 groupes, et que globalement la longueur des cornes semble légèrement plus élevée dans le groupe des vivants (avec un mode autour de 25-26 mm) que dans le groupes des morts (avec un mode autour de 23-24 mm). L’étendue des données semble légèrement plus grande dans le groupe des vivants, mais cela n’est peut-être dû qu’à la différence marquée des tailles d’échantillons.\n\n\n11.5.3 Avec des diagrammes de densité facettés\n\nLizard %&gt;%\n  ggplot(aes(x = Horn_len)) +\n  geom_density(fill = \"firebrick2\", alpha = 0.5) +\n  geom_rug() +\n  facet_wrap(~Survival, ncol = 1, scales = \"free_y\") +\n  labs(x = \"Longueur des cornes (mm)\",\n       y = \"Densité\",\n       title = \"Distribution de la longueur des cornes dans 2 groupes de lézards cornus\",\n       subtitle = \"nb morts : 30, nb vivants : 154\")\n\n\n\n\nLes diagrammes de densité ressemblent ici beaucoup aux histogrammes. C’est normal car la taille des échantillons est importante (30 et 154 pour les groupes killed et living respectivement). C’était moins vrais dans les chapitres précédents car les tailles d’échantillons étaient plus faibles, et la forme des histogrammes dépendait alors beaucoup du nombre de classes que l’on choisissait de représenter. Avec des échantillons de grande taille (n ≥ 30), c’est moins problématique.\nEn général, il est donc inutile de faire à la fois les histogrammes et les diagrammes de densité. Choisissez l’un ou l’autre selon vos préférences et la situation.\n\n\n11.5.4 Avec des boites à moustaches\n\nLizard %&gt;%\n  ggplot(aes(x = Survival, y = Horn_len)) +\n  geom_boxplot(notch = TRUE) +\n  labs(x = \"Groupe de lézards\",\n       y = \"Longueur des cornes (mm)\",\n       title = \"Comparaison de 2 groupes\\nde lézards cornus\",\n       subtitle = \"nb morts : 30, nb vivants : 154\")\n\n\n\n\n\n\n\n\nNous visualisons ici encore plus clairement que sur les histogrammes le fait que les longueurs de cornes des individus vivants sont légèrement plus longues que celles des individus morts. D’ailleurs, puisque les intervalles de confiance à 95% des médianes des 2 groupes (les encoches) ne se chevauchent pas, un test de comparaison des moyennes devrait logiquement conclure à une différence significative en faveur des individus vivants. On peut également noter que la largeur de l’encoche pour les individus morts est plus importante que celle des vivants. Cela traduit une incertitude plus grande autour de la médiane estimée dans le groupe des individus morts. C’est tout à fait logique compte tenu des effectifs plus faibles dans ce groupe.\nEnfin, il est tout à fait possible de représenter sur le même graphique les boîtes à moustaches et les données brutes sous forme de stripchart. On a ainsi à la fois (i) une visualisation simplifiée de la position et de la dispersion des données avec les boîtes à moustache, et (ii) accès à l’ensemble des données brutes, ce qui permet parfois de voir des structures invisibles sur les boîtes à moustaches (regroupement de points par exemples). Afin de ne pas dupliquer les valeurs les plus extrêmes du jeu de données, nous indiquerons à geom_boxplot() de ne pas afficher les outliers sur le graphique : tous les points seront en effet déjà affichés par geom_jitter() :\n\nLizard %&gt;%\n  ggplot(aes(x = Survival, y = Horn_len, fill = Survival)) +\n  geom_boxplot(notch = TRUE, color = \"grey20\", alpha = 0.2,\n               outlier.color = NA, show.legend = FALSE) +\n  geom_jitter(height = 0, width = 0.2, alpha = 0.4, shape = 21,\n              show.legend = FALSE, size = 0.8) +\n  labs(x = \"Groupe de lézards\",\n       y = \"Longueur des cornes (mm)\",\n       title = \"Comparaison de 2 groupes\\nde lézards cornus\",\n       subtitle = \"nb morts : 30, nb vivants : 154\") +\n  scale_fill_manual(values = c(\"purple3\", \"royalblue2\"))"
  },
  {
    "objectID": "11-TwoSampleTests.html#le-test-paramétrique",
    "href": "11-TwoSampleTests.html#le-test-paramétrique",
    "title": "11  Comparaison de moyennes : deux échantillons indépendants",
    "section": "11.6 Le test paramétrique",
    "text": "11.6 Le test paramétrique\nLe test paramétrique le plus puissant que nous puissions faire pour comparer la moyenne de 2 populations est le test de Student. Ce test étant paramétrique, nous devons nous assurer que ses conditions d’application sont vérifiées avant de pouvoir le réaliser.\n\n11.6.1 Conditions d’application\nLes conditions d’application de ce test sont au nombre de 3 :\n\nChacun des deux échantillons est issu d’un échantillonnage aléatoire de la population générale. Comme toujours, en l’absence d’indication contraire, on considère que cette condition est toujours vérifiée.\nLa variable numérique étudiée est distribuée normalement dans les deux populations. Il nous faudra donc faire deux tests de Shapiro-Wilk, un pour chaque échantillon.\nLa variance de la variable numérique étudiée est la même dans les deux populations. C’est ce que l’on appelle l’homoscédasticité.\n\nEn réalité, le test du \\(t\\) de Student sur deux échantillons indépendants est assez robuste face au non respect de cette troisième condition d’application. Cela signifie que si cette troisième condition d’application n’est pas strictement vérifiée, les résultats du tests peuvent malgré tout rester valides. Lorsque les 2 échantillons comparés ont des tailles supérieures ou égales à 30, ce test fonctionne bien même si l’écart-type d’un groupe est jusqu’à 3 fois supérieur ou inférieur à l’écart-type du second groupe, à condition que la taille des 2 échantillons soit proche (ce qui n’est pas le cas ici !). En revanche, si les écart-types diffèrent de plus d’un facteur 3, ou si les tailles d’échantillons sont très différentes, le test du \\(t\\) de Student ne devrait pas être utilisé. De même, si la taille des échantillons est inférieure à 30 et que les variances ne sont pas homogènes, ce test ne devrait pas être réalisé. En conclusion, les résultats du test du \\(t\\) de Student à deux échantillons indépendants peuvent rester valides si la troisième condition d’homoscédasticité n’est pas respectée, mais dans certains cas seulement.\nLe test du \\(t\\) de Student sur deux échantillons indépendants est également assez robuste face à des écarts mineurs à la distribution Normale, tant que la forme des deux distributions comparées reste similaire et unimodale. En outre, la robustesse de ce test augmente avec la taille des échantillons.\n\n\n\n\n\n\nRobustesse\n\n\n\nLa robustesse d’un tests statistique est sa capacité à rester valide même lorsque certaines de ses conditions d’application ne sont pas parfaitement respectées. Plus un test est robuste, plus il est capable de supporter des “entorses” importantes à ses conditions d’application.}\n\n\nAu final, avec un peu d’habitude, même lorsque les conditions d’application ne sont pas toutes vérifiées, on peut parfois passer outre. Mais à ce stade, on préfère s’en tenir à des choses plus simples et claires.\n\n\n\n\n\n\nLa procédure à suivre\n\n\n\n\nFaites un test de Normalité pour chacune des deux séries de données. Si elles suivent la loi Normale toutes les deux, passez au point 2. Sinon, rendez-vous au point 4.\nFaites un test d’homoscédasticité (homogénéité des variances). Si les variances sont homogènes, passez au point 3. Sinon,rendez-vous au point 5.\nFaite un test de comparaison des moyennes paramétrique : le test de Student. Examinez la \\(p-\\)value pour conclure, et rendez-vous au point 6.\nFaite un test de comparaison des moyennes non paramétrique : le test de Wilcoxon de la somme des rangs. Examinez la \\(p-\\)value pour conclure, et rendez-vous au point 6.\nFaite un test de comparaison des moyennes non paramétrique : le test \\(t\\) de Welch. Examinez la \\(p-\\)value pour conclure, et rendez-vous au point 6.\nSi la \\(p-\\)value est supérieure à \\(\\alpha\\), on ne peut pas rejeter l’hypothèse nulle et on conclut alors à une absence de différence significative entre les 2 populations. À l’inverse, si la \\(p-\\)value est inférieure ou égale à \\(\\alpha\\), on rejette l’hypothèse nulle et on valide l’hypothèse alternative. Les deux populations ont des moyennes significativement différentes, et pour savoir laquelle est supérieure ou inférieure à l’autre, on revient aux estimations des moyennes et des intervalles de confiances à 95%, calculés dans la partie consacrée aux statistiques descriptives.\n\n\n\n\n11.6.1.1 Normalité des données\nNous commençons donc par tester la Normalité des 2 populations dont sont issus les échantillons, c’est le point 1 de la procédure détaillée ci-dessus. Pour les individus morts, les hypothèses sont les suivantes :\n\nH\\(_0\\) : la taille des cornes suit une distribution Normale dans la population des lézards cornus morts.\nH\\(_1\\) : la taille des cornes ne suit pas une distribution Normale dans la population des lézards cornus morts.\n\n\nLizard %&gt;%\n  filter(Survival == \"killed\") %&gt;%\n  pull(Horn_len) %&gt;%\n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.93452, p-value = 0.06482\n\n\n\nLa \\(p\\)-value est supérieure à \\(\\alpha = 0.05\\), donc on ne peut pas rejeter l’hypothèse nulle de normalité pour la taille des cornes de la population des lézards cornus morts (test de Shapiro-Wilk, \\(W = 0.93\\), \\(p = 0.065\\)).\n\nPour les individus vivants :\n\nH\\(_0\\) : la taille des cornes suit une distribution Normale dans la population des lézards cornus vivants.\nH\\(_1\\) : la taille des cornes ne suit pas une distribution Normale dans la population des lézards cornus vivants.\n\n\nLizard %&gt;%\n  filter(Survival == \"living\") %&gt;%\n  pull(Horn_len) %&gt;%\n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.96055, p-value = 0.0002234\n\n\n\nLa \\(p\\)-value est inférieure à \\(\\alpha = 0.05\\), donc on rejette l’hypothèse nulle de normalité pour la taille des cornes de la population des lézards cornus vivants (test de Shapiro-Wilk, \\(W = 0.96\\), \\(p &lt; 0.001\\)).\n\nL’une des 2 séries de données ne suit pas la loi Normale, nous sommes donc censés passer directement au point 4 de la procédure.\nToutefois, si l’on examine les histogrammes (Section 11.5.2) ou les diagrammes de densité (Section 11.5.3) des 2 échantillons, on constate que la forme des distributions des 2 séries de données est très proche. Pour les 2 échantillons, la distribution est en effet uni-modale, avec une asymétrie gauche assez marquée (une longue queue de distribution du côté gauche). La forme des distributions étant similaire (on parle bien de la forme des histogrammes et non de la position du pic), et les histogrammes étant proches de la forme typique d’une courbe en cloche, le test de Student devrait rester valide car il est robuste dans cette situation. Ici, pour l’exemple, on va donc passer au point 2 de la procédure. Notez toutefois que passer directement au point 4 de la procédure serait tout à fait correct : on ne pourrait rien vous reprocher si vous passez directement au test non paramétrique de comparaison des moyennes lorsque vous constatez que l’une des 2 séries de données ne suit pas la distribution Normale.\n\n\n11.6.1.2 Homogénéité des variances\nLe test le plus simple pour comparer la variance de 2 populations est le test \\(F\\) :\n\nH\\(_0\\) : la variance des 2 populations est égale, leur ratio vaut 1 \\(\\left(\\frac{\\sigma^2_{killed}}{\\sigma^2_{living}} = 1\\right)\\).\nH\\(_1\\) : la variance des 2 populations est différente, leur ratio ne vaut pas 1 \\(\\left(\\frac{\\sigma^2_{killed}}{\\sigma^2_{living}} \\neq 1\\right)\\).\n\n\nvar.test(Horn_len ~ Survival, data = Lizard)\n\n\n    F test to compare two variances\n\ndata:  Horn_len by Survival\nF = 1.0607, num df = 29, denom df = 153, p-value = 0.7859\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.6339331 1.9831398\nsample estimates:\nratio of variances \n          1.060711 \n\n\n\nIci, le ratio des variances (la variance des individus morts divisée par la variance des individus vivants) est très proche de 1 (\\(F = 1.06\\), IC 95% : [0.63 ; 1.98]). Le test \\(F\\) nous montre qu’il est impossible de rejeter H\\(_0\\) : au seuil \\(\\alpha = 0.05\\), le ratio des variances n’est pas significativement différent de 1 (ddl = 29 et 153, \\(p = 0.79\\)), les variances sont homogènes.\n\nLe test de Bartlett est un autre test qui permet de comparer la variance de plusieurs populations (2 ou plus). Lorsque le nombre de populations est égal à 2 (comme ici), ce test est absolument équivalent au test \\(F\\) ci-dessus.\n\nH\\(_0\\) : toutes les populations ont même variance (\\(\\sigma^2_A = \\sigma^2_B = \\sigma^2_C = \\cdots = \\sigma^2_N\\)).\nH\\(_1\\) : au moins une population a une variance différente des autres.\n\n\nbartlett.test(Horn_len ~ Survival, data = Lizard)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  Horn_len by Survival\nBartlett's K-squared = 0.042411, df = 1, p-value = 0.8368\n\n\nEnfin, le test de Levene (attention, le package car doit être chargé) devrait être préféré la plupart du temps. Comme le test de Bartlett, il permet de comparer la variance de plusieurs populations, mais il est plus robuste vis à vis de la non-normalité des données.\n\nH\\(_0\\) : toutes les populations ont même variance (\\(\\sigma^2_A = \\sigma^2_B = \\sigma^2_C = \\cdots = \\sigma^2_N\\)).\nH\\(_1\\) : au moins une population a une variance différente des autres.\n\n\n# Le test de Levene fait partie du package car. Il doit être chargé en mémoire\n# library(car)\nleveneTest(Horn_len ~ Survival, data = Lizard)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1  0.0035  0.953\n      182               \n\n\nIci encore, les conclusions sont les mêmes :\n\nIl est impossible de rejeter l’hypothèse nulle d’homogénéité des variances au seuil \\(\\alpha = 0.05\\) (test de Levene, \\(F\\) = 0.004, ddl = 1, \\(p = 0.953\\)).\n\nÀ vous de choisir lequel de ces 3 tests vous souhaitez réaliser : il est évident qu’on ne fait jamais les 3 !\nIci, puisque l’homoscédasticité est vérifiée, on passe au point 3 de la procédure.\n\n\n\n11.6.2 Réalisation du test et interprétation\nPuisque la taille des cornes du lézard cornu suit approximativement la même distribution “presque Normale” dans les 2 populations (lézards morts et vivants) et que ces 2 populations ont des variances homogènes, on peut réaliser le test du \\(t\\) de Student sur deux échantillons indépendants.\n\nH\\(_0\\) : la moyenne des 2 populations est égale, leur différence vaut 0 (\\(\\mu_{killed}-\\mu_{living} = 0\\)).\nH\\(_1\\) : la moyenne des 2 populations est différente, leur différence ne vaut pas 0 (\\(\\mu_{killed}-\\mu_{living} \\neq 0\\)).\n\n\nt.test(Horn_len ~ Survival, data = Lizard, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  Horn_len by Survival\nt = -4.3494, df = 182, p-value = 2.27e-05\nalternative hypothesis: true difference in means between group killed and group living is not equal to 0\n95 percent confidence interval:\n -3.335402 -1.253602\nsample estimates:\nmean in group killed mean in group living \n            21.98667             24.28117 \n\n\nNotez bien la syntaxe :\n\nNous utilisons ici la syntaxe du type “formule” faisant appel au symbole “~” (Longueur des cornes en fonction de la Survie) et à l’argument “data =”.\nL’argument “paired = TRUE” a disparu puisque nous avons ici deux échantillons indépendants\nL’argument “var.equal = TRUE” doit obligatoirement être spécifié : nous nous sommes assuré que l’homogénéité des variances était vérifiée. Il faut donc l’indiquer afin que le test de Student classique soit réalisé. Si on omet de le spécifier, c’est un autre test qui est réalisé (voir plus bas).\n\n\nAu seuil \\(\\alpha\\) de 5%, on rejette l’hypothèse nulle d’égalité des moyennes de la longueur des cornes entre lézards vivants et morts (test \\(t\\) de Student sur deux échantillons indépendant, \\(t = -4.35\\), ddl = 182, \\(p &lt; 0.001\\)). Les lézards morts ont en moyenne des cornes plus courtes (\\(\\hat{\\mu}_{killed} = 21.99\\) millimètres) que les lézards vivants (\\(\\hat{\\mu}_{living} = 24.28\\) millimètres). La gamme des valeurs les plus probables pour la différence de moyenne entre les deux populations est fournie par l’intervalle de confiance à 95% de la différence de moyennes : [-3.34 ; -1.25].\n\nCe test confirme donc bien l’impression des chercheurs : les lézards principalement pris pour cibles par les pies grièches migratrices ont des cornes en moyenne plus courtes (probablement entre 1.25 et 3.34 millimètres de moins) que les lézards de la population générale. Avoir des cornes plus longues semble donc protéger les lézards de la prédation, du moins dans une certaine mesure.\nNotez bien que l’intervalle de confiance à 95% qui est fourni avec les résultats du test est l’intervalle de confiance à 95% de la différence de moyenne entre les 2 groupes. Cet intervalle nous donne donc une idée de la magnitude de l’effet, de son ampleur. En effet, dire que les lézards morts ont des cornes en moyenne plus courtes est intéressant, mais cela n’aura pas la même portée si leurs cornes sont plus courtes de 0.02 millimètres ou si elles sont plus courtes de 5 millimètres. Un test statistique permet de rejeter ou non une hypothèse nulle, mais c’est bien l’estimation (la moyenne de chaque groupe et l’intervalle de confiance à 95% de la différence) qui nous dit ce qu’on doit penser des résultats, et de leur pertinence (écologique, biologique, physiologie, comportementale, etc.)."
  },
  {
    "objectID": "11-TwoSampleTests.html#lalternative-non-paramétrique",
    "href": "11-TwoSampleTests.html#lalternative-non-paramétrique",
    "title": "11  Comparaison de moyennes : deux échantillons indépendants",
    "section": "11.7 L’alternative non paramétrique",
    "text": "11.7 L’alternative non paramétrique\nSi les conditions d’application du test de Student ne sont pas vérifiées (c’est bien le cas ici puisque la longueur des cornes ne suit pas une distribution Normale dans la population des lézards vivants), notre procédure nous conduit à l’étape 4 : nous devons utiliser un équivalent non paramétrique au test de Student. C’est le cas du test de Wilcoxon sur la somme des rangs (également appelé test de Mann-Whitney). Comme pour tous les tests de Wilcoxon, la comparaison porte alors non plus sur les moyennes mais sur les médianes.\n\nH\\(_0\\) : la médiane des 2 populations est égale, leur différence vaut 0 (\\(med_{killed}-med_{living} = 0\\)).\nH\\(_1\\) : la médiane des 2 populations est différente, leur différence ne vaut pas 0 (\\(med_{killed}-med_{living}\\neq 0\\)).\n\n\nwilcox.test(Horn_len ~ Survival, data = Lizard, conf.int = TRUE)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  Horn_len by Survival\nW = 1181.5, p-value = 2.366e-05\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -3.200076 -1.300067\nsample estimates:\ndifference in location \n             -2.200031 \n\n\nL’argument var.equal = TRUE n’existe pas pour ce test, puisque c’est justement un test non paramétrique qui ne requiert pas l’homogénéité des variances. En revanche, comme pour tous les autres tests de Wilcoxon que nous avons réalisés jusqu’ici, l’argument conf.int = TRUE permet d’afficher les estimateurs pertinents, ici, la différence de médiane entre les 2 populations et l’intervalle de confiance à 95% de cette différence de médiane.\nLa conclusion est ici la même que pour le test de Student : puisque la \\(p\\)-value est très inférieure à \\(\\alpha\\), on rejette l’hypothèse nulle : les médianes sont bel et bien différentes."
  },
  {
    "objectID": "11-TwoSampleTests.html#lautre-alternative-non-paramétrique",
    "href": "11-TwoSampleTests.html#lautre-alternative-non-paramétrique",
    "title": "11  Comparaison de moyennes : deux échantillons indépendants",
    "section": "11.8 L’autre alternative non paramétrique",
    "text": "11.8 L’autre alternative non paramétrique\nEnfin, dans le cas où la variable étudiée suit la loi Normale dans les deux populations mais qu’elle n’a pas la même variance dans les deux populations (donc si vous arrivez au point 5 de la procédure décrite plus haut), il est toujours possible de réaliser un test de Wilcoxon, il est préférable de réaliser un test de Student modifié : le test approché du \\(t\\) de Welch. Ce test est moins puissant que le test de Student classique, mais il reste plus puissant que le test de Wilcoxon, et surtout, il permet de comparer les moyennes et non les médianes.\n\nH\\(_0\\) : la moyenne des 2 populations est égale, leur différence vaut 0 (\\(\\mu_{killed}-\\mu_{living} = 0\\)).\nH\\(_1\\) : la moyenne des 2 populations est différente, leur différence ne vaut pas 0 (\\(\\mu_{killed}-\\mu_{living} \\neq 0\\)).\n\n\nt.test(Horn_len ~ Survival, data = Lizard)\n\n\n    Welch Two Sample t-test\n\ndata:  Horn_len by Survival\nt = -4.2634, df = 40.372, p-value = 0.0001178\nalternative hypothesis: true difference in means between group killed and group living is not equal to 0\n95 percent confidence interval:\n -3.381912 -1.207092\nsample estimates:\nmean in group killed mean in group living \n            21.98667             24.28117 \n\n\nLa seule différence par rapport à la syntaxe du test \\(t\\) de Student paramétrique est la suppression de l’argument var.equal = TRUE. Attention donc, à bien utiliser la syntaxe correcte. Le test du \\(t\\) de Welch ne devrait être réalisé que lorsque la Normalité est vérifiée pour les 2 populations, mais pas l’homoscédasticité. Par rapport au test de Student classique, on constate que le nombre de degrés de libertés est très différent, et donc la \\(p\\)-value également. Les bornes de l’intervalle de confiance à 95% de la différence de moyenne sont différentes également puisque leur calcul a été fait en supposant que les 2 populations n’avaient pas même variance."
  },
  {
    "objectID": "11-TwoSampleTests.html#exercices-dapplication",
    "href": "11-TwoSampleTests.html#exercices-dapplication",
    "title": "11  Comparaison de moyennes : deux échantillons indépendants",
    "section": "11.9 Exercices d’application",
    "text": "11.9 Exercices d’application\n\n11.9.1 La taille des hommes et des femmes\nOn s’intéresse à la différence de taille supposée entre hommes et femmes. Le fichier HommesFemmes.xls contient les tailles en centimètres de 38 hommes et 43 femmes choisis au hasard parmi les étudiants de première année à La Rochelle Université. Importez, mettez en forme et analysez ces données. Vous prendrez soin de retirer les éventuelles valeurs manquantes, vous prendrez le temps d’examiner les données à l’aide de statistiques descriptives et de représentations graphiques adaptées, puis vous tenterez de répondre à la question suivante : les hommes et les femmes inscrits en première année à La Rochelle Université ont-il la même taille ? Si non, caractérisez cette différence de taille.\n\n\n11.9.2 La longueur du bec des manchots Adélie\nDans le jeu de données penguins du package palmerpenguins, récupérez les lignes du tableau qui correspondent aux manchots Adélie, et comparez la longueur des becs entre mâles et femelles. Comme toujours, avant de vous lancer dans les tests, vous prendrez soin de retirer les éventuelles valeurs manquantes, et vous prendrez le temps d’examiner les données à l’aide de statistiques descriptives et de représentations graphiques adaptées. Faites l’effort d’expliquer votre démarche, de préciser les hypothèses nulles et alternatives de chaque test, et de rédiger l’interprétation que vous faites de chaque résultat."
  },
  {
    "objectID": "11-TwoSampleTests.html#sec-bilat",
    "href": "11-TwoSampleTests.html#sec-bilat",
    "title": "11  Comparaison de moyennes : deux échantillons indépendants",
    "section": "11.10 Tests bilatéraux et unilatéraux",
    "text": "11.10 Tests bilatéraux et unilatéraux\n\n11.10.1 Principe\nJusqu’à maintenant, tous les tests que nous avons réalisés sont des tests bilatéraux. Pour chaque test, l’hypothèse nulle est imposée. En revanche, pour certains tests, l’hypothèse alternative est à choisir (et à spécifier) par l’utilisateur parmi 3 possibilités :\n\nUne hypothèse bilatérale. C’est celle qui est utilisée par défaut si l’utilisateur ne précise rien.\nDeux hypothèses unilatérales possibles, qui doivent être spécifiées explicitement par l’utilisateur.\n\nLes tests unilatéraux peuvent concerner tous les tests pour lesquels les hypothèses sont de la forme suivante :\n\nH\\(_0\\) : la valeur d’un paramètre de la population est égale à \\(k\\) (\\(k\\) peut être une valeur fixe, arbitraire, choisie par l’utilisateur, ou la valeur d’un paramètre d’une autre populations).\nH\\(_1\\) : la valeur d’un paramètre de la population n’est pas égale à \\(k\\).\n\nEn réalité, si nous remplaçons l’hypothèse H\\(_1\\) par :\n\nH\\(_1\\) : la valeur d’un paramètre de la population est supérieure à \\(k\\).\n\nou par :\n\nH\\(_1\\) : la valeur d’un paramètre de la population est inférieure à \\(k\\).\n\nnous réalisons un test unilatéral.\nDans RStudio, la syntaxe permettant de spécifier l’hypothèse alternative que nous souhaitons utiliser est toujours la même. Il faut préciser, au moment de faire le test l’argument suivant :\n\nalternative = \"two.sided\" : pour faire un test bilatéral. Si on ne le fait pas explicitement, c’est de toutes façons cette valeur qui est utilisée par défaut.\nalternative = \"greater\" : pour choisir l’hypothèse unilatérale “&gt;”.\nalternative = \"less\" : pour choisir l’hypothèse unilatérale “&lt;”.\n\nAttention : le choix d’utiliser “greater” ou “less” dépend donc de l’ordre dans lequel les échantillons sont spécifiés. Cette syntaxe est valable pour tous les tests de Student vus jusqu’ici (un échantillon, deux échantillons appariés, deux échantillons indépendants) et pour leurs alternatives non paramétriques (test de Wilcoxon des rangs signés, test de Wilcoxon de la somme des rangs, test du \\(t\\) de Welch).\n\n\n\n\n\n\nAttention\n\n\n\nL’utilisation de tests unilatéraux doit être réservée exclusivement aux situations pour lesquelles le choix de l’hypothèse unilatérale est possible à justifier par un mécanisme quelconque (biologique, physiologique, comportemental, écologique, génétique, évolutif, biochimique, etc.). Observer que l’un des échantillons a une moyenne plus grande ou plus faible qu’un autre lors de la phase des statistiques descriptives des données n’est pas du tout une raison suffisante. Il faut pouvoir justifier le choix de l’hypothèse alternative par une explication valable. D’ailleurs, si on veut être rigoureux, il faudrait toujours formuler les hypothèses que l’on souhaite tester avant de mettre en place le protocole expérimental et avant d’acquérir les données.\n\n\nPour s’embêter avec les tests unilatéraux puisqu’il est si rare qu’on ait le droit de les faire ? Tout simplement parce que toutes choses étant égales par ailleurs, un test unilatéral est toujours plus puissant (parfois, beaucoup plus puissant) qu’un test bilatéral. Or, la puissance est quelque chose qu’on cherche à maximiser (voir sec-puiss). Lorsqu’il est pertinent de réaliser un test unilatéral, on doit donc toujours le faire.\nReprenons l’un des exemples examinés précédemment pour mieux comprendre comment tout cela fonctionne.\n\n\n11.10.2 Un exemple pas à pas\nReprenons l’exemple des lézards cornus. L’étude a été réalisée parce que les chercheurs supposaient que la longueur des cornes des lézards était susceptible de leur fournir une protection face à la prédation. Autrement dit, les chercheurs supposaient que des cornes plus longues devaient fournir une meilleure protection vis à vis de la prédation. Ainsi, les lézards morts devaient avoir des cornes moins longues en moyenne que les les lézards vivants, simplement parce que porter des cornes courtes expose plus fortement les individus à la prédation. Nous avons donc une bonne raison “écologique/évolutive” de considérer un test unilatéral (la susceptibilité face à la prédation qui a entraîné une pression de sélection sur la longueur des cornes des lézards), avant même de collecter les données.\nLorsque nous avons examiné cette question, nous avons fait le test du \\(t\\) de Student sur échantillons indépendants de la façon suivante :\n\nt.test(Horn_len ~ Survival, data = Lizard, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  Horn_len by Survival\nt = -4.3494, df = 182, p-value = 2.27e-05\nalternative hypothesis: true difference in means between group killed and group living is not equal to 0\n95 percent confidence interval:\n -3.335402 -1.253602\nsample estimates:\nmean in group killed mean in group living \n            21.98667             24.28117 \n\n\nComme l’indiquent les résultats fournis, l’hypothèse alternative utilisée pour faire le test est : “La vraie différence de moyenne n’est pas égale à 0”. Autrement dit, nous avons fait un test bilatéral avec les hypothèses suivantes :\n\nH\\(_0\\) : la moyenne des 2 populations est égale, leur différence vaut 0 (\\(\\mu_{killed}-\\mu_{living} = 0\\)).\nH\\(_1\\) : la moyenne des 2 populations est différente, leur différence ne vaut pas 0 (\\(\\mu_{killed}-\\mu_{living} \\neq 0\\)).\n\nCe test est donc rigoureusement équivalent à celui-ci :\n\nt.test(Horn_len ~ Survival, data = Lizard, var.equal = TRUE,\n       alternative = \"two.sided\")\n\n\n    Two Sample t-test\n\ndata:  Horn_len by Survival\nt = -4.3494, df = 182, p-value = 2.27e-05\nalternative hypothesis: true difference in means between group killed and group living is not equal to 0\n95 percent confidence interval:\n -3.335402 -1.253602\nsample estimates:\nmean in group killed mean in group living \n            21.98667             24.28117 \n\n\nIci, nous souhaitons en fait réaliser un test unilatéral avec les hypothèses suivantes :\n\nH\\(_0\\) : la moyenne de longueur des cornes de la population des lézards morts est égale à celle des lézards vivants. Leur différence vaut 0 (\\(\\mu_{killed}-\\mu_{living} = 0\\)).\nH\\(_1\\) : la moyenne de longueur des cornes de la population des lézards morts est inférieure à celle des lézards vivants. Leur différence est inférieure à 0 (\\(\\mu_{killed}-\\mu_{living} &lt; 0\\)).\n\n\nt.test(Horn_len ~ Survival, data = Lizard, var.equal = TRUE,\n       alternative = \"less\")\n\n\n    Two Sample t-test\n\ndata:  Horn_len by Survival\nt = -4.3494, df = 182, p-value = 1.135e-05\nalternative hypothesis: true difference in means between group killed and group living is less than 0\n95 percent confidence interval:\n      -Inf -1.422321\nsample estimates:\nmean in group killed mean in group living \n            21.98667             24.28117 \n\n\nPuisque la \\(p\\)-value de ce test est inférieure à \\(\\alpha = 0.05\\), on rejette l’hypothèse nulle de l’égalité des moyennes. On valide donc l’hypothèse alternative : les lézards cornus morts ont en moyenne des cornes plus courtes que les lézards vivants. Cette différence de longueur de cornes est en faveur des lézards vivants et vaut très probablement au moins \\(1.4\\) millimètres (c’est l’intervalle de confiance à 95% de la différence de moyennes qui nous le dit).\nDernière chose importante : il ne faut pas se tromper dans le choix de l’hypothèse alternative. En effet, nous aurions pu tenter de tester exactement la même chose en formulant les hypothèses suivantes :\n\nH\\(_0\\) : la moyenne de longueur des cornes de la population des lézards vivants est égale à celle des lézards morts. Leur différence vaut 0 (\\(\\mu_{living}-\\mu_{killed} = 0\\)).\nH\\(_1\\) : la moyenne de longueur des cornes de la population des lézards vivants est supérieure à celle des lézards morts. Leur différence est supérieure à 0 (\\(\\mu_{living}-\\mu_{killed} &gt; 0\\)).\n\nCe test est normalement exactement le même que précédemment. Toutefois, si on essaie de le réaliser, on rencontre un problème :\n\nt.test(Horn_len ~ Survival, data = Lizard, var.equal = TRUE,\n       alternative = \"greater\")\n\n\n    Two Sample t-test\n\ndata:  Horn_len by Survival\nt = -4.3494, df = 182, p-value = 1\nalternative hypothesis: true difference in means between group killed and group living is greater than 0\n95 percent confidence interval:\n -3.166684       Inf\nsample estimates:\nmean in group killed mean in group living \n            21.98667             24.28117 \n\n\nIci, la \\(p\\)-value est très supérieure à \\(\\alpha\\) puisqu’elle vaut 1. Une \\(p\\)-value de 1 devrait toujours attirer votre attention. La conclusion devrait donc être que l’on ne peut pas rejeter H\\(_0\\) : les lézards morts et vivants ont en moyenne des cornes de même longueur. Nous savons pourtant que c’est faux.\nLe problème est ici liè à l’ordre des catégories “vivant” ou “mort” dans le facteur Survival du tableau Lizard. Les dernières lignes des tests que nous venons de faire indiquent la moyenne de chaque groupe, mais le groupe “killed” apparaît toujours avant le groupe “living”. C’est l’ordre des niveaux dans le facteur Survival qui doit dicter la syntaxe appropriée :\n\nLizard$Survival\n\n  [1] living living living living living living living living living living\n [11] living living living living living living living living living living\n [21] living living living living living living living living living living\n [31] living living living living living living living living living living\n [41] living living living living living living living living living living\n [51] living living living living living living living living living living\n [61] living living living living living living living living living living\n [71] living living living living living living living living living living\n [81] living living living living living living living living living living\n [91] living living living living living living living living living living\n[101] living living living living living living living living living living\n[111] living living living living living living living living living living\n[121] living living living living living living living living living living\n[131] living living living living living living living living living living\n[141] living living living living living living living living living living\n[151] living living living living killed killed killed killed killed killed\n[161] killed killed killed killed killed killed killed killed killed killed\n[171] killed killed killed killed killed killed killed killed killed killed\n[181] killed killed killed killed\nLevels: killed living\n\n\nPar défaut, dans RStudio, les niveaux d’un facteur sont classés par ordre alphabétique sauf si on spécifie manuellement un ordre différent. Ici, le niveau “killed” est donc le premier niveau du facteur, et “living” le second. Attention, on parle bien ici des niveaux, ou modalités, et non des données elles-mêmes. Ici, le premier lézard mesuré appartient à la catégorie living. Ça n’est pas ça qui est important : c’est bien l’ordre des niveaux qui compte, et on peut le vour tout en bas, après Levels: .... Lorsque l’on réalise un test de Student avec ces données (ou un test de Wilcoxon d’ailleurs), la différence de moyenne qui est examinée par le test est donc “moyenne des killed - moyenne des living”. Lorsque nous avons tapé ceci :\n\nt.test(Horn_len ~ Survival, data = Lizard, var.equal = TRUE,\n       alternative = \"greater\")\n\nnous avons donc en réalité posé les hypothèses suivantes :\n\nH\\(_0\\) : la moyenne de longueur des cornes de la population des lézards morts est égale à celle des lézards vivants. Leur différence vaut 0 (\\(\\mu_{killed}-\\mu_{living} = 0\\)).\nH\\(_1\\) : la moyenne de longueur des cornes de la population des lézards morts est supérieure à celle des lézards vivants. Leur différence est supérieure à 0 (\\(\\mu_{killed}-\\mu_{living} &gt; 0\\)).\n\nCe test est donc erroné, ce qui explique qu’il nous renvoie un résultat faux et une \\(p\\)-value de 1. Ici, puisque l’ordre des catégories est “killed” d’abord et “living” ensuite, la seule façon correcte de faire un test unilatéral qui a du sens est donc celle que nous avons réalisée en premier :\n\nt.test(Horn_len ~ Survival, data = Lizard, var.equal = TRUE,\n       alternative = \"less\")\n\nFaites donc toujours attention à l’ordre des catégories de vos facteurs pour ne pas vous tromper. Une façon simple de vérifier cet ordre et d’observer vos graphiques (par exemple, les boîtes à moustaches). L’ordre dans lequel les catégories apparaissent sur l’axe des x reflète l’ordre des catégorie du facteur porté par cet axe :\n\nLizard %&gt;% \n  ggplot(aes(x = Survival, y = Horn_len)) +\n  geom_boxplot()\n\n\n\n\n\n\n11.10.3 Exercice d’application\nReprenez chaque exemple et exercice traité depuis le premier chapitre et identifiez les situations où un test unilatéral aurait du sens. Si vous en trouvez, faites ce test et assurez-vous que les hypothèses choisies sont bien celles qui sont utilisées lors du test.\n\n\n\n\nFox, John, Sanford Weisberg, et Brad Price. 2023. car: Companion to Applied Regression. https://CRAN.R-project.org/package=car.\n\n\nHorst, Allison, Alison Hill, et Kristen Gorman. 2022. palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://CRAN.R-project.org/package=palmerpenguins.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, et Shannon Ellis. 2022. skimr: Compact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr.\n\n\nWickham, Hadley. 2023. tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, et Jennifer Bryan. 2023. readxl: Read Excel Files. https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, et Teun van den Brand. 2024. ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, et Davis Vaughan. 2023. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, et Jennifer Bryan. 2023. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nYoung, Kevin V., Edmund D. Brodie, et Edmund D. Brodie. 2004. « How the Horned Lizard Got Its Horns ». Science 304 (5667): 65‑65. https://doi.org/10.1126/science.1094790."
  },
  {
    "objectID": "12-ANOVA.html#sec-packages4",
    "href": "12-ANOVA.html#sec-packages4",
    "title": "12  Comparaison de moyennes : plus de 2 groupes",
    "section": "12.1 Pré-requis",
    "text": "12.1 Pré-requis\nComme pour chaque nouveau chapitre, je vous conseille de travailler dans un nouveau script que vous placerez dans votre répertoire de travail, et dans une nouvelle session de travail (Menu Session &gt; Restart R). Inutile en revanche de créer un nouveau Rproject : vos pouvez tout à fait avoir plusieurs script dans le même répertoire de travail et pour un même Rproject. Comme toujours, consultez le livre en ligne du semestre 3 si vous ne savez plus comment faire.\nSi vous êtes dans une nouvelle session de travail (ou que vous avez quitté puis relancé RStudio), vous devrez penser à recharger en mémoire les packages utiles. Dans ce chapitre, vous aurez besoin d’utiliser :\n\nle tidyverse (Wickham 2023), qui comprend notamment le package readr (Wickham, Hester, et Bryan 2023), pour importer facilement des fichiers .csv au format tibble, le package dplyr (Wickham et al. 2023), pour manipuler des tableaux, et le package ggplot2 (Wickham et al. 2024) pour les représentations graphiques.\nreadxl (Wickham et Bryan 2023), pour importer facilement des fichiers Excel au format tibble.\nskimr (Waring et al. 2022), qui permet de calculer des résumés de données très informatifs.\ncar (Fox, Weisberg, et Price 2023), qui permet d’effectuer le test de comparaison des variances de Levene.\nbroom (Robinson, Hayes, et Couch 2023), qui fait partie du tidyverse mais qu’il faut charger explicitement. La fonction tidy() de ce package nous permettra de “ranger” correctement les résultats de tests dans un tibble.\nDescTools (Signorell 2023), afin de réaliser un test spécifique de comparaisons multiples. N’oubliez pas de l’installer si nécessaire, avant de le charger en mémoire.\nle package palmerpenguins (Horst, Hill, et Gorman 2022) pour accéder au jeu de données penguins que nous utiliserons pour les exercices d’application.\n\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(skimr)\nlibrary(car)\nlibrary(broom)\nlibrary(DescTools)\nlibrary(palmerpenguins)\n\nVous aurez également besoin des jeux de données suivants que vous pouvez dès maintenant télécharger dans votre répertoire de travail :\n\nLight.csv\nInsectes.csv\n\nEnfin, je spécifie ici une fois pour toutes le thème que j’utiliserai pour tous les graphiques de ce chapitre. Libre à vous de choisir un thème différent ou de vous contenter du thème proposé par défaut :\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "12-ANOVA.html#contexte",
    "href": "12-ANOVA.html#contexte",
    "title": "12  Comparaison de moyennes : plus de 2 groupes",
    "section": "12.2 Contexte",
    "text": "12.2 Contexte\n\n\nVoyager dans un pays éloigné peut faire souffrir de décalage horaire. Habituellement, la resynchronisation de l’horloge interne circadienne dans le nouveau fuseau horaire est réalisée grâce à la perception de la lumière par les yeux. Ce changement progressif du rythme de notre horloge interne est appelé “décalage de phase”. Ce phénomène a été étudié par 2 chercheurs en 1998 (Campbell et Murphy 1998), qui ont montré que ce décalage de phase pouvait également être obtenu en exposant des sujets à la lumière, non pas au niveau de leurs yeux, mais au niveau de leur fosse (ou creux) poplitée, c’est-à-dire, derrière les genoux.\nCette découverte a été vivement critiquée par certains, et saluée comme une découverte majeure par d’autres. Toutefois, certains aspects du design expérimental de l’étude de 1998 ont été mis en doute en 2002 : il semble en effet que lors de l’exposition du creux poplité, les yeux de certains patients ont été également exposés à de faibles intensités lumineuses. Pour vérifier les trouvailles de Campbell et Murphy, Wright et Czeisler (Wright et Czeisler 2002) ont ré-examiné ce phénomène. La nouvelle expérience a évalué les rythmes circadiens en mesurant les cycles quotidiens de production de mélatonine chez 22 participants placés au hasard dans 3 groupes. Les patients étaient réveillés en pleine nuit et exposés :\n\nSoit à 3 heures de lumière appliquée exclusivement derrière leurs genoux (groupe knee).\nSoit à 3 heures de lumière appliquée exclusivement à leurs yeux (groupe eyes).\nSoit à 3 heures d’obscurité totale (groupe control).\n\nLe décalage de phase du cycle de production de mélatonine était mesuré 48h plus tard. Des chiffres négatifs indiquent un retard de production de mélatonine. C’est l’effet théorique attendu du traitement lumineux administré. Un décalage de phase positif indique une production de mélatonine plus précoce. Une absence de changement se traduit par un décalage de phase de 0."
  },
  {
    "objectID": "12-ANOVA.html#importation-et-mise-en-forme-des-données",
    "href": "12-ANOVA.html#importation-et-mise-en-forme-des-données",
    "title": "12  Comparaison de moyennes : plus de 2 groupes",
    "section": "12.3 Importation et mise en forme des données",
    "text": "12.3 Importation et mise en forme des données\nLes données brutes de cette étude sont fournies dans le fichier Light.csv. Importez ces données dans RStudio et examinez les données brutes grâce à la fonction View().\n\nLight\n\n# A tibble: 22 × 2\n   treatment shift\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 control    0.53\n 2 control    0.36\n 3 control    0.2 \n 4 control   -0.37\n 5 control   -0.6 \n 6 control   -0.64\n 7 control   -0.68\n 8 control   -1.27\n 9 knee       0.73\n10 knee       0.31\n# ℹ 12 more rows\n\n\nLe tableau obtenu est-il au format long ou au format court/large ? Pourquoi un tableau au format suivant n’aurait-il pas de sens ?\n\n\n# A tibble: 8 × 3\n  control  eyes  knee\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.53 -0.78  0.73\n2    0.36 -0.86  0.31\n3    0.2  -1.35  0.03\n4   -0.37 -1.48 -0.29\n5   -0.6  -1.52 -0.56\n6   -0.64 -2.04 -0.96\n7   -0.68 -2.83 -1.61\n8   -1.27 NA    NA   \n\n\nLorsque l’on réalise une analyse de variance, puisque les effectifs ne sont pas nécessairement identiques dans tous les groupes (c’est ce qu’on appelle un design déséquilibré, ou “unbalanced design”), présenter les tableaux au format long est indispensable. Par ailleurs, notez que les ANOVAs réalisées sur des “balanced design” (ou designs équilibrés, pour lesquels tous les groupes sont de même taille), sont beaucoup plus puissantes que les ANOVAs réalisées sur des “unbalanced designs”.\nIci, le tableau de données est très simple (et de petite taille). Il n’y a pas de données manquantes et aucune création de nouvelle variable n’est nécessaire. La seule modification que nous devrions faire est de transformer la variable treatment en facteur :\n\nLight &lt;- Light %&gt;%\n  mutate(treatment = factor(treatment))\n\nComme toujours, les niveaux du facteur sont automatiquement classés par ordre alphabétique :\n\nlevels(Light$treatment)\n\n[1] \"control\" \"eyes\"    \"knee\"   \n\n\nPour les statistiques descriptives et les graphiques qui viendront après, nous souhaitons indiquer l’ordre suivant : control, puis knee, puis eyes :\n\nLight &lt;- Light %&gt;%\n  mutate(treatment = fct_relevel(treatment, \"control\", \"knee\", \"eyes\"))\n\nLight\n\n# A tibble: 22 × 2\n   treatment shift\n   &lt;fct&gt;     &lt;dbl&gt;\n 1 control    0.53\n 2 control    0.36\n 3 control    0.2 \n 4 control   -0.37\n 5 control   -0.6 \n 6 control   -0.64\n 7 control   -0.68\n 8 control   -1.27\n 9 knee       0.73\n10 knee       0.31\n# ℹ 12 more rows\n\nLight$treatment\n\n [1] control control control control control control control control knee   \n[10] knee    knee    knee    knee    knee    knee    eyes    eyes    eyes   \n[19] eyes    eyes    eyes    eyes   \nLevels: control knee eyes\n\n\nAttention à bien respecter la casse (le respect des majuscules/minuscules est toujours aussi important dans RStudio)."
  },
  {
    "objectID": "12-ANOVA.html#exploration-statistique-des-données",
    "href": "12-ANOVA.html#exploration-statistique-des-données",
    "title": "12  Comparaison de moyennes : plus de 2 groupes",
    "section": "12.4 Exploration statistique des données",
    "text": "12.4 Exploration statistique des données\nComme toujours, et maintenant que nos données sont au bon format, il est nécessaire d’examiner quelques statistiques descriptives pour chaque catégorie étudiée. On peut tout d’abord commencer par examiner la taille de chaque échantillon :\n\nLight %&gt;%\n  count(treatment)\n\n# A tibble: 3 × 2\n  treatment     n\n  &lt;fct&gt;     &lt;int&gt;\n1 control       8\n2 knee          7\n3 eyes          7\n\n\nNous avons ici la confirmation que le design expérimental n’est pas équilibré, puisque le groupe control compte un individu de plus. Nous pouvons ensuite utiliser la fonction skim du package skimr pour obtenir un résumé des données :\n\nLight %&gt;%\n  group_by(treatment) %&gt;%\n  skim()\n\n── Data Summary ────────────────────────\n                           Values    \nName                       Piped data\nNumber of rows             22        \nNumber of columns          2         \n_______________________              \nColumn type frequency:               \n  numeric                  1         \n________________________             \nGroup variables            treatment \n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable treatment n_missing complete_rate   mean    sd    p0   p25\n1 shift         control           0             1 -0.309 0.618 -1.27 -0.65\n2 shift         knee              0             1 -0.336 0.791 -1.61 -0.76\n3 shift         eyes              0             1 -1.55  0.706 -2.83 -1.78\n     p50   p75  p100 hist \n1 -0.485  0.24  0.53 ▂▇▂▁▇\n2 -0.29   0.17  0.73 ▃▃▇▃▇\n3 -1.48  -1.10 -0.78 ▂▂▁▇▅\n\n\nIl semble que le groupe eyes se comporte un peu différemment des autres groupes. En effet, pour les groupes control et knee, les valeurs des indices de position observés sont très proches :\n\nles moyennes et les médianes sont négatives mais proches de 0.\nles valeurs observées sont négatives pour certaines, et positives pour d’autres (la colonne p0 contient les minimas et la colonne p100 contient les maximas).\n\nEn revanche, pour le groupe eyes, les décalages de phase observés sont tous négatifs (le maximum, présenté dans la colonne p100 vaut -0.78) et la moyenne est près de 5 fois plus faible que pour les 2 autres groupes.\nConcernant la dispersion, les écart-types semblent en revanche très proches dans les 3 groupes (entre 0.6 et 0.8).\nEnfin, les histogrammes présentés pour chaque groupe semblent très éloignés d’une distribution Normale. C’est logique compte tenu des faibles effectifs dans chaque groupe. Nous verrons plus tard que cela n’a aucune importance puisque les conditions d’application de l’ANOVA portent sur les résidus de l’ANOVA (nous verrons plus loin de quoi il s’agit), et pas sur les données brutes.\nIl semble donc que seul le groupe eyes soit véritablement différent du groupe témoin. Pour le vérifier, on peut calculer les intervalles de confiance à 95% des moyennes. Nous examinerons ensuite quelques graphiques, puis nous ferons un test d’hypothèses.\n\nLight %&gt;% \n  reframe(mean_cl_normal(shift), .by = treatment)\n\n# A tibble: 3 × 4\n  treatment      y   ymin   ymax\n  &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 control   -0.309 -0.825  0.208\n2 knee      -0.336 -1.07   0.396\n3 eyes      -1.55  -2.20  -0.898\n\n\nLà encore, le groupe eyes semble assez différent des 2 autres. L’intervalle de confiance à 95% de ce groupe ([-2.20 ; -0.90]) est totalement disjoint du groupe knee ([-1.07 ; 0.40]) et chevauche à peine celui du groupe control ([-0.82 ; 0.21]). L’intervalle de confiance du groupe kneerecouvre en revanche en totalité celui du groupe control. Il n’y aura donc vraisemblablement pas de différence significative entre ces 2 groupes, mais une différence significative entre le groupe eyes et les 2 autres. Pour visualiser un peu mieux ces résultats préliminaires, examinons quelques graphiques."
  },
  {
    "objectID": "12-ANOVA.html#exploration-graphique",
    "href": "12-ANOVA.html#exploration-graphique",
    "title": "12  Comparaison de moyennes : plus de 2 groupes",
    "section": "12.5 Exploration graphique",
    "text": "12.5 Exploration graphique\nComme toujours, il est indispensable de regarder à quoi ressemblent les données brutes sur un ou des graphiques. Les statistiques descriptives ne racontent en effet pas toujours toute l’histoire. Ici, nous allons superposer les données brutes, sous forme de nuage de points, aux boites à moustaches :\n\nLight %&gt;%\n  ggplot(aes(x = treatment, y = shift, fill = treatment)) +\n  geom_boxplot(notch = TRUE, show.legend = FALSE, \n               alpha = 0.3, outlier.colour = NA) +\n  geom_jitter(width = 0.2, shape = 21, show.legend = FALSE) +\n  labs(x = \"traitement\", y = \"Décalage de phase (sans unité)\") +\n  scale_fill_brewer(palette = \"Accent\")\n\nNotch went outside hinges\nℹ Do you want `notch = FALSE`?\nNotch went outside hinges\nℹ Do you want `notch = FALSE`?\nNotch went outside hinges\nℹ Do you want `notch = FALSE`?\n\n\n\n\n\nPuisqu’il y a peu de données, les intervalles de confiance à 95% sont très larges. Ils dépassent d’ailleurs presque systématiquement les quartiles, ce qui explique l’apparence bizarre des boîtes à moustaches et les messages d’avertissement affichés lors de la création du graphique. Il vaudait donc mieux représenter cette figure sans ces intervalles de confiance. Toutefois, avant de les retirer, on peut constater ici que les IC 95% se chevauchent complètement pour les séries control et knee. En revanche, il n’y a aucun chevauchement de l’IC 95% du groupe eyes avec les 2 autres groupes. Ces réslutats sont très légèrement différents de ceux obtenus plus haut, car on examine ici les intervalles de confiance à 95% des médianes, alors qu’on regardait les intervalles de confiance à 95% des moyennes dans la section précédente. Les conclusions sont toutefois les mêmes : on s’attend donc à trouver une différence de moyenne significative entre le groupe eyes d’une part, et les groupes control et knee d’autre part, mais pas de différence de moyenne entre les groupes control et knee.\n\nLight %&gt;%\n  ggplot(aes(x = treatment, y = shift, fill = treatment)) +\n  geom_boxplot(show.legend = FALSE, \n               alpha = 0.3, outlier.colour = NA) +\n  geom_jitter(width = 0.2, shape = 21, show.legend = FALSE) +\n  labs(x = \"traitement\", y = \"Décalage de phase (sans unité)\") +\n  scale_fill_brewer(palette = \"Accent\")\n\n\n\n\nOn constate ici visuellement que les 3 séries ont une étendue à peu près similaire, et que le groupe eyes semble se distinguer des 2 autres par des valeurs plus faibles. Enfin, les boîtes contenant 50% des valeurs centrales (donc l’étendue des valeurs entre les premiers et troisièmes quartiles) recouvrent le 0 pour les 2 groupes control et knee, mais par pour eyes.\nL’examen d’un graphique de densité facetté donne les mêmes informations :\n\nLight %&gt;% \n  ggplot(aes(x = shift, fill = treatment)) +\n  geom_density(show.legend = FALSE, alpha = 0.5) +\n  geom_rug() +\n  facet_wrap(~treatment, ncol = 1) +\n  scale_fill_brewer(palette = \"Accent\") +\n  labs(x = \"Décalage de phase (sans unité)\", y = \"Densité\")\n\n\n\n\nUn package utile lorsque l’on dispose d’un grand nombre de groupes que l’on souhaite comparer à l’aide de graphiques de densité est le package ggridges :\n\nlibrary(ggridges)\nLight %&gt;% \n  ggplot(aes(x = shift, y = treatment, fill = treatment)) +\n  geom_density_ridges(show.legend = FALSE, alpha = 0.5) +\n  scale_fill_brewer(palette = \"Accent\") +\n  labs(x = \"Décalage de phase (sans unité)\", y = \"Densité\")\n\nPicking joint bandwidth of 0.366"
  },
  {
    "objectID": "12-ANOVA.html#le-test-paramétrique",
    "href": "12-ANOVA.html#le-test-paramétrique",
    "title": "12  Comparaison de moyennes : plus de 2 groupes",
    "section": "12.6 Le test paramétrique",
    "text": "12.6 Le test paramétrique\nLe test paramétrique permettant de comparer la moyenne de plusieurs populations en une seule étape est l’analyse de variance à un facteur. Contrairement aux tests que nous avons vus jusqu’à maintenant, les conditions d’application de ce test ne seront vérifiées qu’après avoir réalisé l’analyse. En effet, les conditions d’application de l’ANOVA ne se vérifient pas sur les données brutes mais sur les résidus de l’ANOVA. C’est d’ailleurs ce que l’on appelle l’analyse des résidus, ou diagnostique de l’ANOVA.\n\n12.6.1 Réalisation du test\nDans R, l’analyse de variance se fait grâce à la fonction aov() (comme “Analysis Of Variance”). La syntaxe est la même que pour un certain nombre de tests déjà vus dans les chapitres précédents : il faut fournir une formule à la fonction. On place la variable numérique expliquée à gauche du ~, et à droite, la variable qualitative explicative (le facteur).\nContrairement aux autres tests réalisés jusqu’ici, les résultats du test devront être sauvegardés dans un objet. Outre les résultats du test, cet objet contiendra également tous les éléments permettant de vérifier si les conditions d’application de l’ANOVA sont réunies ou non.\nLes hypothèses testées sont les suivantes :\n\nH\\(_0\\) : les moyennes de toutes les populations sont égales (\\(\\mu_{\\textrm{control}} = \\mu_{\\textrm{knee}} = \\mu_{\\textrm{eyes}}\\)).\nH\\(_1\\) : toutes les moyennes ne sont pas égales. Au moins l’une d’entre elles diffère des autres.\n\n\n# Réalisation de l'ANOVA 1 facteur\nres &lt;- aov(shift ~ treatment, data = Light)\n\n# Affichage des résultats\nres\n\nCall:\n   aov(formula = shift ~ treatment, data = Light)\n\nTerms:\n                treatment Residuals\nSum of Squares   7.224492  9.415345\nDeg. of Freedom         2        19\n\nResidual standard error: 0.7039492\nEstimated effects may be unbalanced\n\n\nL’affichage des résultats bruts ne nous apprend que peu de choses. En revanche, la fonction summary() donne la réponse du test :\n\nsummary(res)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \ntreatment    2  7.224   3.612   7.289 0.00447 **\nResiduals   19  9.415   0.496                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPour le facteur étudié (treatment), on obtient le nombre de degrés de libertés (Df), la somme des carrés (Sum Sq), les carrés moyens (Mean Sq), la statistique du test (F) et la \\(p-\\)value (Pr(&gt;F)). Ici, la \\(p-\\)value est inférieure à \\(\\alpha\\), donc on rejette H\\(_0\\). Au moins l’une des moyennes est différente des autres.\nAvant d’aller plus loin dans l’interprétation de ces résultats, il nous faut déterminer si nous avions bel et bien le droit de réaliser cette ANOVA, en vérifiant si les conditions d’application de l’ANOVA sont remplies.\n\n\n12.6.2 Conditions d’application\nL’ANOVA est un test paramétrique, et comme pour tous les tests paramétriques, des conditions d’application doivent être vérifiées pour avoir le droit d’effectuer le test. À la différence des autres tests paramétriques que nous avons réalisés jusqu’ici, les conditions d’application de l’ANOVA ne doivent pas être vérifiées avant de faire le test, mais après.\nLes résultats de l’ANOVA ne seront donc valides que si les conditions d’application sont vérifiées. Comme indiqué plus haut, ces conditions d’application doivent être vérifiées sur les résidus de l’ANOVA, donc nécessairement après avoir réalisé l’analyse. Les résidus de l’ANOVA représentent l’écart entre chaque observation et la moyenne de son groupe, et ils sont calculés au moment ou nous réalisons l’ANOVA.\n\n\n\n\n\n\nImportant\n\n\n\nLes conditions d’application de l’ANOVA ne se vérifient pas sur les données brutes comme c’est le cas du test de Student, mais sur les résidus de l’ANOVA, qui sont calculés au moment où l’ANOVA est réalisée. Par conséquent, on ne vérifie pas les conditions d’application avant mais bien après avoir fait le test.\nÇa n’est que si les conditions d’application sont remplies qu’on aura le droit d’interpréter les résultats de l’ANOVA.\n\n\nPour que l’ANOVA soit valide, les résidus doivent :\n\nÊtre indépendants.\nÊtre homogènes.\nÊtre distribués normalement.\n\n\n12.6.2.1 Indépendance des résidus\nL’indépendance des résidus signifie que connaître la valeur d’un résidu ne permet pas de prédire la valeur d’un autre résidu. Si les données ont été collectées correctement (échantillonnage aléatoire simple, indépendance des observations), on considère généralement que cette condition est vérifiée. Les 2 autres conditions d’application se vérifient soit graphiquement, soit avec un test d’hypothèses.\n\n\n12.6.2.2 Homogénéité des résidus\nL’homogénéité des résidus signifie que les résidus doivent avoir à peu près la même variance pour chacun des groupes comparés. On peut vérifier que cette condition d’application est vérifiée grâce à ce graphique, qui représente les résidus (residuals, sur l’axe des y) en fonction des valeurs ajustées (c’est-à-dire la moyenne de chaque groupe, fitted values, sur l’axe des x) :\n\nplot(res, which = 1)\n\n\n\n\nIci, les résidus sont considérés comme homogènes car nous avons à peu près autant de résidus positifs que négatifs et que la ligne rouge est très proche du 0. L’étalement (vertical) des résidus est à peu près le même de la gauche à la droite du graphique. On pourrait donc faire entrer les résidus dans une boite rectangulaire horizontale centrée sur le 0 et ayant la même largeur d’un bout à l’autre du graphique.\nCi-dessous, j’affiche quelques exemples de situations où les résidus ne sont pas homogènes afin que vous puissiez voir à quoi ressemblent les graphiques des résidus en fonction des valeurs ajustées dans ce type de situation :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPour ces 4 graphiques, les résidus ne rentrent pas dans une boîte rectangulaire qui a la même hauteur d’un bout à l’autre du graphique. Les résidus de ces ANOVAs fictives ne sont donc pas homogènes et les conditions d’application de l’ANOVA ne sont donc pas réunies.\nMais revenons à nos données de décalage de phase. Une autre façon de visualiser les résidus est d’utiliser le graphique suivant :\n\nplot(res, which = 3)\n\n\n\n\nSur ce graphique, ce qui compte principalement, c’est la droite en rouge. Elle est ici presque horizontale, ce qui montre que les résidus de tous les groupes (un groupe à gauche et 2 à droite) ont à peu près même moyenne.\nEnfin, cette condition d’homogénéité des résidus entre les groupes peut également être vérifiée grâce au test de Levene. Pour ce test, les hypothèses seront les suivantes :\n\nH\\(_0\\) : les résidus sont homogènes (i.e. identiques dans tous les groupes).\nH\\(_1\\) : les résidus ne sont pas homogènes (i.e. au moins un groupe présente des résidus dont la variance est différente des autres).\n\n\nleveneTest(res$residuals ~ Light$treatment)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  2  0.1586 0.8545\n      19               \n\n\nIci, puisque \\(p &gt; \\alpha\\), on ne peut pas rejeter l’hypothèse nulle. Les résidus sont donc bien homogènes.\nReste à vérifier la normalité des résidus.\n\n\n12.6.2.3 Normalité des résidus\nComme pour l’homogénéité des résidus, leur normalité peut être examinée graphiquement ou avec un test statistique :\n\nplot(res, which = 2)\n\n\n\n\nSur un graphique quantile-quantile comme celui-là, on considère que les observations sont distribuées normalement si les points sont bien alignés sur la droite. Ici, la plupart des points sont très proches de la droite, ce qui laisse penser que les résidus suivent bien la loi Normale. Mais il est souvent difficile, surtout pour les néophytes, de savoir à partir de quel écart entre les points et la droite il faut considérer que les résidus n’ont pas une distribution Normale.\nOn peut donc confirmer (ou non !) notre première impression avec le test de Normalité de Shapiro-Wilk. Il s’agit du même test de Normalité que nous utilisions sur les données brutes pour vérifier les conditions d’application du test de Student. Ici, on applique ce test sur les résidus de l’ANOVA :\n\nshapiro.test(res$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  res$residuals\nW = 0.95893, p-value = 0.468\n\n\nComme pour tous les tests de Shapiro-Wilk, l’hypothèse nulle est la normalité des observations. Ici, puisque \\(p &gt; \\alpha\\), on ne peut pas rejeter H\\(_0\\). Les résidus suivent donc bien la loi Normale.\nToutes les conditions d’application de l’ANOVA sont donc vérifiées. Nous avions donc bien le droit de la réaliser et ses résultats sont valides.\nDernière chose, il est possible de produire les 3 graphiques ci-dessus (et même un quatrième que nous ne décrirons pas ici), en une seule commande :\n\nplot(res)\n\nIl faut alors presser la touche Entrée de votre clavier pour afficher successivement les 4 graphiques produits.\n\n\n\n12.6.3 Interprétation des résultats\nMaintenant que nous avons la confirmation que les conditions d’application sont vérifiées, revenons aux résultats de l’ANOVA :\n\nsummary(res)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \ntreatment    2  7.224   3.612   7.289 0.00447 **\nResiduals   19  9.415   0.496                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nComme indiqué plus haut, la première ligne du tableau d’ANOVA contient toutes les informations pertinentes pour interpréter ces résultats. En particulier, la dernière valeur de la ligne correspond à la \\(p-\\)value. Ici, elle est inférieure au seuil \\(\\alpha\\) de 0.05. On pourrait rédiger les résultats de cette analyse ainsi :\n\nUne analyse de variance montre que la moyenne des 3 groupes n’est pas identique (\\(F = 7.289\\), \\(p = 0.004\\)). Un test de Levene a permis de vérifier la condition d’homogénéité de la variance des résidus (\\(F = 0.189\\), \\(p = 0.855\\)), et un test de Shapiro-Wilk a confirmé la normalité des résidus (\\(W = 0.959\\), \\(p = 0.468\\)).\n\nAinsi, puisque \\(p &lt; \\alpha\\), on rejette \\(H_0\\). ON a donc bien montré que tous les groupes n’avaient pas la même moyenne. Mais à ce stade, on ne sait pas encore si tous les groupes ont des moyennes strictement différentes les unes des autres, ou si seul un groupe (et lequel) présente une moyenne différente des 2 autres.\nÀ l’issue de cette analyse, deux questions restent donc en suspens :\n\nEntre quels groupes les moyennes sont-elles significativement différentes ?\nQuelle est la magnitude de ces différences ?\n\nPour répondre à ces 2 questions, il nous faut réaliser des tests a posteriori ou tests post-hoc.\n\n\n\n\n\n\nLes tests post-hoc\n\n\n\nLes tests post-hoc doivent être réalisés uniquement si l’hypothèse nulle de l’ANOVA est rejetée. Ils sont alors nécessaires pour déterminer entre quels groupes les moyennes sont significativement différentes. Si à l’inverse, l’ANOVA n’a pas permis de rejeter \\(H_0\\), alors on peut conclure à l’absence de différence de moyenne entre les groupes (i.e. toutes les moyennes sont égales), et les tests post-hoc n’ont alors aucun intérêt.\n\n\n\n\n12.6.4 Tests a posteriori ou tests post-hoc\nLorsqu’une ANOVA montre que tous les groupes n’ont pas la même moyenne, il faut en théorie effectuer toutes les comparaisons de moyennes deux à deux possibles. Le problème est que lorsque l’on effectue des comparaisons multiples, les erreurs \\(\\alpha\\) (probabilité de rejeter à tort H\\(_0\\)) de tous les tests s’ajoutent. Ainsi :\n\npour comparer 3 groupes 2 à 2, nous avons besoin de 3 tests.\nPour comparer 4 groupes 2 à 2, nous avons besoin de 6 tests.\npour comparer 5 groupes 2 à 2, nous avons besoin de 10 tests.\npour comparer 6 groupes 2 à 2, nous avons besoin de 15 tests.\npour comparer k groupes 2 à 2, nous avons besoin de \\(\\frac{k(k-1)}{2}\\) tests.\n\nIci, puisque pour chaque test, un risque \\(\\alpha\\) de 5% de rejeter à tort l’hypothèse nulle est commis, réaliser 3 tests ferait monter le risque de s’être trompé quelque part à 15%. C’est la raison pour laquelle des tests spécifiques existent. Nous en verrons 2 : le test de comparaisons multiples de Student et le test de Tukey (ou “Honestly Significant Difference Test”). Pour ces tests, des précautions sont prises qui garantissent que le risque \\(\\alpha\\) global (à l’issue de l’ensemble des tests) est maîtrisé et qu’il reste fixé à 5%, quel que soit le nombre de comparaisons effectuées.\n\n12.6.4.1 Comparaisons multiples de Student\nLe test de comparaisons multiples de Student est réalisé avec la fonction pairwise.t.test(). En réalité, ici, 3 tests de Student seront réalisés. Les \\(p-\\)values des tests seront simplement modifiées afin que globalement, le risque \\(\\alpha\\) n’augmente pas. Pour chaque test réalisé, les hypothèses nulles et alternatives sont les mêmes que celles décrites à la Section 11.6.2 :\n\nH\\(_0\\) : la moyenne des deux populations est égale (\\(\\mu_1 = \\mu_2\\), soit \\(\\mu_1 - \\mu_2\\) = 0).\nH\\(_1\\) : la moyenne des deux populations est différente (\\(\\mu_1 \\neq \\mu_2\\), soit \\(\\mu_1 - \\mu_2 \\neq 0\\)).\n\nAttention, pour ce test, la syntaxe “formules”, qui utilise le tilde (~) n’est pas possible. Il faut obligatoirement fournir à la fonction 2 objets : la colonne contenant la variable expliquée numérique, et la colonne (facteur) contenant les catégories (ici, le facteur contenant le type de traitement appliqué à chaque individu lors de l’expérience) :\n\n# Réalisation du test\npost_hoc1 &lt;- pairwise.t.test(Light$shift, Light$treatment)\n\n# affichage des résultats\npost_hoc1\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  Light$shift and Light$treatment \n\n     control knee  \nknee 0.9418  -     \neyes 0.0088  0.0088\n\nP value adjustment method: holm \n\n\nSeules les \\(p-\\)values de chaque test sont fournies sous la forme d’une demi-matrice. On constate ainsi qu’une seule \\(p-\\)value est supérieure à \\(\\alpha = 0.05\\) : celle du test comparant les moyennes des groupes knee et control. Une autre façon de visualiser ces résultats consiste à utiliser la fonction tidy() du package broom que nous avons mis en mémoire un peu plus tôt. Les résultats seront les mêmes. Ils seront simplement rangés dans un tibble :\n\ntidy(post_hoc1)\n\n# A tibble: 3 × 3\n  group1 group2  p.value\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n1 knee   control 0.942  \n2 eyes   control 0.00879\n3 eyes   knee    0.00880\n\n\nNous avons donc la confirmation que les moyennes des groupes knee et control ne sont pas significativement différentes l’une de l’autre. En revanche, la moyenne du groupe eyes est différente de celle des 2 autres groupes (\\(p = 0.009\\) pour les 2 tests).\nNous avons donc appris des choses nouvelles, mais nous ne savons toujours pas quelle est la magnitude de la différence détectée entre le groupe eyes et les 2 autres. Le test de Tukey HSD nous permet de répondre à cette question.\n\n\n12.6.4.2 Test de Tukey\nCe test est souvent plus avantageux que le test des comparaisons multiples de Student, car outre la \\(p-\\)value de chaque comparaison deux à deux, il renvoie des informations concernant les différences de moyennes entre chaque paire de modalités du facteur étudié, et les intervalles de confiance à 95% de ces différences de moyennes. Donc en plus de savoir quels groupes ou traitements sont significativement différents les uns des autres, ce test nous indique l’importance des différences détectées.\nPour effectuer ce test, on utilise la fonction TukeyHSD(), à laquelle on fournit simplement l’objet contenant les résultats de l’ANOVA :\n\n# Réalisation du test de Tukey HSD\npost_hoc2 &lt;- TukeyHSD(res)\n\n# Affichage des résultats\npost_hoc2\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = shift ~ treatment, data = Light)\n\n$treatment\n                    diff        lwr        upr     p adj\nknee-control -0.02696429 -0.9525222  0.8985936 0.9969851\neyes-control -1.24267857 -2.1682364 -0.3171207 0.0078656\neyes-knee    -1.21571429 -2.1716263 -0.2598022 0.0116776\n\n\nNous obtenons bien à la fois la \\(p-\\)value des comparaisons 2 à 2, ainsi que l’estimation des différences de moyennes (et de leur intervalle de confiance à 95%) entre paires de groupes. Là encore, l’utilisation de la fonction tidy() du package broom peut rendre les résultats plus lisibles (ou en tous cas, plus faciles à manipuler) :\n\ntidy(post_hoc2)\n\n# A tibble: 3 × 7\n  term      contrast     null.value estimate conf.low conf.high adj.p.value\n  &lt;chr&gt;     &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 treatment knee-control          0  -0.0270   -0.953     0.899     0.997  \n2 treatment eyes-control          0  -1.24     -2.17     -0.317     0.00787\n3 treatment eyes-knee             0  -1.22     -2.17     -0.260     0.0117 \n\n\nLa première ligne de ce tableau nous confirme une absence de différence de moyenne significative entre les groupes knee et control (p = 0.997). La différence de moyenne estimée pour ces deux catégories (\\(\\hat{\\mu}_{\\textrm{knee}} - \\hat{\\mu}_{\\textrm{control}}\\)) vaut \\(-0.027\\), avec un intervalle de confiance à 95% pour cette différence qui vaut \\([-0.95 ; 0.90]\\). Cet intervalle, qui rassemble les valeurs les plus probables pour cette différence de moyenne, contient la valeur 0, ce qui confirme qu’il n’y a aucune raison de penser qu’une différence réelle existe entre ces 2 catégories. Le faible écart de moyennes observé entre ces 2 groupes est donc très vraisemblablement le fruit du hasard. L’éclairage du creux poplité donne les mêmes résultats que quand les patients sont maintenus dans le noir.\nEn revanche, les lignes 2 et 3 de ce tableau montrent des différences significatives (\\(p = 0.008\\) et \\(p = 0.012\\) pour les comparaisons eyes/control et eyes/knee respectivement). Les différences sont négatives, de l’ordre de -1.2 pour les 2 comparaisons, ce qui traduit des valeurs plus faibles pour eyes que pour les 2 autres groupes. Pour ces 2 comparaisons, les intervalles de confiance à 95% des différences ne contiennent pas le 0, mais exclusivement des valeurs négatives. Cela traduit donc bien une resynchronisation plus rapide chez les sujets dont les yeux sont exposés à la lumière que chez les sujets des 2 autres groupes.\nEn utilisant le tableau ci-dessus, nous pouvons synthétiser graphiquement ces résultats :\n\ntidy(post_hoc2) %&gt;%\n  ggplot(aes(x = contrast, y = estimate)) +\n  geom_point() +\n  geom_linerange(aes(ymin = conf.low, ymax = conf.high)) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  labs(x = \"Comparaison\",\n       y = \"Différence de moyennes (et IC 95%)\") +\n  coord_flip() +\n  theme_bw()\n\n\n\n\nNotez ici l’utilisation de geom_linerange, pour afficher les intervalles de confiance à 95% des différences de moyennes. Il s’agit d’une alternative à geom_errorbar() dont nous avons déjà parlé dans ce chapitre du livre en ligne du semestre 4. La fonction geom_hline() permet de faire apparaître des lignes horizontales sur un graphique. Ici, avec y = 0, cette fonction fait apparaître un axe horizontal (axe des abscisses). Enfin, la fonction coord_flip() permet d’inverser les axes du graphique : l’axe des x bascule à la verticale, et l’axe des y à l’horizontale. Cela permet d’obtenir un graphique dont l’apparence est typique de ce genre de graphique produit avec les résultats du test de Tukey HSD.\nCe graphique montre bien que pour la comparaison knee - control, le zéro est compris dans l’intervalle de confiance à 95% de la différence de moyennes, ce qui confirme l’absence de différence significative de décalage de phase entre ces 2 groupes. À l’inverse, pour les 2 autres comparaisons (eyes - knee d’une part, et eyes - control d’autre part), les intervalles de confiance à 95% des différences de moyennes ne coupent pas le zéro. Cela indique une différence de moyenne significative : dans la population générale, le zéro ne fait pas partie des valeurs les plus probables pour la différence de décalage de phase entre ces groupes.\nNous avons donc bien montré ici que la re-synchronisation de l’horloge interne n’est possible que par le biais de l’exposition des yeux à la lumière, et non du creux poplité."
  },
  {
    "objectID": "12-ANOVA.html#lalternative-non-paramétrique",
    "href": "12-ANOVA.html#lalternative-non-paramétrique",
    "title": "12  Comparaison de moyennes : plus de 2 groupes",
    "section": "12.7 L’alternative non paramétrique",
    "text": "12.7 L’alternative non paramétrique\n\n12.7.1 La robustesse de l’ANOVA\nDans la suite de cette section, nous faisons l’hypothèse, bien que ça ne soit pas le cas, que les conditions d’application de l’ANOVA ne sont pas vérifiées pour notre jeu de données. Si les conditions d’application de l’ANOVA ne sont pas remplies, alors, les résultats de l’ANOVA ne peuvent pas être examinés car ils ne sont pas valides. Il nous faut alors recourir à un test non-paramétrique afin de comparer la moyenne de plus de deux groupes à la fois.\nLa particularité de l’ANOVA est sa grande robustesse vis-à-vis d’un non respect modéré de ses conditions d’application (voir définition de la robustesse dans la Section 11.6.1). L’ANOVA étant particulièrement robuste, ses résultats resteront valides dans les situations suivantes :\n\nNon normalité modérée des résidus. Si les résidus ne suivent pas parfaitement une loi Normale mais qu’ils sont néanmoins grossièrement distribués selon une courbe en cloche, les résultats de l’ANOVA resteront vrais, surtout si les effectifs sont importants.\nNon homogénéité des résidus. Si les résidus ne sont pas homogènes dans tous les groupes, les résultats de l’ANOVA resteront vrais tant que les échantillons seront grands, approximativement de la même taille dans tous les groupes, et à condition que les écarts de variances entre les groupes ne dépassent pas un facteur 10.\n\nDans tous les autres cas de non respect des conditions de l’ANOVA, par exemple, si la variance des résidus n’est pas homogène et que les groupes sont de petite taille ou de taille différente, ou si les variances diffèrent de plus d’un facteur 10, ou si les résidus s’écartent fortement de la normalité, ou si les deux conditions d’application ne sont pas respectées (même modérément) en même temps, il faudra alors faire un test non paramétrique.\nL’alternative non paramétrique à l’ANOVA à un facteur est le test de la somme des rangs de Kruskal-Wallis\n\n\n\n\n\n\nParamétrique ou non ?\n\n\n\nPour comparer la moyenne de plus de 2 groupes :\n\nlorsque les conditions permettant de réaliser un test paramétrique sont réunies (voir Section 12.6.2), on effectuera une ANOVA, qui n’est qu’une extension du test de Student. Si (et seulement si) on en rejette l’hypothèse nulle, on fera ensuite un test post-hoc paramétrique : le test de comparaisons multiples de Student et/ou le test de Tukey HSD.\nlorsque les conditions permettant de réaliser un test paramétrique ne sont pas réunies, on effectuera un test de Kruskal-Wallis, qui est une extension du test de Wilcoxon. Si (et seulement si) on en rejette l’hypothèse nulle, on fera ensuite un test post-hoc non paramétrique : le test de comparaisons multiples de Wilcoxon et/ou le test de Dunn.\n\n\n\n\n\n12.7.2 Réalisation du tests et interprétation\nLes hypothèses nulle et alternative du test de Kruskal-Wallis sont les suivante. Comme toujours, l’hypothèse nulle concerne l’absence d’effet du facteur étudié :\n\nH\\(_0\\) : le type de traitement appliqué n’a pas d’effet sur le décalage de phase. Les médianes sont égales dans tous les groupes (\\(\\textrm{med}_\\textrm{control} = \\textrm{med}_\\textrm{knee} = \\textrm{med}_\\textrm{eyes}\\)).\nH\\(_1\\) : le type de traitement appliqué a un effet sur le décalage de phase. Les médianes ne sont pas toutes égales, au moins l’une d’entre elles diffère des autres.\n\nLa syntaxe du test est similaire à celle de l’ANOVA. On utilise la notation formule en plaçant la variable numérique expliquée à gauche du ~, et le facteur (variable explicative) à droite du ~ :\n\nkruskal.test(shift ~ treatment, data = Light)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  shift by treatment\nKruskal-Wallis chi-squared = 9.4231, df = 2, p-value = 0.008991\n\n\nIci, la \\(p-\\)value est inférieure à \\(\\alpha\\), on rejette donc H\\(_0\\) : toutes les médianes ne sont pas égales. Comme avec l’ANOVA, il nous faut maintenant déterminer quelles médianes sont différentes les unes des autres, et quelles sont les magnitudes de ces différences. Pour cela, nous devons réaliser des tests post-hoc de comparaisons multiples.\n\n\n12.7.3 Tests a posteriori ou tests post-hoc\nComme pour les tests post-hoc de l’ANOVA, nous allons voir ici 2 tests de comparaisons multiples non paramétriques.\n\n12.7.3.1 Comparaisons multiples de Wilcoxon\nLe premier test est l’équivalent non paramétrique du test de comparaisons multiples de Student : le test de comparaisons multiples de la somme des rangs de Wilcoxon. Le principe est absolument le même que pour le test de comparaisons multiples de Student : toutes les comparaisons 2 à 2 sont effectuées au moyen d’un test de la somme des rangs de Wilcoxon. Les \\(p-\\)values de ces tests sont corrigées afin de garantir que le risque d’erreur \\(\\alpha\\) global soit maintenu constant en dépit de l’augmentation du nombre de tests réalisés. Pour chaque comparaison, les hypothèses sont les suivantes :\n\nH\\(_0\\) : la médiane des deux populations est égale.\nH\\(_1\\) : la médiane des deux populations est différente.\n\n\n# Réalisation du test\npost_hoc3 &lt;- pairwise.wilcox.test(Light$shift, Light$treatment)\n\n# Affichage des résultats\npost_hoc3\n\n\n    Pairwise comparisons using Wilcoxon rank sum exact test \n\ndata:  Light$shift and Light$treatment \n\n     control knee  \nknee 0.9551  -     \neyes 0.0037  0.0524\n\nP value adjustment method: holm \n\n# Utilisation de la fonction `tidy` pour afficher les résultats dans un tibble\ntidy(post_hoc3)\n\n# A tibble: 3 × 3\n  group1 group2  p.value\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n1 knee   control 0.955  \n2 eyes   control 0.00373\n3 eyes   knee    0.0524 \n\n\nIci, la \\(p-\\)value du premier test est supérieure à \\(\\alpha = 0.05\\). Il n’y a donc pas de différence entre les médianes du groupe control et du groupe knee. Le traitement lumineux appliqué dans le creux poplité n’a donc aucun effet sur le décalage de phase.\nLa \\(p-\\)value du second test est en revanche inférieure à \\(\\alpha\\). On rejette l’hypothèse nulle pour ce test ce qui confirme que le traitement lumineux appliqué au niveau des yeux a un effet sur le décalage de phase. Reste toutefois à quantifier l’importance de ce décalage de phase par rapport au groupe control.\nEnfin, la \\(p-\\)value du troisième test est supérieure (tout juste !) à \\(\\alpha\\). La conclusion logique est donc qu’il n’y a pas de différence significative entre les médianes des groupes knee et eyes. On sait que ce n’est pas le cas puisque nous avons montré plus haut (avec les tests paramétriques), que la différence de moyennes entre ces deux populations était significative. Nous avons ici l’illustration parfaite de la faible puissance des tests non paramétriques : leur capacité à détecter un effet lorsqu’il y en a réellement un est plus faible que celle des tests paramétriques. En outre, les procédures de comparaisons multiples sont très conservatives, et font mécaniquement baisser la puissance des tests pour maintenir constante l’erreur \\(\\alpha\\). Je ne peux donc que vous inciter à la prudence lorsque vous interprétez les résultats d’un test de comparaions multiples (a fortiori un test non paramétrique) pour lequel la \\(p-\\)value obtenue est très proche du seuil \\(\\alpha\\).\nComme pour son homologue paramétrique, le test de comparaisons multiples de Wilcoxon nous permet de prendre une décision par rapport à H\\(_0\\), mais il ne nous dit rien de la magnitude des effets mesurés. Pour les connaître, il nous faut réaliser un autre test.\n\n\n12.7.3.2 Le test de Dunn\nLe test de Dunn est au test de Kruskal-Wallis ce que le test de Tukey HSD est à l’ANOVA : un test post-hoc permettant de déterminer la magnitude des effets observés. Pour pouvoir le réaliser, le package DescTools doit être chargé. Sa syntaxe est la même que pour le test de Kruskal-Wallis ou l’ANOVA :\n\n# Réalisation du test\npost_hoc4 &lt;- DunnTest(shift ~ treatment, data = Light)\n\n# Affichage des résultats\npost_hoc4\n\n\n Dunn's test of multiple comparisons using rank sums : holm  \n\n             mean.rank.diff   pval    \nknee-control     -0.4821429 0.8859    \neyes-control     -9.3392857 0.0164 *  \neyes-knee        -8.8571429 0.0214 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAvec ces résultats on progresse un peu, car outre les \\(p-\\)values pour chaque comparaison, le test nous fournit une estimation de la différence des rangs moyens. Malheureusement, ces estimations sont souvent difficiles à interpréter (par exemple, quelle est l’unité utilisée ?) et aucun intervalle de confiance n’est fourni. On constate néanmoins que le test de Dunn donne ici des résultats comparables à ceux fournis par les tests paramétriques : le groupe eyes est significativement différent des deux autres. Pour obtenir les intervalles de confiance dont nous avons besoin, nous n’avons pas d’autre choix que des les calculer à l’aide du test de Wilcoxon classique, en réalisant manuellement les tests dont nous avons besoin. Ici, le test à proprement parler ne nous intéresse pas, d’ailleurs, sa \\(p-\\)value ne doit surtout pas être prise en compte car elle ignore totalement les comparaisons multiples et conduirait donc à augmenter l’erreur de type I. La seule chose pertinente est ici la différence de (pseudo-)médiane estimée et son intervalle de confiance :\n\n# Comparaisons entre les groupes `knee` et `eyes` (dans cet ordre)\nLight %&gt;%\n  filter(treatment %in% c(\"knee\", \"eyes\")) %&gt;%\n  wilcox.test(shift ~ treatment, data = ., conf.int = TRUE) %&gt;%\n  tidy()\n\n# A tibble: 1 × 7\n  estimate statistic p.value conf.low conf.high method               alternative\n     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt;      \n1     1.19        42  0.0262      0.3      2.21 Wilcoxon rank sum e… two.sided  \n\n\nPour le décalage de phase de ces 2 groupes, la différence de médiane estimée vaut donc 1.19, avec un intervalle de confiance à 95% de \\([0.3 ; 2.21]\\). Toutes les valeurs comprises dans cet intervalle de confiance sont strictement positives. Il y a donc très peu de chances pour que la différence de médiane entre ces deux groupes soit nulle. Le test de Dunn ci-dessus, qui montre une différence significative entre ces groupes, est donc confirmé."
  },
  {
    "objectID": "12-ANOVA.html#exercices-dapplication",
    "href": "12-ANOVA.html#exercices-dapplication",
    "title": "12  Comparaison de moyennes : plus de 2 groupes",
    "section": "12.8 Exercices d’application",
    "text": "12.8 Exercices d’application\n\n12.8.1 Cardamine pensylvanica\nEn biologie de la conservation, la question de l’existence d’un lien entre la capacité de dispersion des organismes et le maintien durable des populations dans le temps est étudié de près, notamment en raison de l’anthropisation des milieux qui conduit très souvent à la fragmentation des habitats. Cette question a été étudiée par 2 chercheurs (Molofsky et Ferdy 2005) chez Cardamine pensylvanica, une plante annuelle d’Amérique du Nord qui produit des graines qui sont dispersées de façon explosive. Quatre traitements ont été utilisés pour modifier expérimentalement la dispersion des graines. La distance entre populations contigües a été définie comme suit :\n\nTraitement 1 : continu. Les plants sont conservés au contact les uns des autres.\nTraitement 2 : medium. Les plants sont séparés de 23.2 centimètres.\nTratiement 3 : long. Les plants sont séparés de 49.5 centimètres.\nTraitement 4 : isole. Les plants sont séparés par des panneaux de bois empêchant la dispersion des graines.\n\nCes traitements ont été assignés au hasard à des populations de plantes, et 4 réplicats ont été faits pour chacun d’entre eux. Les résultats de l’expérience sont présentés ci-dessous. Il s’agit du nombre de générations durant lesquelles les plantes ont persisté :\n\ncontinu : 9, 13, 13, 16\nmedium : 14, 12, 16, 16\nlong : 13, 9, 10, 11\nisole : 13, 8, 8, 8\n\nSaisissez ces données dans RStudio et faites-en l’analyse. Vous tenterez de déterminer si l’éloignement entre les populations de plantes a un impact sur leur capacité de survie. Comme toujours, avant de vous lancer dans les tests, vous prendrez le temps de décrire les données avec des statistiques descriptives et des représentations graphiques.\n\n\n12.8.2 Insecticides\nL’efficacité de 6 insecticides nommés A, B, C, D, E et F a été testée sur 6 parcelles agricoles. Chaque insecticide de cette liste a été appliqué sur une parcelle agricole choisie au hasard. Deux semaines plus tard, 12 plants ont été collectés dans chaque parcelle agricole et le nombre d’insectes toujours vivants sur chacun d’entre eux a été compté. Les résultats sont présentés dans le fichier Insectes.csv. Importez ces données dans RStudio et faites-en l’analyse. Tous les insecticides ont-ils la même efficacité ? Si la réponse est non, quels sont les insecticides les plus (ou les moins) efficaces.\n\n\n12.8.3 La longueur des nageoires des manchots femelles\nAvec le jeu de données penguins du package palmerpenguins, comparez la longueur des nageoires des femelles des 3 espèces de manchots. Les femelles des 3 espèces ont-elles toutes des nageoires de longueur différentes, et quelle est la magnitude de ces éventuelles différences ?\n\n\n\n\nCampbell, Scott S., et Patricia J. Murphy. 1998. « Extraocular Circadian Phototransduction in Humans ». Science 279 (5349): 396‑99. https://doi.org/10.1126/science.279.5349.396.\n\n\nFox, John, Sanford Weisberg, et Brad Price. 2023. car: Companion to Applied Regression. https://CRAN.R-project.org/package=car.\n\n\nHorst, Allison, Alison Hill, et Kristen Gorman. 2022. palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://CRAN.R-project.org/package=palmerpenguins.\n\n\nMolofsky, Jane, et Jean-Baptiste Ferdy. 2005. « Extinction dynamics in experimental metapopulations ». Proceedings of the National Academy of Sciences 102 (10): 3726‑31. https://doi.org/10.1073/pnas.0404576102.\n\n\nRobinson, David, Alex Hayes, et Simon Couch. 2023. broom: Convert Statistical Objects into Tidy Tibbles. https://CRAN.R-project.org/package=broom.\n\n\nSignorell, Andri. 2023. DescTools: Tools for Descriptive Statistics. https://CRAN.R-project.org/package=DescTools.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, et Shannon Ellis. 2022. skimr: Compact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr.\n\n\nWickham, Hadley. 2023. tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, et Jennifer Bryan. 2023. readxl: Read Excel Files. https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, et Teun van den Brand. 2024. ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, et Davis Vaughan. 2023. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, et Jennifer Bryan. 2023. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWright, Kenneth P., et Charles A. Czeisler. 2002. « Absence of Circadian Phase Resetting in Response to Bright Light Behind the Knees ». Science 297 (5581): 571‑71. https://doi.org/10.1126/science.1071697."
  },
  {
    "objectID": "13-Correlation.html#sec-packages5",
    "href": "13-Correlation.html#sec-packages5",
    "title": "13  Corrélation",
    "section": "13.1 Pré-requis",
    "text": "13.1 Pré-requis\nComme pour chaque nouveau chapitre, je vous conseille de travailler dans un nouveau script que vous placerez dans votre répertoire de travail, et dans une nouvelle session de travail (Menu Session &gt; Restart R). Inutile en revanche de créer un nouveau Rproject : vos pouvez tout à fait avoir plusieurs script dans le même répertoire de travail et pour un même Rproject. Comme toujours, consultez le livre en ligne du semestre 3 si vous ne savez plus comment faire.\nSi vous êtes dans une nouvelle session de travail (ou que vous avez quitté puis relancé RStudio), vous devrez penser à recharger en mémoire les packages utiles. Dans ce chapitre, vous aurez besoin d’utiliser :\n\nle tidyverse (Wickham 2023), qui comprend notamment le package readr (Wickham, Hester, et Bryan 2023), pour importer facilement des fichiers .csv au format tibble, le package dplyr (Wickham et al. 2023), pour manipuler des tableaux, et le package ggplot2 (Wickham et al. 2024) pour les représentations graphiques.\nskimr (Waring et al. 2022), qui permet de calculer des résumés de données très informatifs.\n\n\nlibrary(tidyverse)\nlibrary(skimr)\n\nVous aurez également besoin des jeux de données suivants, qu’il vous faut donc télécharger dans votre répertoire de travail :\n\nbirds.csv\nloups.csv\nropetrick.csv\n\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "13-Correlation.html#principe",
    "href": "13-Correlation.html#principe",
    "title": "13  Corrélation",
    "section": "13.2 Principe",
    "text": "13.2 Principe\nLorsque des variables numériques sont associées ont dit qu’elles sont corrélées. Par exemple, la taille du cerveau et la taille du corps sont corrélées positivement parmi les espèces de mammifères. Les espèces de grande taille ont tendance à avoir un cerveau plus grand et les petites espèces ont tendance à avoir un cerveau plus petit. Le coefficient de corrélation est la quantité qui décrit la force et la direction de l’association entre deux variables numériques mesurées sur un échantillon de sujets ou d’unités d’observation. La corrélation reflète la quantité de dispersion dans un nuage de points entre deux variables. Contrairement à la régression linéaire, la corrélation n’ajuste aucune droite à des données et ne permet donc pas de mesurer à quel point le changement d’une variable entraîne un changement rapide ou lent de l’autre variable.\nAinsi, sur la figure ci-dessous, le coefficient de corrélation entre X et Y est le même pour les deux graphiques : il vaut 1.\n\n\n\n\n\n\n\n\n\n\nIci, le coefficient de corrélation (noté \\(r\\)) vaut 1 dans les deux cas, car tous les points sont alignés sur une droite. La pente de la droite n’influence en rien la valeur de corrélation. En revanche, le degré de dispersion des points autour d’une droite parfaite a une influence :\n\n\n\n\n\n\n\n\n\n\nPlus la dispersion autour d’une droite parfaite sera grande, plus la corrélation sera faible. C’est la raison pour laquelle lorsque l’on parle de “corrélation”, on sous-entend généralement corrélation linéaire. Ainsi, 2 variables peuvent avoir une relation très forte, mais un coefficient de corrélation nul, si leur relation n’est pas linéaire :\n\n\n\n\n\nL’exploration graphique de vos données devrait donc toujours être une priorité. Calculer un coefficient de corrélation nul ou très faible ne signifie par pour autant une absence de relation entre les 2 variables numériques étudiées. Cela peut signifier une relation non linéaire. La solution la plus simple pour distinguer une relation telle que celle du graphique précédent, et une absence de relation telle que celle présentée dans le graphique ci-dessous, est l’examen visuel des données :\n\n\n\n\n\nEn bref, le coefficient de corrélation \\(r\\) est compris entre -1 et +1 :\n\nUne forte valeur absolue (\\(r\\) proche de -1 ou +1), indique une relation presque linéaire.\nUne faible valeur absolue indique soit une absence de relation, soit une relation non linéaire (la visualisation graphique permet généralement d’en savoir plus).\nUne valeur positive indique qu’une augmentation de la première variable est associée à une augmentation de la seconde variable.\nUne valeur négative indique qu’une augmentation de la première variable est associée à une diminution de la seconde variable.\n\n\n\n\n\n\n\nImportant\n\n\n\nLe coefficient de corrélation suppose une relation linéaire entre les deux variables numériques examinées. Calculer un coefficient de corrélation très faible peut indiquer :\n\nune absence de relation entre les variables étudiées\nune relation forte, mais non linéaire entre les variables étudiées\n\nLa façon la plus simple de distinguer ces 2 cas de figure très différents est l’exploration graphique des données.\n\n\nDans la suite de ce chapitre, nous allons voir comment calculer le coefficient de corrélation entre 2 variables numériques1, et puisque nous travaillons avec des échantillons, ce calcul sera nécessairement entaché d’incertitude. Tout comme la moyenne ou la variance d’un échantillon, la corrélation est un paramètre des populations dont nous ne pourrons qu’estimer la valeur. Toute estimation de corrélation devra donc être encadrée par un intervalle d’incertitude, généralement, il s’agit de l’intervalle de confiance à 95% de la corrélation. Enfin, outre l’estimation de la valeur de la corrélation et de son incertitude, nous pourrons aussi faire des tests d’hypothèses au sujet des corrélations que nous estimerons. En particulier, nous pourrons tester si la corrélation observée est significativement différente de zéro ou non.1 Vous aurez compris je pense qu’un calcul de corrélation n’a de sens que si l’on dispose de 2 variables numériques, enregistrées sur les mêmes individus ou unités d’étude. Voir détails à la fin de la Section 13.4"
  },
  {
    "objectID": "13-Correlation.html#contexte",
    "href": "13-Correlation.html#contexte",
    "title": "13  Corrélation",
    "section": "13.3 Contexte",
    "text": "13.3 Contexte\nLes adultes qui infligent des mauvais traitements à leurs enfants ont souvent été maltraités dans leur enfance. Une telle relation existe-t-elle également chez d’autres espèces animales, chez qui cette relation pourrait être étudiée plus facilement ? Müller et al. (2011) ont étudié cette possibilité chez le fou de Grant (Sula granti), un oiseau marin colonial vivant entre autres aux Galápagos. Les jeunes laissés au nid sans attention parentale reçoivent fréquemment la visite d’autres oiseaux, qui se comportent souvent de manière agressive à leur encontre. Les chercheurs ont compté le nombre de ces visites dans le nid de 24 poussins dotés d’une bague d’identification individuelle. Ces 24 individus ont ensuite été suivis à l’âge adulte, lorsqu’ils sont à leur tour devenus parents. On cherche donc à savoir s’il existe un lien entre le nombre de visites agressives qu’un individu à reçu lorsqu’il était à l’état de poussin, et un degré d’agressivité mesuré à l’âge adulte."
  },
  {
    "objectID": "13-Correlation.html#sec-import5",
    "href": "13-Correlation.html#sec-import5",
    "title": "13  Corrélation",
    "section": "13.4 Importation et mise en forme des données",
    "text": "13.4 Importation et mise en forme des données\nLes données récoltées par les chercheurs figurent dans le fichier birds.csv. Importez ces données dans RStudio dans un objet noté birds.\n\nbirds\n\n# A tibble: 24 × 2\n   nVisitsNestling futureBehavior\n             &lt;dbl&gt;          &lt;dbl&gt;\n 1               1          -0.8 \n 2               7          -0.92\n 3              15          -0.8 \n 4               4          -0.46\n 5              11          -0.47\n 6              14          -0.46\n 7              23          -0.23\n 8              14          -0.16\n 9               9          -0.23\n10               5          -0.23\n# ℹ 14 more rows\n\n\nLa première colonne de ce tableau indique, pour chaque individu suivi, le nombre de visites reçues au nid de la part d’adultes agressifs lorsqu’ils étaient poussins. La seconde colonne indique, pour ces mêmes individus devenus adultes, le nombre de visites agressives effectuées à des nids d’autres poussins. Ce nombre n’est pas dans la même unité que la première variable car il a été corrigé par d’autres variables d’intérêt pour les chercheurs.\nIl manque à ce tableau une variable indiquant le code des individus. Elle n’est pas indispensable, mais la rajouter est une bonne habitude à prendre pour toujours travailler avec des “données rangées”. Puisqu’on dispose de 24 individus, on leur assigne donc un code de 1 à 24 :\n\nbirds &lt;- birds %&gt;%\n  mutate(ID = factor(1:24))\nbirds\n\n# A tibble: 24 × 3\n   nVisitsNestling futureBehavior ID   \n             &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;\n 1               1          -0.8  1    \n 2               7          -0.92 2    \n 3              15          -0.8  3    \n 4               4          -0.46 4    \n 5              11          -0.47 5    \n 6              14          -0.46 6    \n 7              23          -0.23 7    \n 8              14          -0.16 8    \n 9               9          -0.23 9    \n10               5          -0.23 10   \n# ℹ 14 more rows\n\n\nPrésentées sous cette forme, les données ressemblent beaucoup à celles du Chapitre 10. Ça n’est pas un hasard : les données dont nous disposons ici sont appariées. Calculer la corrélation entre 2 variables n’a de sens que si chaque unité d’échantillonnage ou d’observation (ici, les individus), fournissent 2 valeurs dont on souhaite mesurer l’association. Dans l’étude sur les effets de la testostérone chez les carouges à épaulettes, on avait, pour chaque individu étudié, 2 mesures d’immunocompétence : une avant et l’autre après l’opération chirurgicale. Ici, chaque Fou de Grant étudié fournit 2 valeurs également. Contrairement à l’étude des carouges à épaulettes, il s’agit de deux variables distinctes (nombre de visites agressives reçues à l’état de poussin d’une part, et comportement agressif à l’âge adulte d’autre part), mais les 2 mesures sont bien liées puisqu’elles sont obtenues chez le même individu.\nPour bien enfoncer le clou, voici un autre exemple. Calculer la corrélation entre la taille des femmes françaises et la tension artérielle des femmes anglaises n’a strictement aucun sens car ce sont des groupes de femmes distincts qui fournissent les mesures de chaque variable. En revanche, sélectionner un groupe de femmes au hasard dans la population mondiale, et examiner, pour chacune des femmes de l’échantillon, à la fois la taille et la tension artérielle est pertinent. On peut alors se poser la question de lien potentiel existant entre ces 2 variables dans la population générale. L’étude de la corrélation entre la taille et la tension artérielle chez les femmes prend alors tout sons sens.\n\n\n\n\n\n\nImportant\n\n\n\nCalculer une corrélation n’a de sens que si les données étudiées sont appariées."
  },
  {
    "objectID": "13-Correlation.html#exploration-statistique-des-données",
    "href": "13-Correlation.html#exploration-statistique-des-données",
    "title": "13  Corrélation",
    "section": "13.5 Exploration statistique des données",
    "text": "13.5 Exploration statistique des données\nComme toujours, la première chose à faire est d’examiner quelques statistiques descriptives pour se faire une idée de la forme des données et pour repérer les éventuelles données manquantes ou aberrantes.\n\nskim(birds)\n\n── Data Summary ────────────────────────\n                           Values\nName                       birds \nNumber of rows             24    \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts            \n1 ID                    0             1 FALSE         24 1: 1, 2: 1, 3: 1, 4: 1\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable   n_missing complete_rate   mean    sd    p0    p25  p50    p75\n1 nVisitsNestling         0             1 13.1   7.21   1     8.75  13   15.8  \n2 futureBehavior          0             1 -0.119 0.374 -0.92 -0.288 -0.1  0.182\n   p100 hist \n1 31    ▅▇▅▃▁\n2  0.39 ▂▂▃▂▇\n\n\nOutre le facteur ID que nous venons de créer, nous disposons donc de 2 variables numériques qui ne contiennent pas de données manquantes.\n\nLa variable nVisitsNestling, qui indique le nombre de visites agressives reçues par les individus suivis lorsqu’ils étaient de jeunes poussins, varie de 1 à 31, pour une moyenne de 13.12, une médiane proche (13) mais un écart-type important.\nLa variable futureBehavior varie de -0.92 à 0.39, avec une moyenne et une médiane proche de 0 (-0.12 et -0.1 respectivement).\n\nComme toujours, la fonction skim() nous renseigne sur la tendance centrale, ou position, des variables étudiées (grâce aux moyennes et médianes) et sur la dispersion des données (grâce à l’écart-type et aux “minis-histogrammes”). Ici, si la variable nVisitsNestling semble être à peu près distribuée selon une courbe en cloche (asymétrique), ce n’est pas le cas de la variable futureBehavior qui semble présenter une très forte asymétrie à gauche.\nD’habitude, on calcule à ce stade des indices d’incertitude : l’erreur standard de la moyenne ou l’intervalle de confiance de la moyenne. Ici, ça n’est pas utile car les moyennes en elles-mêmes ne nous intéressent pas, et donc leurs incertitudes non plus. C’est en revanche la relation entre les 2 variables numériques qui nous intéresse, en particulier l’intensité et le sens de cette relation. On calcule donc maintenant le coefficient de corrélation linéaire entre les 2 variables :\n\nbirds %&gt;%\n  select(nVisitsNestling, futureBehavior) %&gt;%\n  cor()\n\n                nVisitsNestling futureBehavior\nnVisitsNestling       1.0000000      0.5337225\nfutureBehavior        0.5337225      1.0000000\n\n\nLe résultat est fourni sous la forme d’une matrice symétrique :\n\nSur la diagonale, les corrélations valent 1 (le coefficient de corrélation d’une variable avec elle-même vaut toujours 1).\nEn dehors de la diagonale, on trouve le coefficient de corrélation linéaire entre les 2 variables d’intérêt. Ici, il est positif et vaut 0.534, ce qui est une valeur relativement élevée dans le domaine de la biologie ou de l’écologie. Le signe positif de la corrélation indique que lorsque la première variable augmente, la seconde variable augmente également. Autrement dit, plus les fous de Grant ont été maltraités quand ils étaient poussins, plus ils adoptent un comportement agressif à l’âge adulte."
  },
  {
    "objectID": "13-Correlation.html#exploration-graphique-des-données",
    "href": "13-Correlation.html#exploration-graphique-des-données",
    "title": "13  Corrélation",
    "section": "13.6 Exploration graphique des données",
    "text": "13.6 Exploration graphique des données\nPour répondre à la question posée et visualiser la relation entre les deux variables numériques, on peut simplement associer chaque variable à un axe d’un graphique et faire un nuage de points. Je vous encourage à jeter un œil à ce chapitre du livre en ligne du semestre 3 pour voir quels types de graphiques sont pertinents dans cette situation.\nAfin de savoir si la valeur de \\(r\\) calculée précédemment dans notre échantillon (0.534) reflète une relation linéaire mais moyenne, ou une relation qui n’est pas vraiment linéaire, nous pouvons donc faire un nuage de points :\n\nbirds %&gt;%\n  ggplot(aes(x = nVisitsNestling, y = futureBehavior)) +\n  geom_point() +\n  labs(x = \"Nombre de visites reçues par le poussin\",\n       y = \"Agressivité à l'âge adulte\")\n\n\n\n\nFigure 13.1: Relation entre agressivité à l’âge adulte et nombre de visites agressives reçues par les poussins de l’espèce Sula granti\n\n\n\n\nOn constate ici que la corrélation moyenne obtenue plus haut est due au fait que les points sont assez dispersés, et non au fait que la relation n’est pas linéaire. On peut donc dire que la relation, si elle existe, n’est pas parfaite. Le comportement des individus devenus adultes semble donc en partie lié au nombre de visites agressives qu’ils ont reçues étant jeunes, mais ce n’est certainement pas le seul facteur influençant leur comportement. Un test d’hypothèses devrait nous permettre de déterminer si la corrélation linéaire observée ici est simplement le fruit du hasard de l’échantillonnage, ou si au contraire la relation observée n’est pas seulement le fruit du hasard, mais bien le reflet d’un lien réel entre les 2 variables.\nSi visualiser la distribution des données n’est pas indispensable pour se faire une idée de la nature du lien qui existe (ou non) entre les deux variables, cela sera néanmoins utile pour vérifier les conditions d’application du test de corrélations paramétrique de Pearson. Comme dans les chapitres précédents, nous avons donc intérêt à examiner la distribution de ces 2 variables par le biais d’histogrammes, de graphiques de densité ou de boîtes à moustaches.\n\nbirds %&gt;% \n  ggplot(aes(x = nVisitsNestling)) +\n  geom_density(fill = \"firebrick2\", alpha = 0.5) +\n  geom_rug() +\n  labs(x = \"Visites reçues par le poussin\",\n       y = \"Densité\")\nbirds %&gt;% \n  ggplot(aes(x = futureBehavior)) +\n  geom_density(fill = \"firebrick2\", alpha = 0.5) +\n  geom_rug() +\n  labs(x = \"Agressivité à l'âge adulte\",\n       y = \"Densité\")\n\n\n\n\n\n\n\n\n\n\n\nAucune des 2 variables ne semble suivre parfaitement une distribution Normale. Il faudra réaliser des tests de normalité pour en avoir le cœur net."
  },
  {
    "objectID": "13-Correlation.html#le-test-paramétrique",
    "href": "13-Correlation.html#le-test-paramétrique",
    "title": "13  Corrélation",
    "section": "13.7 Le test paramétrique",
    "text": "13.7 Le test paramétrique\n\n13.7.1 Les hypothèses\nComme pour la plupart des grandeurs calculées à partir d’un échantillon, la corrélation \\(r\\) n’est qu’un estimateur de la corrélation qui existe réellement entre ces deux variables dans la population générale. Dans la population générale, la corrélation linéaire est généralement notée \\(\\rho\\). Son estimateur, \\(r\\) est donc souvent noté \\(\\hat{\\rho}\\).\nLe test d’hypothèses que nous allons faire maintenant permet de vérifier si le coefficient de corrélation \\(\\rho\\) dans la population générale est différent de 0 ou non. Les hypothèses de ce test sont les suivantes :\n\nH\\(_0\\) : le coefficient de corrélation entre les deux variables étudiées vaut 0 dans la population générale (\\(\\rho = 0\\)). Autrement dit, la corrélation observée dans l’échantillon n’est que le fruit du hasard de l’échantillonnage : il n’y a aucun lien entre les 2 variables dans la population générale.\nH\\(_1\\) : le coefficient de corrélation entre les deux variables étudiées est différent de 0 dans la population générale (\\(\\rho \\neq 0\\)). La fluctuation d’échantillonnage ne suffit pas à expliquer la corrélation observée : en plus du hasard de l’échantillonnage, il existe bel et bien un lien entre les 2 variables étudiées.\n\nCe test est réalisé dans RStudio grâce à la fonction cor.test(), qui permet, selon les arguments renseignés, de réaliser soit :\n\nle test de corrélation paramétrique de Pearson.\nle test de corrélation non paramétrique de Spearman.\n\n\n\n13.7.2 Conditions d’application\nComme toujours, on cherche à réaliser un test paramétrique (ici, le test de Pearson) si les données le permettent. Pour avoir le droit de réaliser le test de corrélation de Pearson, il nous faut donc en vérifier les conditions d’application :\n\nLes individus doivent être indépendants les uns des autres\nLes mesures effectuées doivent suivre une distribution Normale bivariée\n\nComme toujours, sauf si on a de bonnes raisons de penser le contraire, on considère généralement que si l’échantillonnage a été fait de façon aléatoire, l’indépendance des observations est garantie. La condition de “distribution Normale bivariée” des données est en revanche nouvelle. Elle suppose essentiellement que les 3 critères suivants soient vérifiés :\n\nLa relation entre les 2 variables doit être linéaire. C’est que nous tentons de vérifier visuellement en réalisant un nuage de points des données.\nSur un graphique représentant une variable en fonction de l’autre, le nuage de points doit avoir une forme circulaire ou elliptique. Là encore, une représentation graphique nous permet d’apprécier cette condition.\nLes 2 variables étudiées doivent suivre une distribution Normale dans la population générale. Avant de faire ce test, il nous faut donc vérifier la Normalité des données pour chacune des 2 variables séparément, à l’aide, par exemple, d’un test de Shapiro-Wilk.\n\nPour résumer, l’examen du nuage de points permet de vérifier les 2 premières conditions et 2 tests de Shapiro permettent de vérifier la troisième. Pour l’examen du nuage de points, les conditions ne seront pas remplies dans les situations suivantes (voir les exemples du graphique ci-dessous) :\n\nLe nuage de points a une forme d’entonnoir ou de nœud papillon.\nDes ouliers sont présents (quelques points fortement éloignés du reste des observations).\nUne relation non linéaire existe entre les deux variables.\n\n\n\n\n\n\nEnfin, si l’une, l’autre ou les deux séries de données ne suivent pas la loi Normale, il faudra faire un test non paramétrique.\nDans notre cas, le graphique Figure 13.1 semble indiquer que les 2 premières conditions d’application sont remplies (la relation entre les deux variable semble globalement linéaire et le nuage de points a globalement une forme elliptique). Il nous reste donc à vérifier la normalité des 2 variables. Les hypothèses nulles et alternatives du test de Shapiro-Wilk sont toujours les mêmes :\n\n\\(H_0\\) : les données suivent une distribution Normale dans la population générale.\n\\(H_1\\) : les données ne suivent pas une distribution Normale dans la population générale.\n\n\nbirds %&gt;% \n  pull(nVisitsNestling) %&gt;% \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.95783, p-value = 0.3965\n\n\nContrairement à ce que pouvait laisser croire le graphique de densité, la variable nVisitsNestling suit bien une distribution Normale (test de Shapiro-Wilk, \\(p = 0.397\\)).\n\nbirds %&gt;% \n  pull(futureBehavior) %&gt;% \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.91575, p-value = 0.04709\n\n\nEn revanche, au seuil \\(\\alpha = 0.05\\), la variable futureBehavor ne suit pas une distribution Normale (test de Shapiro-Wilk, \\(p = 0.047\\)).\nLes conditions d’application ne sont pas vérifiées. En toute rigueur, il nous faudrait donc réaliser ici le test non-paramétrique de Spearman. Nous verrons comment le faire plus tard. Pour l’instant, et pour que vous sachiez comment faire, nous allons faire comme si les conditions d’application du tests paramétrique étaient bel et bien remplies, et nous allons donc réaliser le test paramétrique de Pearson.\n\n\n13.7.3 Réalisation du test et interprétation\nLa syntaxe du test est très simple :\n\ncor.test(birds$nVisitsNestling, birds$futureBehavior)\n\n\n    Pearson's product-moment correlation\n\ndata:  birds$nVisitsNestling and birds$futureBehavior\nt = 2.9603, df = 22, p-value = 0.007229\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1660840 0.7710999\nsample estimates:\n      cor \n0.5337225 \n\n\nComme expliqué plus haut (et sur la première ligne des résultats du test), il s’agit du test paramétrique de corrélation de Pearson. Comme pour tous les tests examinés jusqu’ici, les premières lignes des résultats fournissent toutes les informations utiles au sujet du test. Ici, on peut dire :\n\nAu seuil \\(\\alpha = 0.05\\), le test de corrélation de Pearson a permis de rejeter l’hypothèse nulle selon laquelle le nombre de visites agressives au nid des poussins et leur futur comportement agressif sont indépendants (\\(t = 2.96\\), \\(ddl = 22\\), \\(p = 0.007\\)).\n\nCe test prouve donc que \\(\\rho\\) est significativement différent de 0. La valeur de 0.53 observée ici n’est pas due au seul hasard de l’échantillonnage.\nComme toujours, les résultats du test que nous avons réalisé ne nous disent rien de la valeur de la corrélation estimée, ni de son incertitude. Il nous faut pour cela examiner les autres lignes fournies par RStudio lorsque nous faisons ce test et qui relèvent de l’estimation (voir section suivante).\nDernière chose concernant ce test, nous avons fait ici un test bilatéral comme nous le rappelle cette ligne des résultats :\nalternative hypothesis: true correlation is not equal to 0\nComme pour les tests de comparaisons de moyennes, il est possible de réaliser un test unilatéral, à condition que cela ait un sens, à condition que nous soyons en mesure d’expliquer le choix de notre hypothèse alternative. La syntaxe est la même que pour les tests de Student ou de Wilcoxon : on utilise l’argument alternative = \"less\" ou alternative = \"greater\" au moment de faire le test, selon l’hypothèse que l’on souhaite tester.\nIci, si les hypothèses que nous souhaitons tester sont les suivantes :\n\nH\\(_0\\) : le coefficient de corrélation entre les deux variables étudiées vaut 0 dans la population générale (\\(\\rho = 0\\))\nH\\(_1\\) : le coefficient de corrélation entre les deux variables étudiées est positif dans la population générale (\\(\\rho &gt; 0\\))\n\nOn utilise la syntaxe suivante :\n\ncor.test(birds$nVisitsNestling, birds$futureBehavior,\n         alternative = \"greater\")\n\n\n    Pearson's product-moment correlation\n\ndata:  birds$nVisitsNestling and birds$futureBehavior\nt = 2.9603, df = 22, p-value = 0.003615\nalternative hypothesis: true correlation is greater than 0\n95 percent confidence interval:\n 0.2320921 1.0000000\nsample estimates:\n      cor \n0.5337225 \n\n\nComme pour les autres test unilatéraux, le choix d’une hypothèse alternative aberrante se traduit par une \\(p-\\)value très forte, généralement égale à (ou très proche de) 1. Dans le cas précis de cette étude, il serait abusif de faire un tel test unilatéral. En effet, les scientifiques suppose que si un lien existe entre les deux variables, la corrélation devrait être positive. Mais avoir observé une relation de cette nature chez d’autres espèces (qui plus est, chez des espèces très différentes) n’est pas suffisant. Car peut-être qu’une relation inverse peut également être observée dans d’autres groupes, et on peut très bien imaginer des mécanismes permettant de l’expliquer. Enfin, avoir observé une corrélation positive dans notre échantillon lors de l’examen préliminaire des données n’est jamais une raison suffisante pour choisir une hypothèse alternative unilatérale. Le choix des hypothèses devrait en effet toujours être effectué avant la collecte des données (voir Section 11.10). Il est donc bien plus honnête de réaliser un test unilatéral, puis, en cas de rejet de \\(H_0\\), de revenir aux estimation pour interpréter les résultats et conclure. C’est que nous allons voir maintenant.\n\n\n13.7.4 Estimation et intervalle de confiance\nRevenons à notre test bilatéral. La section “estimation” des résultats de ce test nous indique que la meilleure estimation du coefficient de corrélation linéaire de Pearson dans la population générale vaut \\(\\hat{\\rho} = 0.533\\). C’est la valeur que nous avions calculé à la main avec la fonction cor().\nL’intervalle de confiance à 95% de cette valeur estimée est également fourni. La conclusion de cette procédure pourrait donc être formulée de la façon suivante :\n\nAu seuil \\(\\alpha = 0.05\\), le test de corrélation de Pearson a permis de rejeter l’hypothèse nulle selon laquelle le nombre de visites agressives au nid des poussins et leur futur comportement agressif sont indépendants (\\(t = 2.96\\), \\(ddl = 22\\), \\(p = 0.007\\)). La meilleure estimation du coefficient de corrélation dans la population générale vaut \\(\\hat{\\rho} = 0.533\\). La vraie valeur dans la population générale a de bonnes chances de se trouver dans l’intervalle [0.17 ; 0.77] (intervalle de confiance à 95%).\n\nAutrement dit, le test a permis de rejeter l’hypothèse nulle et d’affirmer que les 2 variables sont corrélées. L’estimation du coefficient de corrélation et de son intervalle de confiance nous permettent de préciser le sens de cette relation (positive ou négative) et quantifier l’intensité de cette relation. Ici, la relation est bien positive : plus un individu est exposé à des comportements agressif au stade de poussin, plus il aura tendance à reproduire de tels comportements à l’âge adulte. L’incertitude associé à cette estimation de coefficient de corrélation est très grande (IC95% : [0.17 ; 0.77]). Un échantillonnage plus large permettrait de le réduire. Mais les études de ce type sont très coûteuses, notamment en temps, et on est souvent obligé de se contenter des données dont on dispose. La vraie corrélation entre ces 2 variables pourrait donc être relativement faible dans la population générale (0.17), laissant supposer que l’agressivité à l’âge adulte est finalement peu liée à l’agressivité à laquelle les poussins ont été exposés. Mais elle pourrait aussi être très forte (0.77), laissant supposer que l’agressivité à l’âge adulte est fortement liée à l’exposition des poussins à des comportement agressif. On voit bien ici que le seul test de corrélation ne permet pas de trancher dans l’absolu. Tout ce que fait un test, c’est dire si oui ou non on dispose d’assez de preuve pour affirmer qu’une hypothèse nulle est fausse. Le reste de l’interprétation dépend de l’estimation des paramètres de la population générale et de leur incertitude. Quand l’incertitude est faible, on peut être assez affirmatif. Mais quand elle est forte, comme ici, il faut rester prudent quant aux interprétations possibles."
  },
  {
    "objectID": "13-Correlation.html#corrélation-et-causalité",
    "href": "13-Correlation.html#corrélation-et-causalité",
    "title": "13  Corrélation",
    "section": "13.8 Corrélation et causalité",
    "text": "13.8 Corrélation et causalité\n\n13.8.1 Quelques exemples évidents\nOn entend souvent que “Corrélation n’est pas causalité”. Cela signifie que la corrélation ne mesure qu’un lien entre 2 variables, mais pas nécessairement que les variations de la première influencent celles de la deuxième. En réalité, si on dispose d’un nombre de variables suffisamment grand, on pourra toujours en trouver 2 qui sont fortement corrélées, sans qu’il n’y ait la moindre relation de causalité entre les deux.\nPar exemple, le nombre de morts par noyade aux états unis est corrélé à 66% (\\(r = 0.66\\)) avec le nombre de films dans lesquels Nicolas Cage joue chaque année. L’un n’est certainement pas la cause de l’autre. De même, le nombre de doctorats accordés chaque année dans le domaine du génie civil est corrélé à 96% (\\(r = 0.959\\)) avec la consommation de mozzarella. Là encore, on ne voit pas bien quelle relation de cause à effet pourrait exister entre ces 2 variables. Ces exemples (et de nombreux autres) peuvent être retrouvés, chiffres à l’appui, sur ce site web.\nPour ces exemples extrêmes, il est évident que la corrélation ne doit pas être interprétée comme une relation de cause à effet. Deux variables fortement corrélées sont simplement deux variables qui varient conjointement, dans le même sens (si la corrélation est positive) ou dans le sens opposé (si la corrélation est négative).\n\n\n13.8.2 Les variables confondantes\nDans certaines situations, il est pourtant tentant de parler de causalité. Par exemple, à la fin des année 1990, des scientifiques ont montré, dans une étude tout à fait sérieuse, que dans les villes de France où l’on consomme le plus de crème solaire, la prévalence des cancers de la peau est également la plus forte. Certains journaux de vulgarisation scientifique se sont empressés de reprendre ce résultat (une corrélation positive entre utilisation de crème solaire et prévalence des mélanomes), et de conclure, à tort, que la crème solaire contribuait donc à donner le cancer de la peau. Pourtant, “corrélation n’est pas causalité” ! Une variable importante, pourtant évoquée dans l’article scientifique, est restée ignorée des journalistes scientifiques de l’époque : l’exposition au soleil. En effet, dans les villes où l’exposition au soleil est la plus forte (les villes de la côte méditerranéenne par exemple), on met en moyenne plus de crème solaire qu’ailleurs, mais on développe aussi plus de mélanomes qu’ailleurs. À l’inverse, dans les villes les moins ensoleillées de France, on utilise beaucoup moins de crème solaire, mais on développe aussi beaucoup moins de mélanomes, simplement parce qu’on est moins exposé au risque.\nDans ce dernier exemple, la variable exposition au soleil est une variable confondante (ou “confounding variable” en anglais). C’est elle qui cause à la fois l’augmentation de la prévalence des mélanomes, et l’augmentation de l’utilisation de crème solaire. On a donc bien 2 relations de causalité, mais pas entre les variables que l’on étudie. La corrélation que l’on observe entre prévalence des mélanomes et utilisation de crème solaire n’est que la conséquence des relations de causalité avec la variable confondante.\nLa difficulté est ici que l’on ne peut pas savoir à l’avance quelle variable confondante pourrait venir influencer les variables que nous mesurons. Dans le cas de l’agressivité du fou de Grant, des traits génétiques particuliers pourraient par exemple expliquer le lien que nous observons entre nos 2 variables. Imaginons par exemple que la présence de certains allèle dans le génome des individus soit responsables à la fois d’une plus grande agressivité à l’âge adulte, et d’une plus grande autonomie lorsqu’ils sont jeunes. Des poussins possédant ces allèles seront plus autonomes que d’autres, il seront donc laissés plus souvent seuls par leurs parents, ce qui les exposera à des visites plus fréquentes d’adultes agressifs. Sous cette hypothèse, les 2 variables que nous avons étudiées ne sont liées entre elles que parce qu’il existe une relation de causalité entre les traits génétique des individus et chacune des 2 variables étudiées. C’est la raison pour laquelle, lorsque j’ai décrit les résultats de nos tests et des analyses descriptives, j’ai bien fait attention à ne pas dire que les visites agressives auprès des poussins étaient la cause de l’agressivité future des adultes. Je me suis contenté de dire que les variables étaient liées, et que plus un poussin reçoit de visites agressives, plus il aura lui même un comportement agressif à l’âge adulte. Prouver que l’un est la cause de l’autre, ou que l’autre est la conséquence de l’un est impossible avec ce type d’étude.\n\n\n13.8.3 Études expérimentales ou observationnelles\nLes exemples que nous venons d’aborder concernent tous des études dîtes observationnelles. À l’inverse des études expérimentales, dans lesquels l’expérimentateur a un certain contrôle des variables confondantes potentielles, ça n’est presque jamais le cas des études observationnelles. Dans l’exemple des fous de Grant, les scientifiques n’ont fait qu’observer des comportements dans une population naturelle. Ils n’ont pas eu la possibilité de vérifier en amont que tous les individus suivis avaient des gènes “normaux” vis-à-vis de l’agressivité. Dans le cas de l’étude sur la crème solaire, les chercheurs n’ont fait qu’observer ce qui se passe à plusieurs endroits de France. Ils n’ont pas pu s’assurer que l’ensoleillement était le même dans toutes les villes sur laquelle a porté cette étude.\nLes études expérimentales sont les seules à permettre d’établir des relations de cause à effet. C’est comme cela que par exemple, on peut affirmer qu’un vaccin est efficace ou non contre tel ou tel virus, ou à l’inverse qu’il présente tel ou tel effet secondaire. Pour tester l’efficacité d’un vaccin vis-à-vis d’un virus spécifique, on met en place une étude expérimentale dite “en double aveugle”. Dans la population générale, on va constituer 2 échantillons de patients atteints par le virus. On administrera ensuite le vaccin à l’un des deux groupes, alors qu’on distribuera un placebo (le même vaccin mais sans son composé actif) à l’autre groupe, dans les mêmes conditions. L’étude est “en double aveugle”, car les patients ne savent pas s’ils reçoivent le vaccin actif ou inactif, et les médecins qui administrent le traitement non plus. Cela a pour but de contrôler l’effet placebo.\nDans ces études, la façon dont les 2 groupes de patients sont constitués est sous le contrôle des expérimentateurs. Pour pouvoir établir des relations de causalité (entre administration du vaccin et guérison par exemple) ils doivent s’assurer que les groupes présentent les mêmes caractéristiques vis-à-vis de toutes les variables confondantes potentielles. Par exemple, on peut supposer que les hommes et les femmes ne réagissent pas de la même façon face au virus, ou face au vaccin. Ainsi, placer tous les hommes dans le premier groupe, et toutes les femmes dans le second groupe, serait évidemment une erreur. Car si le premier groupe guérit plus vite, comment peut-on être sûr de la cause de cette guérison ? Le premier groupe a-t-il guérit plus vite parce qu’il était constitué d’hommes, ou parce que les individus ont reçu le vaccin ? Puisque le sexe des individus est une variable confondante potentielle, il est important de répartir équitablement hommes et femmes dans les deux groupes. Et il en va de même pour énormément de variables confondantes potentielles : sexe des individus, âge, niveau d’études, revenu moyen, catégorie socio-professionnelle, etc. Ça n’est qu’en s’assurant que les 2 groupes sont homogènes vis-à-vis de l’ensemble de ces facteurs que les différences éventuelles qui seront observées à l’issue de l’expérience pourront être attribuées sans le moindre doute au traitement étudié : ici, l’administration du vaccin.\nDans le domaine de l’écologie, la plupart des études sont observationnelles, et elles permettent au mieux d’établir des corrélations, des liens entre variables, mais beaucoup plus rarement des liens de causalité formels. Il est toutefois souvent possible, après avoir observé un lien entre variables dans le milieu naturel, de mettre au point des expériences (donc des études expérimentales) permettant de tester des hypothèses précises, y compris des relations de causalité.\nPar exemple, dans le milieu marin, en particulier littoral, l’apparition d’imposex chez certains mollusques2 a pu être associé à la présence de tributylétain (TBT) à l’état de trace dans l’eau de mer&gt; lorsque ce phénomène a été décelé, il était impossible d’affirmer que le TBT causait l’apparition d’imposex ; il n’y avait qu’une corrélation. Ça n’est que dans un second temps qu’une étude expérimentale en milieu contrôlé a permis d’établir un lien de causalité. Deux groupes de mollusques identiques en tous points sont placés dans différents bassins. On répartit ensuite de façon aléatoire les bassins en plusieurs lots, et chaque lot de bassin se voit attribuer un traitement : absence de TBT, TBT à la concentration X, TBT à la concentration Y, etc. C’est ce type d’étude expérimentale qui a permis d’établir avec certitude le caractère de perturbateur endocrinien du TBT, de prouver que l’imposex des mollusques pouvait être causé par le TBT, et de connaitre les concentrations à partir desquelles les effets apparaissent.2 apparitions d’organes génitaux mâles chez des femelles saines par ailleurs\n\n\n\n\n\n\nImportant\n\n\n\nCorrélation n’est pas causalité. Les études observationnelles ne peuvent (presque) jamais établir de lien de causalité formel entre variables. Au mieux, elles peuvent constater que des variables varient conjointement, dans le même sens ou en un sens opposé.\nSeules les études expérimentales, dans lesquelles toutes les variables confondantes potentielles sont contrôlées, sont susceptibles de faire apparaître de véritables relations de cause à effet."
  },
  {
    "objectID": "13-Correlation.html#lalternative-non-paramétrique",
    "href": "13-Correlation.html#lalternative-non-paramétrique",
    "title": "13  Corrélation",
    "section": "13.9 L’alternative non paramétrique",
    "text": "13.9 L’alternative non paramétrique\nQuand les conditions d’application du test de corrélation de Pearson ne sont pas remplies (ce qui était le cas ici, voir Section 13.7.2), il faut faire un test équivalent non paramétrique. Le test utilisé le plus fréquemment dans cette situation est le test du \\(\\rho\\) de Spearman (\\(\\rho\\) est la lettre grecque “rho”, et non la lettre “p”). On l’effectue comme le test de Pearson en précisant simplement un argument supplémentaire : method = \"spearman\" (sans majuscule) :\n\ncor.test(birds$nVisitsNestling, birds$futureBehavior,\n         method = \"spearman\")\n\nWarning in cor.test.default(birds$nVisitsNestling, birds$futureBehavior, :\nImpossible de calculer la p-value exacte avec des ex-aequos\n\n\n\n    Spearman's rank correlation rho\n\ndata:  birds$nVisitsNestling and birds$futureBehavior\nS = 1213.5, p-value = 0.01976\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n     rho \n0.472374 \n\n\nLe test de Spearman est au test de Pearson ce que le test de Wilcoxon est au test de Student, ou ce que le test de Kruskal-Wallis est à l’ANOVA. Il travaille non pas sur les données brutes (ici, les mesures des scientifiques), mais sur des données modifiées, en l’occurrence, sur les rangs des données. La première conséquence évidente est une perte de puissance notable par rapport au test de Pearson. Cette perte de puissance peut être ici observée par le biais de la \\(p-\\)value plus élevée (donc moins significative) que pour le test précédent. Cela indique que même si la conclusion est la même, on rejette ici l’hypothèse nulle avec moins de confiance que pour le test de Pearson.\nLe \\(\\rho\\) de Spearman est équivalent au \\(r\\) de Pearson calculé sur les rangs des données. Lorsque plusieurs valeurs observées sont égales, plusieurs valeurs ont le même rang, ce qui cause l’apparition du message d’avertissement suivant :\nImpossible de calculer la p-value exacte avec des ex-aequos\nCe message est sans conséquence tant que la \\(p-\\)value du test de Spearman est éloignée du seuil \\(\\alpha\\) (ce qui est le cas ici). Mais quand \\(p \\approx \\alpha\\), il faut être particulièrement prudent quant à l’interprétation qui est faite des résultats.\nEnfin, comme pour le test de Pearson, il est possible de réaliser un test de Spearman unilatéral en utilisant l’argument alternative = \"less\" ou alternative = \"greater\". Les précautions à prendre pour utiliser ce genre de test sont toujours les mêmes."
  },
  {
    "objectID": "13-Correlation.html#exercices",
    "href": "13-Correlation.html#exercices",
    "title": "13  Corrélation",
    "section": "13.10 Exercices",
    "text": "13.10 Exercices\n\n13.10.1 Canis lupus\nEn 1970, le loup canis lupus a été éradiqué en Norvège et en Suède. Autour de 1980, un couple de loups, originaire d’une population plus à l’Est, a fondé une nouvelle population en Suède. En l’espace de 20 ans, cette population comptait approximativement 100 loups. Il y a toutefois fort à craindre qu’une population fondée par un si petit nombre d’individus souffre de consanguinité. Liberg et al. (2005) ont compilé les informations sur la reproduction dans cette population entre 1983 et 2002, et ils ont pu reconstruire le pédigrée des individus la composant. Ils ont ainsi été en mesure de déterminer avec précision le coefficient individuel de consanguinité dans 24 portées de louveteaux. Pour mémoire, le coefficient individuel de consanguinité vaut 0 si ses parents ne sont pas apparentés, 0.25 si ses parents sont frères et sœurs issus de grands-parents non apparentés, et plus de 0.25 si les associations consanguines se répètent depuis plusieurs générations.\nOn souhaite déterminer si le coefficient de consanguinité est associé à la probabilité de survie des jeunes durant leur premier hiver. Les données de Liberg et al. (2005) sont disponibles dans le fichier loups.csv. La première colonne contient les coefficients de consanguinité et la seconde, le nombre de jeunes de chaque portée ayant survécu à leur premier hiver. Vous analyserez ces données en suivant l’ordre des étapes décrites plus haut. En particulier, vous prendrez soin de :\n\nVérifier la qualité des données.\nMettre les données dans un format approprié si besoin.\nRéaliser une exploration statistique puis visuelle des données.\nVérifier les conditions d’application d’un test paramétrique.\nFaire le test approprié en posant les hypothèses nulles et alternatives judicieuses.\nRépondre à la question posée en intégrant tous les éléments utiles.\n\n\n\n13.10.2 Les miracles de la mémoire\nÀ quel point les souvenirs d’évènements miraculeux sont-il fiables ? Une façon d’étudier cette question est de comparer différents récits de tours de magie extraordinaires. Parmi les tours célèbres, on trouve celui de la corde du fakir. Dans l’une de ses versions, un magicien jette l’extrémité d’une corde d’apparence normale en l’air et cette corde devient rigide. Un garçon grimpe à la corde et finit par disparaître en haut de la scène. Le magicien lui demande de répondre mais n’obtient pas de réponse. Il attrape alors un couteau, grimpe à son tour, et le garçon, découpé en morceaux, tombe du ciel dans un panier posé par terre. Le magicien redescend de la corde et aide le garçon vivant, en un seul morceau et non blessé, à sortir du panier.\nWiseman et Lamont (1996) ont retrouvé 21 récits écrits de ce tour par des personnes ayant elles-mêmes assisté à ce tour. Ils ont attribué un score à chaque description selon le caractère plus ou moins impressionnant de la description. Par exemple, un score de 1 était attribué si le récit faisait état que “le garçon grimpe à la corde, puis il en redescend”. Les récits les plus impressionnants se sont vus attribuer la note de 5 (“le garçon grimpe, disparaît, est découpé en morceaux et réapparaît en chair et en os devant le public”). Pour chaque récit, les chercheurs ont également enregistré le nombre d’années écoulées entre le moment où le témoin a assisté au tour de magie, et le moment où il a consigné son récit par écrit.\nY a-t-il un lien entre le caractère impressionnant (“impressiveness”) d’un souvenir et le temps écoulé jusqu’à l’écriture de sa description (“years”) ? Si oui, cela pourrait indiquer une tendance de la mémoire humaine à exagérer et à perdre en précision avec le temps.\nLes données de Wiseman et Lamont (1996) sont disponibles dans le fichier ropetrick.csv. Importez ces données et analysez-les en respectant les consignes de l’exercice précédent.\n\n\n\n\nLiberg, Olof, Henrik Andrén, Hans-Christian Pedersen, Håkan Sand, Sejberg, Petter Wabakken, Mikael Åkesson, et Staffan Bensch. 2005. « Severe Inbreeding Depression in a Wild Wolf Canis Lupus Population ». Biology Letters 1 (1): 17‑20. https://doi.org/10.1098/rsbl.2004.0266.\n\n\nMüller, Martina S., Elaine T. Porter, Jacquelyn K. Grace, Jill A. Awkerman, Kevin T. Birchler, Alex R. Gunderson, Eric G. Schneider, Mark A. Westbrock, et David J. Anderson. 2011. « Maltreated Nestlings Exhibit Correlated Maltreatment as Adults: Evidence of a “Cycle of Violence” in Nazca Boobies ( Sula Granti ) ». The Auk 128 (4): 615‑19. https://doi.org/10.1525/auk.2011.11008.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, et Shannon Ellis. 2022. skimr: Compact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr.\n\n\nWickham, Hadley. 2023. tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, et Teun van den Brand. 2024. ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, et Davis Vaughan. 2023. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, et Jennifer Bryan. 2023. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWiseman, Richard, et Peter Lamont. 1996. « Unravelling the Indian Rope-Trick ». Nature 383 (6597): 212. https://doi.org/10.1038/383212a0."
  },
  {
    "objectID": "14-Regression.html#sec-packages6",
    "href": "14-Regression.html#sec-packages6",
    "title": "14  Régression linéaire",
    "section": "14.1 Pré-requis",
    "text": "14.1 Pré-requis\nComme pour chaque nouveau chapitre, je vous conseille de travailler dans un nouveau script que vous placerez dans votre répertoire de travail, et dans une nouvelle session de travail (Menu Session &gt; Restart R). Inutile en revanche de créer un nouveau Rproject : vos pouvez tout à fait avoir plusieurs script dans le même répertoire de travail et pour un même Rproject. Comme toujours, consultez le livre en ligne du semestre 3 si vous ne savez plus comment faire.\nSi vous êtes dans une nouvelle session de travail (ou que vous avez quitté puis relancé RStudio), vous devrez penser à recharger en mémoire les packages utiles. Dans ce chapitre, vous aurez besoin d’utiliser :\n\nle tidyverse (Wickham 2023), qui comprend notamment le package readr (Wickham, Hester, et Bryan 2023), pour importer facilement des fichiers .csv au format tibble, le package dplyr (Wickham et al. 2023), pour manipuler des tableaux, et le package ggplot2 (Wickham et al. 2024) pour les représentations graphiques.\nskimr (Waring et al. 2022), qui permet de calculer des résumés de données très informatifs.\ndatasauRus (Davies, Locke, et D’Agostino McGowan 2022), qui fournit plusieurs jeux de données fictifs que nous examinerons en guise d’exercices.\n\n\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(datasauRus)\n\nVous aurez également besoin des jeux de données suivants, qu’il vous faut donc télécharger dans votre répertoire de travail :\n\nplantbiomass.csv\nhockey.csv\n\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "14-Regression.html#principe",
    "href": "14-Regression.html#principe",
    "title": "14  Régression linéaire",
    "section": "14.2 Principe",
    "text": "14.2 Principe\nLa régression linéaire est une méthode qui fait partie de la famille des modèles linéaires, tout comme l’ANOVA. ANOVA et régression linéaires sont en effet deux méthodes très proches, et leur mise en œuvre dans RStudio présente de nombreuses similitudes, tant dans la syntaxe des fonctions que nous utiliserons, que dans la façon de vérifier les conditions d’application.\nLa régression linéaire est une méthode souvent utilisée pour prédire les valeurs d’une variable numérique (appelée variable expliquée) à partir des valeurs d’une seconde variable 9appelée variable explicative). Par exemple, le nuage de points de la figure ci-dessous montre comment la diversité génétique dans une population humaine locale peut être prédite par sa distance de dispersion depuis l’Est africain en ajustant une droite aux données (d’après Whitlock et Schluter 2015). L’homme moderne est apparu en Afrique et nos ancêtres ont perdu un peu de diversité génétique à chaque étape de leur colonisation de nouveaux territoires.\n\nContrairement à la corrélation, ici, on n’examine pas seulement une éventuelle liaison entre 2 variables numériques : on suppose qu’une variable peut être (en partie) expliquée par une autre. Nous aurons donc à distinguer les variables expliquées (ou dépendantes) qui figureront sur l’axe des ordonnées et seront nos variables prédites, et les variables explicatives (ou indépendantes) qui figureront sur l’axe des abscisses et seront les prédicteurs.\nContrairement à la corrélation qui, comme nous l’avons expliqué en détail, ne permet pas d’aborder les questions de causalité, lorsque l’on s’intéresse à la régression linéaire, on essaie au contraire de prédire ou d’expliquer les variations de la variable expliquée par celles de la variable explicative. En d’autres termes, on considère que les variations de la variable explicative sont au moins en partie la cause des variations de la variable expliquée.\nLorsque l’on s’intéresse à la régression linéaire, on considère que la relation qui lie les deux variables est linéaire, et on souhaite quantifier l’intensité de cette relation (quand la variable explicative augmente d’une unité, de combien d’unité la variable expliquée augmente ou diminue-t-elle ?). Nous allons voir maintenant comment mettre en œuvre cette méthode dans RStudio."
  },
  {
    "objectID": "14-Regression.html#contexte",
    "href": "14-Regression.html#contexte",
    "title": "14  Régression linéaire",
    "section": "14.3 Contexte",
    "text": "14.3 Contexte\nLes activités humaines réduisent le nombre d’espèces dans un grand nombre d’écosystèmes à la surface du globe. Est-ce que cette diminution du nombre d’espèces affecte le fonctionnement de base des écosystèmes ? Où est-ce qu’au contraire, les espèces végétales sont majoritairement interchangeables, les fonctions écologique des espèces disparues1 pouvant être assurées par les espèces toujours présentes ?1 par exemple, la production d’O\\(_2\\) et la fixation de CO\\(_2\\), la dépollution des sols, leur fixation, la protection contre les inondations et l’érosion…\nPour tenter de répondre à cette question, Tilman, Reich, et Knops (2006) ont ensemencé 161 parcelles de 9 mètres sur 9 mètres dans la réserve de Cedar Creek (Minesota, USA). Ils ont utilisé un nombre variable d’espèces typiques des prairies et ont mesuré la production de biomasse de chaque parcelle pendant 10 ans. Des lots de 1, 2, 4, 8 ou 16 plantes pluriannuelles (choisies au hasard parmi une liste de 18 espèces possibles) ont été assignés au hasard dans chacune des 161 parcelles. À l’issue des 10 années d’étude, les chercheurs ont mesuré un indice de stabilité de la biomasse en divisant la moyenne des biomasses sur 10 ans, par l’écart-type de ces mêmes biomasses."
  },
  {
    "objectID": "14-Regression.html#importation-et-mise-en-forme-des-données",
    "href": "14-Regression.html#importation-et-mise-en-forme-des-données",
    "title": "14  Régression linéaire",
    "section": "14.4 Importation et mise en forme des données",
    "text": "14.4 Importation et mise en forme des données\nLes données de cette expérience sont disponibles dans le fichier plantbiomass.csv.\nComme toujours, on importe les données et on commence par un examen visuel afin de détecter les éventuels problèmes et pour savoir où l’on va.\n\nplant\n\n# A tibble: 161 × 2\n   nSpecies biomassStability\n      &lt;dbl&gt;            &lt;dbl&gt;\n 1        1             2.01\n 2        1             1.91\n 3        1             1.89\n 4        1             1.86\n 5        1             1.74\n 6        1             1.66\n 7        1             1.57\n 8        1             1.48\n 9        1             1.48\n10        1             1.45\n# ℹ 151 more rows\n\n\nCe premier examen nous montre que nous disposons bien de 161 observations pour 2 variables : le nombre d’espèces présentes dans la parcelle pendant 10 ans, et l’indice de stabilité de la biomasse de chaque parcelle. Visiblement, les données sont au bon format, on dispose bien de toutes les variables dont on a besoin et leurs noms sont parlants. Nous n’aurons donc pas besoin de modifier quoi que ce soit dans ces données."
  },
  {
    "objectID": "14-Regression.html#exploration-statistique-des-données",
    "href": "14-Regression.html#exploration-statistique-des-données",
    "title": "14  Régression linéaire",
    "section": "14.5 Exploration statistique des données",
    "text": "14.5 Exploration statistique des données\nComme toujours, on examine quelques statistiques descriptives de position et de dispersion (voir d’incertitude), pour se faire un idée de la forme des données et pour repérer les éventuelles données manquantes ou valeurs aberrantes.\n\nskim(plant)\n\n── Data Summary ────────────────────────\n                           Values\nName                       plant \nNumber of rows             161   \nNumber of columns          2     \n_______________________          \nColumn type frequency:           \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable    n_missing complete_rate mean    sd    p0  p25  p50  p75  p100\n1 nSpecies                 0             1 6.32 5.64  1     2    4    8    16   \n2 biomassStability         0             1 1.41 0.394 0.293 1.12 1.39 1.65  2.76\n  hist \n1 ▇▁▂▁▃\n2 ▁▆▇▃▁\n\n\nCe premier examen nous montre que nous n’avons aucune données manquantes et que l’indice de stabilité a une distribution à peu près symétrique et qu’il varie d’un peu plus de 0.3 à près de 2.8. Pour en apprendre un peu plus, nous pouvons examiner les données en groupes. Ici, la variable nSpecies est bien une variable numérique, mais elle prend seulement quelques valeurs entières (1, 2, 4, 8 ou 16 espèces). Il est donc possible de regarder les valeurs de stabilité de biomasse pour chaque nombre d’espèces dans les parcelles:\n\nplant %&gt;%\n  group_by(nSpecies) %&gt;%\n  skim()\n\n── Data Summary ────────────────────────\n                           Values    \nName                       Piped data\nNumber of rows             161       \nNumber of columns          2         \n_______________________              \nColumn type frequency:               \n  numeric                  1         \n________________________             \nGroup variables            nSpecies  \n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable    nSpecies n_missing complete_rate mean    sd    p0   p25  p50\n1 biomassStability        1         0             1 1.21 0.388 0.728 0.880 1.10\n2 biomassStability        2         0             1 1.28 0.360 0.293 1.09  1.32\n3 biomassStability        4         0             1 1.31 0.314 0.756 1.10  1.35\n4 biomassStability        8         0             1 1.50 0.251 0.928 1.32  1.48\n5 biomassStability       16         0             1 1.71 0.403 1.08  1.39  1.66\n   p75 p100 hist \n1 1.48 2.01 ▇▅▃▂▂\n2 1.51 2.00 ▁▃▇▇▂\n3 1.51 2.00 ▅▇▇▇▁\n4 1.61 2.05 ▁▅▇▂▂\n5 1.96 2.76 ▇▆▆▃▂\n\n\nCette fois, on obtient des informations pour chaque groupe de parcelles contenant un nombre d’espèces spécifique. On constate par exemple que la moyenne de l’indice de stabilité de la biomasse augmente très peu entre les catégories 1, 2 et 4 espèces par parcelle, mais que l’augmentation semble plus marquée pour 8 et 16 espèces par parcelle. Tous les écarts-types semblent très proches. Les parcelles avec 1 et 16 espèces en particulier présentent des histogrammes nettement asymétriques.\nComme pour la corrélation, il est inutile ici de calculer des indices d’imprécision. Ça n’est pas la moyenne de ces variables qui nous intéresse, mais la relation entre elles. Nous serons en revanche amenés à calculer des intervalles de confiances à 95% des paramètres de la régression linéaire, puisque ce sont eux qui nous permettront de qualifier (et quantifier) la relation entre les 2 variables."
  },
  {
    "objectID": "14-Regression.html#exploration-graphique-des-données",
    "href": "14-Regression.html#exploration-graphique-des-données",
    "title": "14  Régression linéaire",
    "section": "14.6 Exploration graphique des données",
    "text": "14.6 Exploration graphique des données\nVisualiser les données est toujours aussi indispensable. Ici, comme pour la corrélation, on commence par un nuage de points pour visualiser les données et la forme de leur relation :\n\nplant %&gt;%\n  ggplot(aes(x = nSpecies, y = biomassStability)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Nombre d'espèces par parcelle\",\n       y = \"Indice de stabilité de la biomasse\")\n\n\n\n\nCe graphique nous apprend que contrairement à la plupart des méthodes statistiques vues jusqu’ici, il n’est pas nécessaire que les données des variables soient distribuées selon une loi Normale. Ici, nous avons des données qui sont tout sauf normales pour la variable explicative puisque nous avons seulement les entiers 1, 2, 4, 8 et 16. Un histogramme ou une courbe de densité montre que la distribution de cette variable est très loin de la Normalité :\n\nplant %&gt;%\n  ggplot(aes(x = nSpecies)) +\n  geom_density(fill = \"firebrick2\", adjust = 0.2) +\n  labs(x = \"Nombre d'espèces par parcelle\", \n       y = \"Densité\")\n\n\n\n\nCela n’est pas du tout problématique : comme pour l’ANOVA, les conditions d’application porteront sur les résidus de la régression, pas sur les variables elles-mêmes. Comme pour l’ANOVA, ce sont les résidus de la régression qui devront suivre une distribution Normale, pas les variables de départ.\nOn peut visualiser dès maintenant la droite de régression linéaire qui permet de lier ces deux variables grâce à la fonction geom_smooth(method = \"lm\", se = FALSE)` :\n\nplant %&gt;%\n  ggplot(aes(x = nSpecies, y = biomassStability)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Nombre d'espèces par parcelle\",\n       y = \"Transformation log\\n de l'indice de stabilité de la biomasse\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nL’argument method = \"lm\" indique qu’on souhaite ajouter une droite de régression sur le graphique, et se = FALSE permet de ne faire apparaître que la droite de régression, sans son intervalle d’incertitude. Nous reviendrons sur la notion d’incertitude de la régression un peu plus loin.\nÀ supposer que nous ayons le droit d’effectuer une régression linéaire (ce qu’il faudra vérifier avec les conditions d’application, après avoir fait la régression), la pente devrait être positive."
  },
  {
    "objectID": "14-Regression.html#le-test-paramétrique",
    "href": "14-Regression.html#le-test-paramétrique",
    "title": "14  Régression linéaire",
    "section": "14.7 Le test paramétrique",
    "text": "14.7 Le test paramétrique\n\n14.7.1 Les hypothèses\nÀ une exception près, la procédure de régression linéaire est en tous points identique à l’analyse de variance. Quand on fait une ANOVA, la variable expliquée est numérique et la variable explicative est catégorielle (c’est un facteur). Dans RStudio, la formule ressemble donc à ceci :\n\\[Y \\sim F\\] Quand on fait une régression linéaire, les 2 variables sont numériques. La formule dans `RStudio ressemble donc à ça :\n\\[Y \\sim X\\] Dans ces formules, Y est la variable numérique expliquée, F est une variable catégorielle (ou facteur) et X est une variable numérique explicative. La forme est donc très proche, et tout le reste est identique : on exprime la variable expliquée en fonction de la variable explicative et on vérifie après coup, grâce aux résidus, si nous avions le droit ou non de faire l’analyse.\nLa différence majeure entre ANOVA et régression linéaire concerne les hypothèses du test. Faire une régression linéaire revient en effet à effectuer en même temps 2 tests d’hypothèses indépendants : le premier concerne l’ordonnée à l’origine de la droite de régression et le second concerne la pente de la droite de régression. On ne parle donc plus de comparer des moyennes entre groupes : on cherche à déterminer si la pente et l’ordonnée à l’origine de la meilleure droite de régression possible valent zéro ou non. Les hypothèses de ces tests sont les suivantes :\nPour l’ordonnée à l’origine (“intercept” en anglais) :\n\nH\\(_0\\) : l’ordonnée à l’origine de la droite de régression vaut 0 dans la population générale.\nH\\(_1\\) : l’ordonnée à l’origine de la droite de régression est différente de 0 dans la population générale.\n\nPour la pente (“slope” en anglais) :\n\nH\\(_0\\) : la pente de la droite de régression vaut 0 dans la population générale. Autrement dit, il n’y a pas de lien entre les deux variables.\nH\\(_1\\) : la pente de la droite de régression est différente de 0 dans la population générale. Autrement dit, il y a bien un lien entre les deux variables étudiées.\n\nVous notez qu’ici, comme pour tous les autres tests statistiques traités dans ce livre en ligne, les tests ne permettent que de rejeter ou non les hypothèses nulles. Si on rejette ces hypothèses, le test ne nous dit rien de la valeur de la pente et de l’ordonnée à l’origine. On sait que ces paramètres sont significativement différents de zéro, mais rien de plus. Il faudra alors recourir à l’estimation pour déterminer la valeur de ces paramètres, ainsi que leurs intervalles d’incertitude.\n\n\n14.7.2 Réalisation du test\nPour faire une régression linéaire dans RStudio, on utilise la fonction lm() (comme linear model). Et comme pour l’ANOVA, les résultats de l’analyse doivent être stockés dans un objet puisque cet objet contiendra tous les éléments utiles pour vérifier les conditions d’application :\n\nreg1 &lt;- lm(biomassStability ~ nSpecies, data = plant)\n\nComme pour l’ANOVA, on affiche les résultats de ces tests à l’aide de la fonction summary()\n\nsummary(reg1)\n\n\nCall:\nlm(formula = biomassStability ~ nSpecies, data = plant)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.97148 -0.25984 -0.00234  0.23100  1.03237 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.198294   0.041298  29.016  &lt; 2e-16 ***\nnSpecies    0.032926   0.004884   6.742 2.73e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3484 on 159 degrees of freedom\nMultiple R-squared:  0.2223,    Adjusted R-squared:  0.2174 \nF-statistic: 45.45 on 1 and 159 DF,  p-value: 2.733e-10\n\n\nDans la forme, ces résultats sont très proches de ceux de l’ANOVA. La rubrique Residuals donne des informations sommaires sur les résidus. Ces informations sont utiles puisque les résidus serviront à vérifier les conditions d’application de la régression. À ce stade, on regarde surtout si la médiane des résidus est proche de 0 et si les résidus sont à peu près symétriques (les premier et troisième quartiles ont à peu près la même valeur absolue, idem pour le minimum et le maximum).\nLe tableau Coefficients est celui qui nous intéresse le plus puisqu’il nous fournit, outre la réponse aux 2 tests, les estimations pour l’ordonnée à l’origine et la pente de la droite de régression.\nAvant d’aller plus loin dans l’interprétation de ces résultats, il nous faut déterminer si nous avions bel et bien le droit de réaliser cette régression, en vérifiant si ses conditions d’application sont remplies.\n\n\n14.7.3 Conditions d’application\nLes conditions d’application de la régression sont les mêmes que celles de l’ANOVA. Je vous renvoie donc à la Section 12.6.2 pour savoir quelles sont ces conditions d’application et comment les vérifier. J’insiste bien sur le fait que les conditions d’application sont absolument identiques à celles de l’ANOVA. Si je fais ici l’économie de la description, vous ne devez jamais faire l’économie de la vérification des conditions d’application.\n\npar(mfrow = c(2, 2))\nplot(reg1)\n\n\n\npar(mfrow = c(1, 1))\n\nC’est seulement après avoir réalisé, examiné et commenté ces graphiques que vous serez en mesure de dire si oui ou non vous aviez le droit de faire la régression linéaire, et donc d’en interpréter les résultats.\nIci, les conditions d’application semblent tout à fait remplies :\n\nLes deux graphiques de gauche confirment que les résidus sont homogènes. En particulier, sur le premier graphique (en haut à gauche), la ligne rouge est presque parfaitement horizontale, il y a à peu près autant de résidus au-dessus qu’en dessous de la ligne pointillée, et les résidus pourraient rentrer dans une boîte ayant la même hauteur d’un bout à l’autre du graphique (pas d’effet “entonnoir” ou “nœud papillon”).\nLe graphique quantile-quantile (en haut à droite), montre des points qui sont presque parfaitement alignés sur la droite pointillée, indiquant des résidus distribués selon une distribution Normale.\n\nOn pourrait vérifier ces éléments avec des tests statistiques (encore une fois, reportez vous à la Section 12.6.2 si vous ne vous rappelez plus comment faire), mais c’est ici inutile tant les conditions semblent bien respectées.\nLe dernier graphique (en bas à droite, “Residuals vs Leverage”) ne permet pas de vérifier les conditions d’application à proprement parler, mais permet de repérer des points ayant un poids trop important dans l’analyse. Ces points devraient être retirés s’il y en a (ce qui n’est pas le cas ici), car leur influence est tellement forte qu’ils faussent grandement les résultats de l’analyse. Pour voir à quoi ce graphique ressemble quand de tels points sont présents, je représente ci-dessous un exemple fictif.\nImaginez un jeu de données dans lequel absolument aucune tendance n’est présente. Le nuage de points d’un tel jeu de données devrait être approximativement circulaire, avec une droite de régression presque horizontale, indiquant une absence de lien entre les 2 variables étudiées x et y :\n\n\n\n\n\nImaginez maintenant qu’on ajoute à ces données un unique point (en rouge sur le graphique), très éloigné des autres :\n\n\n\n\n\nLa seule présence de ce point modifierait très fortement les résultats de la régression linéaire :\n\n\n\n\n\nSans ce point supplémentaire, la droite de régression a une pente légèrement négative, avec ce point, la pente est fortement positive. Il n’est pas normal qu’une observation unique prenne le pas sur toutes les autres (il y en a 50) et qu’elle affecte autant les résultats de la régression. la situation est ici caricaturale, et on voit bien qu’il faudrait retirer la valeur atypique pour obtenir des résultats censés. Les points ayant une influence démesurée sur les résultats ne sont pas toujours aussi évidents à repérer. C’est justement à cela que sert le graphique “Residuals vs Leverage” :\n\n\n\n\n\nSur ce graphique, les points qui apparaissent au-delà des lignes pointillées (en haut à droite ou en bas à gauche du graphique) sont ceux qui ont une influence trop forte sur les résultats et qu’il faudrait donc retirer des données pour obtenir des résultats plus représentatifs de la tendance observée pour la majorité des points.\nSi je reviens à nos données de stabilité des biomasses en fonction du nombre d’espèces par parcelles, les lignes courbes pointillées qui délimitent les zones “à problème” ne sont même pas visibles sur le graphique. Nous n’avons donc pas de points problématiques.\nAu final, les conditions d’application de la régression sont parfaitement vérifiées et nous pouvons donc en interpréter les résultats.\n\n\n14.7.4 Interprétation des résultats\nRevenons donc à l’affichage des résultats :\n\nsummary(reg1)\n\n\nCall:\nlm(formula = biomassStability ~ nSpecies, data = plant)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.97148 -0.25984 -0.00234  0.23100  1.03237 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.198294   0.041298  29.016  &lt; 2e-16 ***\nnSpecies    0.032926   0.004884   6.742 2.73e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3484 on 159 degrees of freedom\nMultiple R-squared:  0.2223,    Adjusted R-squared:  0.2174 \nF-statistic: 45.45 on 1 and 159 DF,  p-value: 2.733e-10\n\n\nOutre une description synthétique de la distribution des résidus, ces résultats nous apprennent que :\n\nl’ordonnée à l’origine (intercept) est estimée à 1.198 (rappelez-vous que cette valeur fait référence à l’indice de stabilité de la biomasse)\nla pente est estimée à 0.033 (quand le nombre d’espèces augmente d’une unité, l’indice de stabilité de la biomasse augmente de 0.033 unités)\n\nLes \\(p-\\)values de chacun des 2 tests sont fournies dans la dernière colonne et sont ici très inférieures à \\(\\alpha\\) : on rejette donc les 2 hypothèses nulles. En particulier, puisque l’hypothèse nulle est rejetée pour le test qui concerne la pente de la droite, on peut considérer que le nombre de plantes dans les parcelles influence bel et bien l’indice de stabilité de la biomasse. Autrement dit, le nombre de plantes dans les parcelles, permet, dans une certaine mesure, de prédire la valeur de l’indice de stabilité de la biomasse.\nLa relation n’est toutefois pas très forte : le nombre de plantes dans chaque parcelle ne permet de prédire l’indice de stabilité de la biomasse que dans une mesure assez faible. C’est le Adjusted R-squared qui nous indique quelle est la “qualité” de prédiction du modèle. Ici, il vaut 0.22. Cela signifie que 22% des variations de l’indice de stabilité de la biomasse sont prédits par le nombre de plantes dans les parcelles. Une autre façon de présenter les choses consiste à dire que 78% des variations de l’indice de stabilité de biomasse sont expliqués par d’autres facteurs que celui que nous avons pris en compte dans notre modèle de régression linéaire (i.e. le nombre d’espèces par parcelle). Le \\(R^2\\) (à en pas confondre avec le coefficient de corrélation \\(r\\)) renseigne sur la qualité de l’ajustement des données à la droite de régression. Il nous indique ici que le pouvoir prédictif de notre modèle linéaire est assez faible. Il est néanmoins significatif, ce qui indique que notre variable explicative joue bel et bien un rôle non négligeable dans les variations de la variable expliquée. Une autre façon de comprendre ce résultat est la suivante : si on connait le nombre de plantes dans une parcelle, on peut prédire 22% de la valeur de l’indice de stabilité de la biomasse.\n\n\n14.7.5 Intervalle de confiance de la régression\nL’équation de notre droite de régression vaut donc :\n\\[ y = 0.033 \\times x + 1.198\\] Avec y, l’indice de stabilité de la biomasse, et x, le nombre d’espèces par parcelles. On voit bien que la droite nous permet de prédire une valeur d’indice de stabilité de la biomasse pour un nombre d’espèces donnée par parcelle, y compris pour des nombres d’espèces qui n’ont pas été testés. par exemple, pour $n = $ 6 espèces, on peut s’attendre à un indice de stabilité de la biomasse de \\(0.033 * 6 + 1.198 = 1.396\\). Il convient toutefois de prendre deux précautions très importantes quand on fait ce genre prédiction :\n\nla régression et son équation ne sont valables que sur l’intervalle que nous avons étudié pour la variable explicative. Ainsi, on peut faire des prédictions pour des valeurs de nombre d’espèces comprises entre 1 et 16, mais pas au-delà. En effet, rien ne nous dit que cette relation reste valable au-delà de la gamme \\(n =\\) [1 ; 16]. Peut-être la relation change-t-elle de nature à partir de \\(n =\\) 20 espèces par parcelles. Peut-être que la pente devient nulle ou négative. Ou peut-être la relation n’est-elle plus linéaire au-delà de \\(n =\\) 16 espèces par parcelle. En bref, puisque nous n’avons des informations sur le comportement de notre système d’étude que pour une gamme de valeurs précises sur l’axe des x, il nous est impossible de prédire quoi que ce soit en dehors de cette gamme de valeurs.\nMême à l’intérieur de la gamme de valeur permettant de faire des prédictions, toute prédiction est entachée d’incertitude. Pour s’en convaincre, il suffit de regarder la grande dispersion des valeurs observées pour y pour chaque valeur de x. Par exemple, pour $n = $ 8 espèces par parcelle, les valeurs observées pour l’indice de stabilité vont de moins de 1 à plus de 2. Le modèle prédit une valeur d’environ 1.46 (\\(0.033 * 8 + 1.198 = 1.462\\)), mais on voit bien que l’incertitude persiste. C’est la raison pour laquelle on aura toujours besoin de calculer des indices d’incertitude, pour avoir une idée de l’erreur commise lorsque l’on fait une prédiction.\n\n\n\n\n\n\n\nPrudence avec les prédictions\n\n\n\nUne droite de régression permet de faire des prédictions :\n\nuniquement sur la gamme de valeurs de l’axe des x qui a permis d’établir l’équation de la droite de régression\navec une imprécision/incertitude qu’il est toujours nécessaire d’estimer.\n\n\n\nLa pente et l’ordonnée à l’origine de cette droite de régression ont été obtenues à partir des données d’un échantillon (ici, \\(n =\\) 161 parcelles). Il s’agit donc d’estimations des pentes et ordonnées à l’origine de la relation plus générale qui concerne la population globale, mais que nous ne connaîtrons jamais avec précision. Comme toute estimation, les valeurs de pente et d’ordonnée à l’origine de la droite de régression sont entachées d’incertitude. Nous pouvons quantifier ces incertitudes grâce au calcul des intervalles de confiance à 95% de ces 2 paramètres :\n\nconfint(reg1)\n\n                 2.5 %     97.5 %\n(Intercept) 1.11673087 1.27985782\nnSpecies    0.02328063 0.04257117\n\n\nCes résultats nous indiquent que les valeurs d’ordonnées à l’origine les plus probables dans la population générale sont vraisemblablement comprises entre 1.117 et 1.280. De même, les valeurs de pentes les plus probables dans la population générale sont vraisemblablement situées dans l’intervalle [0.023 ; 0.043]. Autrement dit, pour la pente de la droite de régression, la meilleure estimation possible vaut 0.033, mais dans la population générale, les valeurs comprises dans l’intervalle [0.023 ; 0.043] sont parmi les plus probables.\nIl est possible de visualiser cette incertitude grâce à la fonction geom_smooth() utilisée plus tôt, en spécifiant se = TRUE :\n\nplant %&gt;%\n  ggplot(aes(x = nSpecies, y = biomassStability)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(x = \"Nombre d'espèces par parcelle\",\n       y = \"Transformation log\\n de l'indice de stabilité de la biomasse\") +\n  scale_y_continuous(breaks = seq(0, 3, 0.25))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nDans la population générale, la vraie droite de régression peut se trouver n’importe où dans la bande grise. Cet intervalle d’incertitude est bien moins large que l’étendue des données sur l’axe des ordonnées, et c’est tant mieux. Il correspond à l’incertitude de la moyenne de l’indice de stabilité de la biomasse pour une valeur donnée de la variable explicative. Ainsi, par exemple, si on réalise une expérience avec plusieurs parcelles contenant toutes 8 espèces, alors, la régression et son incertitude associée nous disent que **la moyenne* de l’indice de stabilité de la biomasse vaudra environ 1.46 (la valeur de la droite de régression pour x = 8), avec un intervalle de confiance à 95% de [1.40 ; 1.51] (l’étendue de la zone grisée autour de la courbe pour \\(n =\\) 8 espèces."
  },
  {
    "objectID": "14-Regression.html#lalternative-non-paramétrique",
    "href": "14-Regression.html#lalternative-non-paramétrique",
    "title": "14  Régression linéaire",
    "section": "14.8 L’alternative non paramétrique",
    "text": "14.8 L’alternative non paramétrique\nLorsque les conditions d’application de la régression linéaire ne sont pas vérifiées, on a principalement deux options :\n\nOn essaie de transformer les données afin que les résidus de la régression se comportent mieux. Cela signifie tester différents types de transformations (passage au logarithme, à l’inverse, à la racine carrée…), ce qui peut être chronophage pour un résultat pas toujours garanti. Il existe de très nombreuses transformations et trouver la meilleure n’est pas trivial. Par ailleurs, l’interprétation d’une relation linéaire impliquant des données transformées n’est pas toujours aisée.\nOn utilise d’autre types de modèles de régression, en particulier les modèles de régressions linéaires généralisées (GLM), qui s’accommodent très bien de résidus non normaux et/ou non homogènes. Ces méthodes sont aujourd’hui préférées à la transformation des données et elles donnent de très bons résultats. Mais il s’agit là d’une toute autre classe de méthodes qui ne sont pas au programme de la licence."
  },
  {
    "objectID": "14-Regression.html#exercices",
    "href": "14-Regression.html#exercices",
    "title": "14  Régression linéaire",
    "section": "14.9 Exercices",
    "text": "14.9 Exercices\n\n14.9.1 Datasaurus et Anscombe\nExécutez les commandes suivantes :\n\nlibrary(datasauRus)\n\ndatasaurus_dozen %&gt;%\n    group_by(dataset) %&gt;%\n    summarize(\n      moy_x    = mean(x),\n      moy_y    = mean(y),\n      ecart_type_x = sd(x),\n      ecart_type_y = sd(y),\n      correl_x_y  = cor(x, y),\n      pente = coef(lm(y~x))[2],\n      ordonnee_origine = coef(lm(y~x))[1]\n    )\n\nExaminez attentivement les nombreux résultats produits par cette commande. Vous devriez remarquer que pour ces 13 jeux de données, 2 variables numériques x et y sont mises en relation. Pour tous ces jeux de données, on observe que la moyenne de tous les x est la même, la moyenne de tous les y est la même, les écarts-types des x sont identiques, les écarts-types des y aussi, la corrélation entre x et y est également extrêmement proche pour tous les jeux de données, et lorsque l’on effectue une régression linéaire de y en fonction de x, les ordonnées à l’origine et les pentes des droites de régression sont extrêmement proches pour les 13 jeux de données.\nSi on s’en tient à ces calculs d’indices synthétiques, on pourrait croire que ces jeux de données sont identiques ou presque. Pourtant, ce n’est pas par hasard que je vous répète à longueur de temps qu’il est indispensable de regarder les données avant de se lancer dans les analyses et les statistiques. Car ici, ces jeux de données sont très différents ! Conclure qu’ils sont identiques simplement parce que les statistiques descriptives sont égales, serait une erreur majeure :\n\ndatasaurus_dozen %&gt;%\n  ggplot(aes(x, y, color = dataset)) +\n  geom_point(show.legend = FALSE) +\n  facet_wrap(~dataset, ncol = 3) +\n  theme_bw()\n\n\n\n\nLe quartet d’Anscombe est un autre exemple de ce type de problème.\nDans la console, exécutez la commande suivante (il vous faudra peut-être presser la touche Entrée plusieurs fois) pour produire les 4 graphiques d’Anscombe :\n\nexample(anscombe)\n\nExaminez attentivement les nombreux résultats produits par cette commande dans la console, ainsi que les 4 graphiques obtenus. Vous devriez remarquer que pour ces 4 jeux de données, 2 variables numériques sont là encore mises en relation, et qu’elles présentent toutes les mêmes caractéristiques. En particulier, les régressions linéaires ont toutes les mêmes pentes et ordonnées à l’origine. Pourtant, seule l’une de ces régressions linéaires est valide. Pourquoi ?\n\n\n14.9.2 In your face\nLes hommes ont en moyenne un ratio “largeur du visage sur longueur du visage” supérieur à celui des femmes. Cela reflète des niveaux d’expression de la testostérone différents entre hommes et femmes au moment de la puberté. On sait aussi que les niveaux de testosterone permettent de prédire, dans une certaine mesure, l’agressivité chez les mâles de nombreuses espèces. On peut donc poser la question suivante : la forme du visage permet-elle de prédire l’agressivité ?\nPour tester cela, Carré et McCormick (2008) ont suivi 21 joueurs de hockey sur glace au niveau universitaire. Ils ont tout d’abord mesuré le ratio largeur du visage sur longueur du visage de chaque sujet, puis, ils ont compté le nombre moyen de minutes de pénalité par match reçu par chaque sujet au cours de la saison, en se limitant aux pénalités infligées pour cause de brutalité. Les données sont fournies dans le fichier hockey.csv.\nImportez, examinez et analysez ces données pour répondre à la question posée.\n\n\n\n\nCarré, Justin M, et Cheryl M McCormick. 2008. « In Your Face: Facial Metrics Predict Aggressive Behaviour in the Laboratory and in Varsity and Professional Hockey Players ». Proceedings of the Royal Society B: Biological Sciences 275 (1651): 2651‑56. https://doi.org/10.1098/rspb.2008.0873.\n\n\nDavies, Rhian, Steph Locke, et Lucy D’Agostino McGowan. 2022. datasauRus: Datasets from the Datasaurus Dozen. https://CRAN.R-project.org/package=datasauRus.\n\n\nTilman, David, Peter B. Reich, et Johannes M. H Knops. 2006. « Biodiversity and Ecosystem Stability in a Decade-Long Grassland Experiment ». Nature 441 (7093): 629. https://doi.org/10.1038/nature04742.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, et Shannon Ellis. 2022. skimr: Compact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr.\n\n\nWhitlock, Michael, et Dolph Schluter. 2015. The Analysis of Biological Data. Second edition. Greenwood Village, Colorado: Roberts and Company Publishers.\n\n\nWickham, Hadley. 2023. tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey Dunnington, et Teun van den Brand. 2024. ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, et Davis Vaughan. 2023. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, et Jennifer Bryan. 2023. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr."
  },
  {
    "objectID": "15-PropTests.html",
    "href": "15-PropTests.html",
    "title": "15  Comparaison de proportions",
    "section": "",
    "text": "Ne sera finalement pas traité cette année faute de temps…"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Campbell, Scott S., and Patricia J. Murphy. 1998. “Extraocular\nCircadian Phototransduction in Humans.” Science 279\n(5349): 396–99. https://doi.org/10.1126/science.279.5349.396.\n\n\nCarré, Justin M, and Cheryl M McCormick. 2008. “In Your Face:\nFacial Metrics Predict Aggressive Behaviour in the Laboratory and in\nVarsity and Professional Hockey Players.” Proceedings of the\nRoyal Society B: Biological Sciences 275 (1651): 2651–56. https://doi.org/10.1098/rspb.2008.0873.\n\n\nDavies, Rhian, Steph Locke, and Lucy D’Agostino McGowan. 2022.\ndatasauRus: Datasets from the Datasaurus Dozen. https://CRAN.R-project.org/package=datasauRus.\n\n\nFox, John, Sanford Weisberg, and Brad Price. 2023. Car: Companion to\nApplied Regression. https://CRAN.R-project.org/package=car.\n\n\nFuller, Andrea, Peter R. Kamerman, Shane K. Maloney, Graham Mitchell,\nand Duncan Mitchell. 2003. “Variability in Brain and Arterial\nBlood Temperatures in Free-Ranging Ostriches in Their Natural\nHabitat.” Journal of Experimental Biology 206 (7):\n1171–81. https://doi.org/10.1242/jeb.00230.\n\n\nGorman, Kristen B., Tony D. Williams, and William R Fraser. 2014.\n“Ecological Sexual Dimorphism and Environmental Variability Within\na Community of Antarctic Penguins (Genus Pygoscelis).” PLOS\nONE 9 (March): 1–14. https://doi.org/10.1371/journal.pone.0090081.\n\n\nHasselquist, Dennis, James A. Marsh, Paul W. Sherman, and John C.\nWingfield. 1999. “Is Avian Humoral Immunocompetence Suppressed by\nTestosterone?” Behavioral Ecology and Sociobiology 45\n(3): 167–75. https://doi.org/10.1007/s002650050550.\n\n\nHorst, Allison, Alison Hill, and Kristen Gorman. 2022.\nPalmerpenguins: Palmer Archipelago (Antarctica) Penguin Data.\nhttps://CRAN.R-project.org/package=palmerpenguins.\n\n\nIhaka, Ross, Paul Murrell, Kurt Hornik, Jason C. Fisher, Reto Stauffer,\nClaus O. Wilke, Claire D. McWhite, and Achim Zeileis. 2023.\nColorspace: A Toolbox for Manipulating and Assessing Colors and\nPalettes. https://CRAN.R-project.org/package=colorspace.\n\n\nJeppson, Haley, Heike Hofmann, and Di Cook. 2021. Ggmosaic: Mosaic\nPlots in the Ggplot2 Framework. https://github.com/haleyjeppson/ggmosaic.\n\n\nLiberg, Olof, Henrik Andrén, Hans-Christian Pedersen, Håkan Sand,\nSejberg, Petter Wabakken, Mikael Åkesson, and Staffan Bensch. 2005.\n“Severe Inbreeding Depression in a Wild Wolf Canis\nLupus Population.” Biology Letters 1 (1): 17–20. https://doi.org/10.1098/rsbl.2004.0266.\n\n\nMolofsky, Jane, and Jean-Baptiste Ferdy. 2005. “Extinction\nDynamics in Experimental Metapopulations.” Proceedings of the\nNational Academy of Sciences 102 (10): 3726–31. https://doi.org/10.1073/pnas.0404576102.\n\n\nMüller, Martina S., Elaine T. Porter, Jacquelyn K. Grace, Jill A.\nAwkerman, Kevin T. Birchler, Alex R. Gunderson, Eric G. Schneider, Mark\nA. Westbrock, and David J. Anderson. 2011. “Maltreated Nestlings\nExhibit Correlated Maltreatment as Adults: Evidence of a\n‘Cycle of Violence’ in Nazca Boobies (\nSula Granti ).”\nThe Auk 128 (4): 615–19. https://doi.org/10.1525/auk.2011.11008.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2023. Broom: Convert\nStatistical Objects into Tidy Tibbles. https://CRAN.R-project.org/package=broom.\n\n\nSignorell, Andri. 2023. DescTools: Tools for Descriptive\nStatistics. https://CRAN.R-project.org/package=DescTools.\n\n\nTilman, David, Peter B. Reich, and Johannes M. H Knops. 2006.\n“Biodiversity and Ecosystem Stability in a Decade-Long Grassland\nExperiment.” Nature 441 (7093): 629. https://doi.org/10.1038/nature04742.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia,\nHao Zhu, and Shannon Ellis. 2022. Skimr: Compact and Flexible\nSummaries of Data. https://CRAN.R-project.org/package=skimr.\n\n\nWhitlock, Michael, and Dolph Schluter. 2015. The Analysis of\nBiological Data. Second edition. Greenwood Village, Colorado:\nRoberts and Company Publishers.\n\n\nWickham, Hadley. 2021. Nycflights13: Flights That Departed NYC in\n2013. https://github.com/hadley/nycflights13.\n\n\n———. 2023. Tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. Readxl: Read Excel\nFiles. https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen,\nKohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey\nDunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant\nData Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis\nVaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2023. Readr: Read\nRectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nWickham, Hadley, Thomas Lin Pedersen, and Dana Seidel. 2023. Scales:\nScale Functions for Visualization. https://CRAN.R-project.org/package=scales.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. Tidyr:\nTidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWilkinson, Leland. 2005. The Grammar of Graphics. 2nd ed.\nNew-York: Springer-Verlag. https://www.springer.com/us/book/9780387245447.\n\n\nWiseman, Richard, and Peter Lamont. 1996. “Unravelling the\nIndian Rope-Trick.” Nature 383 (6597): 212.\nhttps://doi.org/10.1038/383212a0.\n\n\nWright, Kenneth P., and Charles A. Czeisler. 2002. “Absence of\nCircadian Phase Resetting in Response to Bright Light Behind the\nKnees.” Science 297 (5581): 571–71. https://doi.org/10.1126/science.1071697.\n\n\nYoung, Kevin V., Edmund D. Brodie, and Edmund D. Brodie. 2004.\n“How the Horned Lizard Got Its Horns.” Science 304\n(5667): 65–65. https://doi.org/10.1126/science.1094790."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analyse de Données",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "index.html#objectifs",
    "href": "index.html#objectifs",
    "title": "Analyse de Données",
    "section": "Objectifs",
    "text": "Objectifs\nCe livre contient l’ensemble du matériel (contenus, exemples, exercices…) nécessaire à la réalisation des travaux pratiques de l’EC Stratégie d’échantillonnage et analyse de données consacrés à la prise en main de R et RStudio.\nCes travaux pratiques ont essentiellement 4 objectifs :\n\nVous faire découvrir les logiciels R et Rstudio (Chapitre 1 et Chapitre 2) dans lesquels vous allez passer beaucoup de temps tout au long de votre cursus de master. Vous avez choisi une spécialité de master qui implique de traiter des données et de communiquer des résultats d’analyses statistiques : R et RStudio devraient être les logiciels vers lesquels vous vous tournez naturellement pour faire l’un et l’autre.\nVous apprendre à faire des graphiques de qualités dans RStudio et vous faire prendre conscience de l’importance des visualisations graphiques (Chapitre 3 : attention, ce chapitre est très long, ne vous laissez pas surprendre ! Et Chapitre 8) :\n\n\nd’une part, pour comprendre à quoi ressemblent les données en votre possession,\nd’autre part, pour vous permettre de formuler des hypothèses pertinentes et intéressantes concernant les systèmes que vous étudiez,\net enfin, pour communiquer efficacement vos trouvailles à un public qui ne connaît pas vos données aussi bien que vous (cela inclut évidemment vos enseignants à l’issue de vos stages).\nLes données que vous serez amenés à traiter lors de vos stages, ou plus tard, lorsque vous serez en poste, ont souvent été acquises à grands frais, et au prix d’efforts importants. Il est donc de votre responsabilité d’en tirer le maximum. Et ça commence toujours (ou presque), par la réalisation de visualisations graphiques parlantes.\n\n\nVous apprendre à manipuler efficacement des tableaux de données de grande taille (Chapitre 4 et Chapitre 5). Cela signifie que vous devriez être en mesure de sélectionner des variables (colonnes) d’un tableau, d’en créer de nouvelles en modifiant et/ou combinant des variables existantes, de filtrer des lignes spécifiques, d’effectuer des tris de données, de transformer des tableaux larges en tableaux longs (et réciproquement), d’effectuer des jointures entre plusieurs tableaux, etc.\nVous apprendre comment calculer des statistiques descriptives simples, sur plusieurs types de variables (Chapitre 6 et Chapitre 7), et comment mettre en œuvre, dans RStudio, les procédures statistiques décrites en cours (Chapitres 9, 10, 11, 12, 13, 14 et 15), afin de vous mettre dans les meilleures conditions possibles pour aborder d’une part les comptes-rendus de TP et rapports de stage que vous aurez à produire dans ce cursus de master et d’autre part les statistiques plus avancées que vous découvrirez lors des semestres 2 et 3. Vos enseignants attendent de vous la plus grande rigueur lorsque vous analysez et présentez des résultats d’analyses statistiques. Ces TP ont pour objectifs de vous fournir les bases nécessaires pour satisfaire ce niveau d’exigence.\n\nÀ l’issue de ces TP et TEA, vous devriez donc être suffisamment à l’aise avec le logiciel RStudio pour y importer des données issues de tableurs, les manipuler pour les mettre dans un format permettant les représentations graphiques et les analyses statistiques, pour produire des graphiques pertinents, adaptés aux données dont vous disposez, et d’une qualité vous permettant de les intégrer sans honte à vos compte-rendus de TP et rapports de stages, et de réaliser les tests et analyses statistiques les plus adaptés aux questions auxquelles vous tenterez de répondre.\nLes données que vous serez amenés à traiter lors de vos stages, ou plus tard, lorsque vous serez en poste, ont souvent été acquises à grands frais, et au prix d’efforts importants. Il est donc de votre responsabilité d’en tirer le maximum. Et ça commence toujours (ou presque), par la manipulation de données dans RStudio et la réalisation de visualisations graphiques parlantes. Se lancer dans les tests statistiques sans avoir une idée claire de la structure des données dont on dispose est toujours une erreur. C’est pourquoi les chapitres consacrés aux statistiques n’arrivent que dans la seconde partie de ce livre en ligne. En règle générale, face à une question scientifique précise, lorsque l’on traite des données, environ 80% du temps est consacré à la mise en forme et l’exploration (statistique et graphique) des données. La réalisation des tests et leur interprétation ne prend que rarement plus de 20% du temps. Cela souligne l’importance des 5 premiers chapitres de ce livre en ligne : plus vous serez à l’aise avec les notions et concepts décrits dans ces chapitres, plus vous serez efficaces et plus vous gagnerez du temps par la suite.\n\n\n\n\n\n\nImportant\n\n\n\nÀ partir de maintenant, tous les compte-rendus de TP que vous aurez à produire dans le cadre du master Gestion de l’Environnement et Écologie Littorale devront respecter les bonnes pratiques décrites dans ce document. En particulier, les collègues de l’équipe pédagogique attendent que les graphiques que vous intégrerez à vos compte-rendus de TP soient systématiquement produits dans RStudio. C’est la raison pour laquelle cet enseignement arrive si tôt dans votre cursus."
  },
  {
    "objectID": "index.html#organisation",
    "href": "index.html#organisation",
    "title": "Analyse de Données",
    "section": "Organisation",
    "text": "Organisation\nAu total, la partie analyse de données de l’EC “Stratégie d’échantillonnage et analyse de données” contient :\n\n15 heures de cours magistraux\n9 heures de travaux pratiques (pour chaque groupe)\n16 heures de TEA\n\n\nLes cours magistraux\nLes cours magistraux sont globalement découpés en 2 blocs à peu près indépendants :\n\nun bloc de 10 heures consacrées aux notions statistiques élémentaires, aux statistiques descriptives et aux statistiques inférentielles. Nous couvrirons notamment les notions d’incertitude et d’inférence, les tests d’hypothèses, la comparaison de proportions, l’ajustement de données observées à des distributions théoriques, l’analyse de tables de contingences, les comparaisons de moyennes, les régressions linéaires, les ANOVA et ANCOVA…\nun bloc de 5 heures consacrées aux statistiques multivariées telles que l’Analyse en Composantes Principales (ACP) et l’Analyse Factorielle des Correspondances (AFC).\n\nMon objectif n’est pas de survoler l’ensemble du matériel dans ce faible volume horaire : s’il n’est pas suffisant, nous ajouterons quelques séances afin de traiter correctement l’ensemble du matériel. Je suis convaincu que tout le monde est capable de comprendre les grands principes des statistiques, et de réaliser des analyses dans un logiciel tel que R, y compris les plus réfractaires aux mathématiques et à l’informatique. Mais il est nécessaire de démystifier cette discipline essentielle, et si certains ont besoin de plus de temps que d’autres, nous prendrons ce temps. Les TP et TEA, décrits plus bas, sont justement organisés pour permettre à chacun d’avancer à son rythme. Mais ne vous y trompez pas, cela vous demandera beaucoup de travail pendant ces 3 semaines.\nTous les aspects vus en cours seront en effet développés lors des séances de TP et de TEA. Vous aurez, pour chaque partie, des exercices à préparer et nous les corrigerons ensemble lors des séances de TP et/ou de TEA. ils doivent d’une part vous préparer aux évaluations (voir plus bas) mais surtout, vous permettre d’acquérir des compétences en analyse de données, compétences qui seront attendues de vous lorsque vous sortirez diplômé·e de ce master.\n\n\nLes Travaux pratiques\nLe contenu des séances de travaux pratiques sera découpé en 3 parties (inégales) :\n\nPrise en main des logiciels R et RStudio\nIllustration du cours sur les statistiques descriptives et inférentielles, mise en pratique et réalisation d’exercices\nIllustration du cours sur les statistiques multivariées, mise en pratique et réalisation d’exercices\n\nPour chaque séance de TP, vous travaillerez soit à distance, soit en salle banalisée, sur vos ordinateurs personnels. La première séance aura lieu en présentiel et sera consacrée à l’installation des logiciels ainsi qu’à la présentation de l’organisation des séances.\nLes séances de travaux pratiques ne seront pas toutes obligatoires : seules quelques séances en présentiel (les dates vous seront présentées ultérieurement) le seront, probablement pas plus d’une ou deux par semaine.\n\n\n\n\n\n\nImportant\n\n\n\nL’essentiel du contenu de cet enseignement peut être abordé en autonomie, à distance, grâce à ce livre en ligne, aux ressources mises à disposition sur Moodle et à votre ordinateur personnel. Cela signifie que la présence physique lors de ces séances de TP n’est pas obligatoire.\n\n\nPour toutes les autres séances, le fonctionnement sera celui d’une permanence non obligatoire : seuls celles et ceux qui en éprouvent le besoin sont tenus de se déplacer. Ces séances de permanence n’auront lieu que si certains parmi vous m’ont fait part de difficultés ou ont formulé des questions en amont des séances. Si aucune question ne m’a été posée en amont, les permanences n’auront pas lieu. Si une permanence a lieu, elle est ouverte à tous, quel que soit votre groupe de TP. Vous n’êtes d’ailleurs pas tenus de rester pendant 90 minutes : vous venez avec votre question, on y répond ensemble, et vous êtes libres de repartir quand bon vous semble. Les années précédentes, je voyais certains de vos collègues à chaque séance de permanence alors que d’autres ne sont jamais venus. Si vous n’en avez pas besoin, libre à vous de ne pas venir. Tant que le travail est fait et que les exercices ne vous posent pas de problème, vous êtes libres de vous organiser comme vous l’entendez.\nAttention toutefois, venir à une séance de permanence en n’ayant pas préparé de question au préalable ne vous sera d’aucune aide. C’est parce que vous avez travaillé en amont de ces séances et que vous arrivez avec des questions que ces permanences sont utiles et efficaces. Donc si vous venez, c’est que vous avez bossé en amont !\nCe fonctionnement très souple a de nombreux avantages :\n\nvous vous organisez comme vous le souhaitez\nvous ne venez que lorsque vous en avez vraiment besoin\ncelles et ceux qui se déplacent reçoivent une aide personnalisée “sur mesure”\nvous travaillez sur vos ordinateurs\nles effectifs étant réduits, les conditions de travail sont idéales\n\nToutefois, pour que cette organisation fonctionne, cela demande de la rigueur de votre part, en particulier sur la régularité du travail que vous devez fournir. Si la présence en salle de TP n’est pas requise, le travail demandé est bel et bien obligatoire ! Si vous venez en salle de TP sans avoir travaillé en amont, votre venue sera totalement inutile puisque vous n’aurez pas de question à poser et que vous passerez votre séance à lire ce livre en ligne. Vous perdrez donc votre temps, celui de vos collègues, et le mien. De même, si vous attendez la 3e semaine pour vous y mettre, vous irez droit dans le mur. Outre les heures de TP/TEA prévues dans vos emplois du temps, vous devez donc prévoir du travail personnel supplémentaire, chaque jour ou presque.\n\n\nUtilisation de Slack\n\n\n\n\n\n\nComment savoir si une séance de permanence a lieu, comment poser une question ?\n\n\n\nTout se passera en ligne, grâce au logiciel Slack, qui fonctionne un peu comme un “Twitter privé”.\n\n\nSlack facilite la communication des équipes et permet de travailler ensemble. Créez-vous un compte en ligne et installez le logiciel sur votre ordinateur (il existe aussi des versions pour tablettes et smartphones). Lorsque vous aurez installé le logiciel, cliquez ici pour vous connecter à notre espace de travail commun, et reprenez la lecture de ce document.\nVous verrez que 3 “chaînes” sont disponibles :\n\n#organisation : c’est là que les questions liées à l’organisation du cours, des TP et TEA doivent être posées. Si vous ne savez pas si une séance de permanence a lieu, posez la question ici.\n#rstudio : c’est ici que toutes les questions pratiques liées à l’utilisation de R et RStudio devront êtres posées. Problèmes de syntaxe, problèmes liés à l’interface, à l’installation des packages ou à l’utilisation des fonctions… Tout ce qui concerne R ou RStudio mais pas directement les statistiques sera traité ici. Vous êtes libres de poser des questions, de poster des captures d’écran, des morceaux de code, des messages d’erreur. Et vous êtes bien entendus vivement encouragés à vous entraider et à répondre aux questions de vos collègues. Je n’interviendrai ici que pour répondre aux questions laissées sans réponse ou si les réponses apportées sont inexactes. Le fonctionnement est celui d’un forum de discussion instantané. Vous en tirerez le plus grand bénéfice en participant et en n’ayant pas peur de poser des questions, même si elles vous paraissent idiotes. Rappelez-vous toujours que si vous vous posez une question, d’autres se la posent aussi probablement.\n#statistiques : c’est ici que toutes les questions liées aux méthodes statistiques devront être posées. Comme pour la chaîne #rstudio, vous êtes encouragés à poster des questions mais aussi des réponses. Le fonctionnement de l’ensemble se veut participatif.\n\nAinsi, quand vous travaillerez à vos TP ou TEA, prenez l’habitude de garder Slack ouvert sur votre ordinateur. Même si vous n’avez pas de question à poser, votre participation active pour répondre à vos collègues est souhaitable et souhaitée. Votre niveau de participation sur Slack pourra faire partie de votre note finale.\nSi toutes les questions posées sur Slack ont trouvé une réponse, alors, inutile d’organiser une permanence. Si en revanche, certains n’ont pas compris, si les mêmes questions reviennent fréquemment, ou si des explications “en direct” sont plus efficaces qu’un long message sur Slack, alors une permanence aura lieu.\n\n\nLe TEA\nLes séances de TEA auront toutes lieu “à distance”. Je ne suis pas tenu d’être présent lors des séances de TEA, même si une salle banalisée est systématiquement réservée pour vous permettre de vous retrouver et de travailler ensemble. Je m’engage en revanche à être disponible sur Slack pour répondre rapidement aux questions posées lors des TEA. Et si certaines questions n’ont pas trouvé de réponse pendant les séances de TEA, nous y répondront lors du TP suivant.\nGénéralement, l’organisation de votre journée sera la suivante :\n\nEn début de matinée, 1h30 ou 3h de cours magistraux\nEn milieu de journée du temps libre ou pour avancer sur ce document, les exercices, la prise en main de R et RStudio, etc.\nEn fin de journée une séance de TEA et/ou de TP/permanence non obligatoire de 90 minutes pour ceux qui en ont besoin et se manifestent."
  },
  {
    "objectID": "index.html#progression-conseillée",
    "href": "index.html#progression-conseillée",
    "title": "Analyse de Données",
    "section": "Progression conseillée",
    "text": "Progression conseillée\nPour apprendre à utiliser un logiciel comme R, il faut faire les choses soi-même, ne pas avoir peur des messages d’erreurs (il faut d’ailleurs apprendre à les déchiffrer pour comprendre d’où viennent les problèmes), essayer maintes fois, se tromper beaucoup, recommencer, et surtout, ne pas se décourager. J’utilise ce logiciel presque quotidiennement depuis plus de 15 ans et à chaque session de travail, je rencontre des messages d’erreur. Avec suffisamment d’habitude, on apprend à les déchiffrer, et on corrige les problèmes en quelques secondes. Ce livre est conçu pour vous faciliter la tâche, mais ne vous y trompez pas, vous rencontrerez des difficultés, et c’est normal. C’est le prix à payer pour profiter de la puissance du meilleur logiciel permettant d’analyser des données, de produire des graphiques de qualité et de réaliser toutes les statistiques dont vous aurez besoin d’ici la fin de vos études et au-delà.\nPour que cet apprentissage soit le moins problématique possible, il convient de prendre les choses dans l’ordre. C’est la raison pour laquelle les chapitres de ce livre doivent être lus dans l’ordre, et les exercices d’application faits au fur et à mesure de la lecture.\nUne fois compilé en pdf, ce document représente plus de 350 pages, ce qui veut dire que vous devriez vous approprier environ 25 pages par jour. En particulier, la Chapitre 3 est très longue et il est facile de se laisser dépasser.\n\n\n\n\n\n\nTravaillez régulièrement !\n\n\n\nQue vous veniez aux séances de permanence ou non, j’insiste sur l’importance de travailler cette matière régulièrement. Vous devez vous y mettre dès maintenant et y consacrer quelques heures chaque jour. Interrogez vos collègues de M2 qui ont eu cet enseignement l’an dernier : il y a beaucoup de temps à y passer et il est hélas facile de prendre et d’accumuler du retard…\n\n\nUne fois cette UE terminée, vous pourrez évidemment consulter ce livre quand bon vous semblera, et dans n’importe quel ordre. Le champ de recherche situé en haut à gauche est d’ailleurs très utile pour (re)trouver les passages que vous recherchez. Ce livre restera en ligne et vous pourrez y accéder même après avoir quitté l’université de La Rochelle. Vos prédécesseurs me confirment régulièrement à quel point il leur est resté utile bien après le master. Soyez toutefois prévenu que les contenus de ce livre peuvent évoluer avec le temps : j’essaie en effet de remettre à jour tout ce qui doit l’être le plus régulièrement possible. Et cela signifie parfois que des sections peuvent disparaître ou être remplacées si des façons de procéder plus modernes sont préférables."
  },
  {
    "objectID": "index.html#lévaluation",
    "href": "index.html#lévaluation",
    "title": "Analyse de Données",
    "section": "L’évaluation",
    "text": "L’évaluation\nVous aurez plusieurs types d’évaluations cette année :\n\nUne évaluations par les pairs qui portera sur la qualité de vos scripts. Cette évaluation qui entrera pour une toute petite partie dans la note finale de l’EC a pour objectif principal de vous permettre de vous situer dans vos apprentissages. Vous évaluerez vous même, et de façon anonyme, plusieurs copies de vos camarades en suivant une grille d’évaluation critériée que nous construirons ensemble. De même, votre copie sera évaluée par plusieurs de vos camarades. Cette approche a de nombreux avantages. Elle vous permet notamment de mieux vous approprier les grilles de notations (par exemple, qu’est-ce qu’un bon script sous R ? À l’inverse, qu’est-ce qu’un script médiocre ? Comment être sûr que la méthode statistique choisie est la bonne pour répondre à une question donnée ? Suis-je capable de décrire correctement un tableau de données de grande taille ? Suis-je capable de produire des graphiques informatifs ?) et rends possible un retour personnalisé sur vos travaux beaucoup plus rapidement que si votre enseignant était le seul à corriger l’ensemble de vos travaux. Pas d’inquiétude, vous serez guidés à chaque étape.\nUne évaluation individuelle courte qui ne portera pas sur les analyses statistiques à proprement parler, mais sur votre capacité à produire un graphique de qualité, original et qui raconte une histoire intéressante sur un jeu de données imposé. Cet exercice n’est pas réalisé chaque année faute de temps.\nUne évaluation individuelle plus classique, sur table ou à la maison, avec quelques exercices qui vous demanderont de mettre en œuvre les méthodes statistiques décrites lors de cours magistraux.\nEnfin, une évaluation qui prendra la forme d’un rapport et qui sera réalisé conjointement avec les travaux de stratégie d’échantillonnage réalisés avec Fanny Cusset. Cette partie de l’EC est en effet complémentaire de l’analyse de données puisqu’elle permet d’avoir une approche globale, de la question scientifique à la production d’un rapport et d’une soutenance, en passant par la réflexion sur la stratégie d’échantillonnage, la mise en œuvre sur le terrain, le traitement des échantillons au laboratoire, et l’exploitation statistique des résultats. Ce travail sera donc évalué conjointement par Fanny Cusset et moi. La note de la partie analyse de données portera essentiellement sur les parties “matériels et méthodes” et “résultats” du rapport. Il est en effet important de comprendre dès maintenant que l’analyse de données n’est pas une fin en soi : on ne fait pas des statistiques pour le plaisir, ou sans but précis. Ça n’est qu’un outil de votre panoplie d’écologue au service d’une question scientifique. L’analyse de données et les statistiques vous permettront de répondre à des questions scientifiques de façon objective, mais leur utilisation appropriée suppose que vous ayez les idées claires en amont sur la question scientifique à laquelle vous tentez de répondre. C’est cette démarche qui devrait vous guider tout au long de votre cursus de master et au-delà, dans votre vie professionnelle.\n\nDans le cadre de l’approche compétences, j’essaierai d’indiquer, dans la mesure du possible, quelles sont les compétences et résultats d’apprentissages dont vous devrez faire l’acquisition pour chaque évaluation. À l’issue de cet enseignement, vous devriez être capables de :\n\nMettre en forme des données acquises sur le terrain ou au laboratoire afin d’en permettre l’importation dans R ou RStudio.\nProduire des statistiques descriptives informatives permettant de comprendre la structure et les tendances principales d’un jeu de données.\nCréer dans R ou RStudio des graphiques lisibles et informatifs permettant de mettre en évidence les tendances principales d’un jeu de données.\nProduire des scripts clairs sous R ou RStudio, permettant la reproductibilité des traitements de données et des analyses statistiques ainsi que la communication avec vos pairs.\nAnalyser des données uni-, bi- ou multi-variées issues d’observations et de mesures sur le terrain et au laboratoire en choisissant les méthodes appropriées pour répondre à une problématique scientifique précise.\nMaîtriser les logiciels R ou RStudio pour réaliser des analyses statistiques, des représentations graphiques ou des simulations numériques."
  },
  {
    "objectID": "index.html#licence",
    "href": "index.html#licence",
    "title": "Analyse de Données",
    "section": "Licence",
    "text": "Licence\nCe livre est ligne est sous licence Creative Commons (CC BY-NC-ND 4.0)\n\n\n\n\n\nVous êtes autorisé à partager, copier, distribuer et communiquer ce matériel par tous moyens et sous tous formats, tant que les conditions suivantes sont respectées :\n\n\n Attribution : vous devez créditer ce travail (donc citer son auteur), fournir un lien vers ce livre en ligne, intégrer un lien vers la licence Creative Commons et indiquer si des modifications du contenu original ont été effectuées. Vous devez indiquer ces informations par tous les moyens raisonnables, sans toutefois suggérer que l’auteur vous soutient ou soutient la façon dont vous avez utilisé son travail.\n\n\n Pas d’utilisation commerciale : vous n’êtes pas autorisé à faire un usage commercial de cet ouvrage, ni de tout ou partie du matériel le composant. Cela comprend évidemment la diffusion sur des plateformes de partage telles que studocu.com qui tirent profit d’œuvres dont elles ne sont pas propriétaires, souvent à l’insu des auteurs.\n\n\n Pas de modifications : dans le cas où vous effectuez un remix, que vous transformez, ou créez à partir du matériel composant l’ouvrage original, vous n’êtes pas autorisé à distribuer ou mettre à disposition l’ouvrage modifié.\n\n\n Pas de restrictions complémentaires : vous n’êtes pas autorisé à appliquer des conditions légales ou des mesures techniques qui restreindraient légalement autrui à utiliser cet ouvrage dans les conditions décrites par la licence."
  }
]